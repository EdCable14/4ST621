[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "4ST621",
    "section": "",
    "text": "This is website dedicated to the 4ST621 course Probability and Mathematical Statistics 1."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is website dedicated to the 4ST621 course Probability and Mathematical Statistics 1."
  },
  {
    "objectID": "Course_Intro.html",
    "href": "Course_Intro.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to the course 4ST621 – Probability Theory and Mathematical Statistics I.\nAs the name suggests, this course has two main parts. So, what is the difference between probability and statistics?\nIn probability theory, we deal with the following question: Given some assumptions about the generating process, what can we say about (possible) data?\nIn (inferential) statistics, we ask quite the opposite question: Given the data, what can we say about the (possible) generating process?  \nIn the first (longer) part, we will start with some basic terms and operations used upon probabilities, which will lead us to the Bayes theorem, a potent tool for thinking about data. Then, we will move to random variables, distributing probabilities among real numbers and what characteristics can come from these distributions. Generalising into two dimensions, we will also add problematics of relations between random variables. Then, with some general ideas in mind, we will move to specific probability distributions, or probabilistic models, arising from various assumptions. This tour is finished with stochastic convergences, which provide reasoning machinery for inferential statistics, connecting it with the probability theory.\nIn the second (shorter) part, we will start with the basics of sampling – how data come into existence and how we can use stochastic convergences or some other assumptions to model the probability distribution of various statistics. We will make a slight detour into the area of point estimates to cover important notions of estimators’ variability and bias. After this, we will return to the sampling theory and see how to use it to test various claims about the generating process. Hypothesis testing is an essential tool in the statistical bag if done correctly, which is not so easy without understanding some basic and often overlooked philosophical connections. To finish the course, we will introduce confidence intervals, which are dual to hypothesis testing but bring additional information.\nThis course is by no means a complete treatise on the subjects outlined in the previous paragraphs. It is rather an introductory, sometimes more and sometimes less comprehensive, into the topics.\nPlease avoid falling into the Dunning-Kruger-like trap of being overconfident after learning the basics."
  },
  {
    "objectID": "LN1_a.html",
    "href": "LN1_a.html",
    "title": "Interpretation of Probability",
    "section": "",
    "text": "Let´s begin with the interpretation of probability – what does probability mean?\nWhat does it mean when someone says the \"probability of dying at 80 is 0.03\"?\nWhat does it mean when someone says the \"probability of tomorrow’s rain is 0.5\"?\nThere must be some meaning. Throughout history, two competing schools of probability thought have always existed. One is known as objectivist, and thesecond is known as subjectivist.\nFor the first school, the one that dominates this course, probability is an object that we might attempt to discover, but something that exists no matter what we do. Historically, this school has been presented as a physical school, which describes probability as the result of the world’s physics. In a more modern setting, this school is presented more as frequentist, which holds that probability is a limit of relative frequency in a repeated series of random trials.\nFor the second school, the one that has been somewhat suppressed in the 20th century but is on the rise recently, probability is a subject – something that is dependent on the observer. In this school of thought, the probability is an expression of uncertainty.\nWhile this distinction might, for you, now seem a somewhat interesting but useless philosophical quarrel, it eventually leads to two surprisingly different ways of doing statistical inference, which is in scope in the second half of the course."
  },
  {
    "objectID": "LN2_a.html",
    "href": "LN2_a.html",
    "title": "Total Probability Law",
    "section": "",
    "text": "In the previous part of the lecture notes, we defined conditional probability. We will continue in the journey to reach two very important results of probability theory - total probability law now and Bayes theorem in the next part.\nLet´s start with the simplest case: we have one random event B followed by the second random event A. Random event A is dependent on the random event B, meaning its conditional probabilities are different based on whether event B or its complement occurred: \\(P(A|B) \\not= P(A|\\bar{B})\\).\nThe first question arises: what is (un)conditional probability of the random event A? If we think a little while about this, the four intersections create elementary sample space:\n\\(E=(A\\cap{B}, A\\cap{\\bar{B}}, \\bar{A}\\cap{B}), \\bar{A}\\cap{\\bar{B}})\\).\nRandom event A is simply union: \\(A=(A\\cap{B})\\cup({A\\cap{\\bar{B}}})\\).\nAnd since the two intersections are elementary, they are disjoint and the probability of such a union:\n\\(P(A)=P(A\\cap{B})+P({A\\cap{\\bar{B}}})\\).\nIn simple terms, the random event A occurs if random event A occurs with B or with the complement to B, and total probability is the summation of the two possibilities. If we apply the formula from the previous section, we can find the usual way total probability law is written in this case:\n\\(P(A)=P(B)*P(A|B)+P(\\bar{B})*P(A|\\bar{B})\\).\nIn the last step, we can generalise. Instead of the random event B and its complement, let´s now assume that the first random trial can end up in k various disjoint random events, which together fully partition the sample space of this random trial:\n\\(E=\\cup_{i=1}^kB_i\\).\nHence, the summation of probabilities of \\(B_i\\) is equal to one:\n\\(\\sum_{i=1}^k(P(B_i))=1\\).\nThen we can follow the same logic as in the first case, where k = 2, and find out the general formula for total probability law:\n\\(P(A)=\\sum_{i=1}^k(P(B_i)*P(A|B_i))\\).\nWe can interpret this formula - total probability is the weighted average of the conditional probabilities \\(P(A|B_i)\\) with weights equal to respective probabilities of conditions \\(P(B_i)\\)."
  },
  {
    "objectID": "LN1_b.html",
    "href": "LN1_b.html",
    "title": "Definitions of Probability",
    "section": "",
    "text": "In the previous part, the focus was on the interpretation of probability. Let´s now discuss the definition of probability. These are hard to separate from the interpretation and, in some cases, are tightly connected.\n\nThe classical definition of probability\nHistorically, formal treatises on probabilities emerged in the 16th and 17th centuries with scholars dealing with the “game of chance”. These games were based chiefly on throwing dice, but others were also based on cards or roulette wheels. With these games, the idea of a fair outcome emerged – that each outcome at one random trial has equivalent probability. With this assumption, it was easy to calculate probabilities by counts and combinatorics theory, which was emerging to solve many probabilistic puzzles.\nWhat emerged in the works of Italian Gerolamo Cardano is now called classical definition and was cemented by Pierre-Simon Laplace. It states that the probability of an event is the ratio of favourable outcomes to all possible outcomes with the principle of difference – no specific outcome is more likely than any other.\n\n\nIntermezzo - Important Terms\nRandom trial –a trial (experiment) in which results cannot be predicted with certainty and repeatable (at least theoretically) under the same conditions.\nRandom event – is a result of random trial.\nElementary random event – is the smallest possible outcome of a random trial. More formally, it is such a random event that the union of other random events cannot create it. Is indivisible.\nSample space E – is a set of all possible elementary random events.\nRandom events (usually denoted by capital letters from the beginning of the alphabet – A, B, C,…)can thus be viewed as any non-empty subset of sample space.\nSet operations are essential for probability calculations because the definitions of the initial terms are based on the sets.\nIntersection \\(A \\cap B\\) is a set operation meaning both A and B true.\nUnion \\(A \\cup B\\) is a set operation, meaning at least one of A and B is true.\nComplement \\(\\bar A\\) is aset operation meaning true if A is not true. Opposite of A.\n\n\nGoing back to the classical definition of probability\nWith new terms, we can define the classical definition of probability as\n\\[\nP(A)= \\frac {m}{n}\n\\]\nWhere A is the union of m (number of favourable) random events and n is the number of all possible random events, where individual events are equally likely and mutually exclusive.\n\n\nGeometric definition of probability\nThe classical definition of probability was, among other things, limited by its discrete nature. To overcome this limitation and solve new issuing problems, scholars in the 18th century started thinking about continuous sample spaces. This led to the generalisation called the geometric definition of probability, which can be expressed as\n\\[\nP(A)=\\frac {V(A)}{V(E)}\n\\]\nWhere the V operator represents continuous measure – in one dimension length, in two dimensions area and in more dimensions simply volume. What remained was the idea that each pair of regions of the same volume was equally likely.\nThe most famous problem solved using the geometric definition of probability was Buffon´s needle: Imagine a floor made of parallel strips of wood, each with the same width t. Now, randomly drop a needle with a length of l onto this floor. How likely is the needle to lie across a line between two strips? This probability, assuming \\(l \\leq t\\) was proved to be\n\\[\nP(A)=\\frac {2*l}{\\pi*t}\n\\]\nMuch later, this problem was turned around, and Buffon´s needle was used to estimate the value of \\(π\\) by substituting relative frequency p for probability P(A):\n\\[\nest(\\pi)=\\frac{2*l} {p *t}\n\\]\nSubstituting relative frequency for probability is a trick available thanks to the laws describing convergences in probability and is the basis of the frequentist definition.\n\n\nFrequentist definition of probability\nThe classical definition of probability stumbled upon a severe limitation – its reliance on the principle of indifference, which might be justifiable in the games of chance but hardly so in other real-world instances. This led to more broad definition of probability.\nHave a series of random trials. Based on this series, have a series of relative frequencies of observing random event A: pn(A). This series of relative frequencies will converge (in probability) to its limit, the underlying probability of an event A.\n\\[p_n(A) \\rightarrow P(A)\\]We will deal with stochastic convergences later because these laws connect probability theory with statistics.\n\n\nAxiomatic definition\nAround the same time R.A. Fischer or Richard von Misses (brother of not less famous Austrian economist Ludwig von Misses) were championing and rigorously proving frequentist definition and interpretation of probability, a completely new approach emerged: axiomatic definition, work of brilliant Russian mathematician A. N. Kolmogorov. This approach focused on the smallest basic set of rules, axioms, which can serve as a basis for all manipulations with probabilities. Kolmogorov´s axioms, in simplicity, are three:\nNon-negativity – The probability of a random event is not a negative number: \\(P(A) \\geq 0\\)\nNormalisation – The probability of an entire sample space is 1: \\(P(S)=1\\)\nAdditivity – For mutually exclusive random events A and B, the probability of their union is the sum of their probabilities: \\(P(A\\cap B)=0\\rightarrow P(A\\cup B)=P(A)+P(B)\\)\nBased on these three axioms, all the rules for computing with probabilities can be derived. The most essential derivations are:\nThe probability of an empty set is 0: \\(P(\\emptyset )=0\\)\nThe probability of a random event is between 0 and 1: \\(0 \\leq P(A)\\leq 1\\)\nThe probability of a complement is: \\(P(\\bar{A})=1-P(A)\\)\nThe probability of a union is: \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\)"
  },
  {
    "objectID": "LN1_c.html",
    "href": "LN1_c.html",
    "title": "Transformations of Probability",
    "section": "",
    "text": "It is derived from Kolmogorov´s axioms, that probability is a number between 0 and 1. However, for statistical modelling and sometimes in the usual language, we need numbers related to the probability but used on a different scale. This is what various transformations of probability ensure.\n\nOdds\nOdds are an everyday linguistic use of probability, often seen in betting, that is a transformation:\n\\[\nOdds(A)=\\frac {P(A)}{P(\\bar A)}= \\frac {P(A)}{1-P(A)}\n\\]\nThe transformation lands in the scale \\((0, \\infty)\\).\nSo, instead of saying that the probability of a home team winning a soccer match is 0.2, this probability is one to four because odds equal 0.2 over 0.8 equals 1 fourth. Odds can also be used to calculate the Bayes formula.\n\n\nLogit\nExpressing probability on the unbounded scale might be helpful for linear statistical modelling. This is where logit, using natural base logarithm, comes in handy:\n\\[\nLogit(A)=log(odds(A))=log \\left( \\frac {P(A)}{1-P(A)} \\right)\n\\]\n\n\nNegative log transformation\nThis type of transformation is vital in information theory, where we transform probability on the positive scale \\((0, \\infty)\\), expressing the surprisingness levels of random events. The base of the logarithm tells us the information units, e.g. base = 2 leads to the result expressed in bits.\n\\[\n-log_2(P(A))\n\\]"
  },
  {
    "objectID": "LN1_d.html",
    "href": "LN1_d.html",
    "title": "Two Random Events",
    "section": "",
    "text": "Considering two (or more, of course) random events, we can introduce important statistical concepts – conditionality and (in)dependence.\n\nIndependent random events\nThe two random events, coming from two random trials, are independent if the result of one of the trials does not influence the probabilities of the results of the other trial, and vice versa.\nWe note conditional probability as \\(P(A|B)\\) meaning the probability of A under the condition of B. The fact that the change in condition does not change this probability can be expressed as\n\\[\nP(A|B)=P(A|\\bar B)=P(A)\n\\]\nand vice versa.\nLet´s make a quick detour. What is the probability of the intersection of two random events? Intersection means that both events occur. The product of two probabilities gives the probability of two events occurring, but which? Having random events A and B, we can say that both occur is equivalent to A occurring and B occurring under condition A has occurred. And vice versa, so\n\\[\nP(A \\cap B)=P(A)*P(B|A)=P(B)*P(A|B)\n\\]\nWith this detour, we can posit necessary and sufficient condition of independence:\n\\[\nP(A \\cap B)=P(A)*P(B)\n\\]\nIf two random events are independent, the probability of their intersection is a product of individual probabilities. And if the product of individual probabilities is equal to the probability of intersection, the two random events are independent. This has two critical consequences – first, it simplifies calculation if we assume independence (e.g., standard likelihood function used in estimation theory). Second, it can be used to test the hypothesis of independence when having a set of data.\n\n\nConditional probability\nLast but not least comes an essential formula: conditional probability calculation. Since we introduced that \\(P(A \\cap B)=P(A)*P(B|A)\\), it should be easy to see how to calculate\n\\[\nP(B|A)=\\frac{P(A \\cap B)}{P(A)}\n\\]\nand vice versa.\nThis can be interpreted this way: the probability of B under the condition of A is the probability of both events occurring divided by the probability of the condition P(A). The condition limits the sample space, leading to normalisation such that\n\\[\nP(B|A)+P(\\bar B|A)=\\frac {P(A \\cap B)}{P(A)} + \\frac {P(A \\cap \\bar B)}{P(A)}=\\frac{P(A \\cap B)+P(A\\cap \\bar B)}{P(A)}=\\frac {P(A)}{P(A)}=1\n\\]\nYes, under condition A, either B or complement to B must happen."
  },
  {
    "objectID": "LN2_b.html",
    "href": "LN2_b.html",
    "title": "Bayes Theorem",
    "section": "",
    "text": "We used total probability law to determine event A’s probability independent of the preceding event B. But we can be, and often are, interested in a different question: knowing that event A happened, what is the probability that before it, event B happened?\nThis question was generally answered in the late 18th century independently by two persons: first by English reverend Thomas Bayes, whose work was corrected and published posthumously by his friend Richard Price, and later and more generally by Pierre-Simon Laplace, who put the ground to what is now called Bayesian inference. The last name to mention here is Sir Jeffreys, who put Laplace´s formulation in line with the axiomatic definition of probability.\nIt is important to note that even though Bayes´ theorem is a basis for Bayesian inference (statistic) and was developed as such, it is generally an undisputable and uncontroversial finding in probability theory.\nLet´s start with \\(P(B|A)=\\frac{P(A\\cap B)}{P(A)}\\); then P(A) is given by total probability law, hence:\n\\[\nP(B|A)=\\frac{P(A\\cap B)}{P(A\\cap B)+P(A\\cap \\bar{B})}\n\\]\nWe can interpret it, e.g., in this way: What part of the total probability of event A is attributable to events A and B occurring together? In a more general way, this specific case of Bayes theorem can be written using the notation from the previous section:\n\\[ P(B_{i}|A)=\\frac{P(A\\cap B_{i})}{\\sum_{i=1}^kP(B_i) *P(A|B_i)} \\]\nIn statistics, probabilities \\(P(B_i)\\) are usually called prior probabilities or simply priors to stress that they represent probability before an event A does or does not occur (before data). Probabilities \\(P(A|B_i)\\) are in statistics called likelihoods (probabilities of data). Finally, probabilities \\(P(B_i|A)\\) are called posterior probabilities or simply posteriors because they represent newly acquired probabilities after observing event A (after seeing data)."
  },
  {
    "objectID": "LN3_a.html",
    "href": "LN3_a.html",
    "title": "Random Variable",
    "section": "",
    "text": "So far, we have dealt with the topic of random events. But in statistics, it is much more usual to deal with the numbers - how can we get to the theoretical concepts upon which we can build statistical models?\nThe answer is by developing the concept of random variables.\nA random variable X is a measurable function that maps from the sample space of random events to the measurable space, typically a set or interval of real numbers.\n\\[\nP(X\\in{S})=P(\\omega\\in{\\Omega}|X(\\omega)\\in{S})\n\\]\nSimply put, it defines how we get from random events to numbers.\nWe usually use capital letters from the end of the alphabet (X, Y, Z or X1, X2, …) to denote the random variable (function) itself and small letters (x, y, z or x1, x2, …) to represent numerical values of this random variable.\nRandom variables are usually divided into two classes: discrete random variable is such a variable in which the range of possible values is countable (finitely or infinitely), and we can assign a probability to individual values;\ncontinuous random variable is, on the other hand, such a variable in which the range is uncountably infinite, most commonly interval, and we cannot assign a probability to individual values."
  },
  {
    "objectID": "LN3_b.html",
    "href": "LN3_b.html",
    "title": "Cumulative Distribution Function",
    "section": "",
    "text": "The cumulative distribution function is the most universal way to describe a probability distribution of any random variable. It is defined as\n\\[\nF(x)=P(X\\leq{x})\n\\]\nThe cumulative distribution function at point x is a probability that a random variable X is lower or equal to the value x.\nThis function has several important properties:\n\nIs non-decreasing \\(x_1\\leq{x_2}\\to{F(x_1)}\\leq{F(x_2)}\\)\nAs any probability, takes values in the range \\([0,1]\\)\nFrom the two properties, we can see the limits: \\(lim_{x\\to-\\infty}F(x)=0\\) and \\(lim_{x\\to\\infty}F(x)=1\\)\nIs right-continuous\n\nThe last point is important for formally distinguishing discrete and continuous random variables."
  },
  {
    "objectID": "LN3_b.html#definition",
    "href": "LN3_b.html#definition",
    "title": "Cumulative Distribution Function",
    "section": "",
    "text": "The cumulative distribution function is the most universal way to describe a probability distribution of any random variable. It is defined as\n\\[\nF(x)=P(X\\leq{x})\n\\]\nThe cumulative distribution function at point x is a probability that a random variable X is lower or equal to the value x.\nThis function has several important properties:\n\nIs non-decreasing \\(x_1\\leq{x_2}\\to{F(x_1)}\\leq{F(x_2)}\\)\nAs any probability, takes values in the range \\([0,1]\\)\nFrom the two properties, we can see the limits: \\(lim_{x\\to-\\infty}F(x)=0\\) and \\(lim_{x\\to\\infty}F(x)=1\\)\nIs right-continuous\n\nThe last point is important for formally distinguishing discrete and continuous random variables."
  },
  {
    "objectID": "LN3_b.html#how-to-calculate-probabilities",
    "href": "LN3_b.html#how-to-calculate-probabilities",
    "title": "Cumulative Distribution Function",
    "section": "How to calculate probabilities",
    "text": "How to calculate probabilities\nOnce we have a random variable described by its cumulative distribution function, we can calculate any probability related to the random variable. In the most general way, we can write.\n\\[\nP(x_1&lt;X\\leq{x_2})=P(X\\leq{x_2})-P(X\\leq{x_1})=F(x_2)-F(x_1)\n\\]\nUsing the knowledge of limits of the cumulative distribution function, we can see special cases:\n\\[\nP(X\\leq{x_2})=P(-\\infty&lt;X\\leq{x_2})=F(x_2)-F(-\\infty)=F(x_2)-0=F(x_2)\n\\]\n\\[\nP(X&gt;x_1)=P(x_1&lt;X\\leq{\\infty})=F(\\infty)-F(x_1)=1-F(x_1)\n\\]\nBoth of these cases can also be derived from the cumulative distribution function’s definition and the complement operation.\nThe last special case is the calculation of the point probability.\n\\[\nP(X=x)=P(x&lt;X\\leq{x})= P(X\\leq{x})-P(X&lt;x)=F(x)-F(x^-)\n\\]\nWe can read it this way: the probability of a point x is the value of cumulative distribution at this point minus the value of cumulative distribution function at this point evaluated from the left."
  },
  {
    "objectID": "LN3_b.html#distinguishing-discrete-and-continuous-random-variables",
    "href": "LN3_b.html#distinguishing-discrete-and-continuous-random-variables",
    "title": "Cumulative Distribution Function",
    "section": "Distinguishing discrete and continuous random variables",
    "text": "Distinguishing discrete and continuous random variables\nThe cumulative distribution function can have points of discontinuity - at these points, values of F(x) and F(x-) are different, and there is a point probability. If the cumulative distribution function is otherwise constant, it is so called step function as it seems to look like steps. This is the characteristic outlook of the cumulative distribution function of discrete random variables.\nIf the cumulative distribution function contains no points of discontinuity, it is smooth, then the random variable is continuous.\nTheoretically, a mix can occur - the cumulative distribution function can have points of discontinuity and be smoothly increasing over some range - we will, however, not cover this specific case."
  },
  {
    "objectID": "LN3_c.html",
    "href": "LN3_c.html",
    "title": "Discrete Random Variables",
    "section": "",
    "text": "As already noted, discrete random variables are such random variables that have a countable range of possible values, and we can thus assign specific point probabilities. Its cumulative distribution function is a step function, and the probability of value x can be calculated as the height of the step of the cumulative distribution function at this point x. \\[\nP(X=x)=P(x)=(x)-F(x^-)\n\\]\nThe function assigning probabilities P(x) for all points x is called the probability function (or, in older publications, probability mass function) and is the most common way to describe the probability distribution of discrete random variables.\nSome basic properties are:\n\nIt is a probability \\(0\\leq{P(x)}\\leq1\\)\nOne of the values has to occur:\\(\\sum_{x=-\\infty}^{\\infty}P(x)=1\\)\nThe value of the cumulative distribution function can be calculated by cumulating probabilities (heights of the steps) up to the point x: \\(F(x)=P(X\\leq{x})=\\sum_{i=-\\infty}^{x}P(t)\\)"
  },
  {
    "objectID": "LN3_d.html",
    "href": "LN3_d.html",
    "title": "Continuous Random Variables",
    "section": "",
    "text": "Continuous random variables are such random variables that have an uncountably infinite range, usually an interval range, and we thus can not assign specific point probabilities. Probabilities are assigned only to the intervals. Its cumulative distribution function is smooth.\nSo, in this type of random variable, we can not have point probabilities in a classical sense. Still, we can describe point intensity as a speed of increase of cumulative distribution function at point x, thus:\n\\[f(x)=\\frac{\\partial{F(x)}}{\\partial{x}}\\]\nFunction f(x) is called a probability density function. This function is the most common way to display a probability distribution of random variables. Even though it does not represent point probabilities, it still can be used to compare the likeliness of values close to the points of interest.\nThe most important relation between the probability density function and the probability is that the probability of a specific range of values of a random variable is an area under the probability density function in the specified range:\n\\[\nP(x_1&lt;X&lt;x_2)=P(x_1&lt;X\\leq{x_2})=P(x_1\\leq{X}&lt;x_2)=P(x_1\\leq{X}\\leq{x_2})=F(x_2)-F(x_1)=\\int_{x_1}^{x_2}f(x)dx\n\\]\nSome important properties of probability density functions (compare them to properties of probability (mass) functions:\n\nIt is non-negative because the cumulative distribution function is non-decreasing: \\(f(x)\\geq0\\)\nTotal area is 1: \\(\\int_{x=-\\infty}^{\\infty}f(x)=1\\)\n\\(F(x)=P(X\\leq{x})=\\int_{t=-\\infty}^{x}f(t)\\)"
  },
  {
    "objectID": "LN3_e.html",
    "href": "LN3_e.html",
    "title": "Quantile Function",
    "section": "",
    "text": "In describing a probability distribution of random variables, we have utilised the cumulative distribution function, which is universal, the probability function for discrete random variables, and the probability density function for continuous random variables.\nThere are other ways to describe probability distribution, e.g. via survival function or hazard function. We will skip these, but one other function is important in this course: a quantile function.\n100P% quantile, denoted as xp, is such a value that a random variable can take this value or lower with a given probability P. Hence\n\\[\nP(X\\leq{x_p})=P\\to F(x_p)=P\n\\]\nThe quantile function is closely related to the cumulative distribution function; it is the inverse of it\n\\[\nx_p=Q(P)=F^{-1}(P)\n\\]\nTo rephrase it: when we know the value and want to find the yet unknown probability that the random variable is less or equal to this already known value, we use the cumulative distribution function: \\(x\\to{P}\\).\nAnd when we know the probability that the random variable is less or equal to some yet unknown value, we are solving inverse problem \\(P\\to{x}\\).\nThe cumulative distribution function must be monotonic because we need to find its inverse function. However, the cumulative distribution function is generally only non-decreasing, which means it is not necessarily monotonic.\nThe first obvious problem arises for discrete random variables, in which the cumulative distribution function is generally a step function. We will avoid this problem by simply avoiding using quantile functions for discrete random variables, even though generalisation is available.\nFor continuous random variables, we will assume it is strictly monotonic for 0 &lt; P &lt; 1. We will thus not look for 100% and 0% quantiles.\nMany quantiles have their specific names, usually derived from the number of probabilistically equal portions into which they divide the distribution. The most well-known is median x0.5, which divides the distribution into two halves with the same probability. Others include:\n\nTerciles x0.33 and x0.67\nQuartiles x0.25, x0.5, and x0.75\nQuintiles x0.2, x0.4, x0.6, and x0.8\n….\nPercentiles x0.01,x0.02,…,x0.99"
  },
  {
    "objectID": "LN4_a.html",
    "href": "LN4_a.html",
    "title": "Characteristics of Random Variables",
    "section": "",
    "text": "In the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\nWhat is this other information? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\nLocation\nVariability\nSkewness\nKurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later."
  },
  {
    "objectID": "LN4_a.html#introduction",
    "href": "LN4_a.html#introduction",
    "title": "Characteristics of Random Variables",
    "section": "",
    "text": "In the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\nWhat is this other information? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\nLocation\nVariability\nSkewness\nKurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later."
  },
  {
    "objectID": "LN4_a.html#moments",
    "href": "LN4_a.html#moments",
    "title": "Characteristics of Random Variables",
    "section": "Moments",
    "text": "Moments\n\nRaw Moments\nThe moment is a concept known from ancient Greek physics, e.g. Archimedes on the centre of gravity of a lever: “A and B are equally balanced if their distances to the centre are inversely proportional to their weights.” The centre of gravity is generally a point around which the resultant torque due to gravity forces vanishes; the object is in balance.\nLet´s imagine two persons weighing 60 and 80 kg swinging on a seesaw; the first is at point 0, and the second is at point 1. Where should the fulcrum be put so they can be balanced? According to Archimedes, their distance to the centre should be inversely proportional to their weights. So the first one should be at 8/14 of the distance to the centre, and the second one should be at 6/14 of the distance to the centre. The overall distance is 1, and the fulcrum should be at the point of 8/14. This point is the centre of gravity and the (first) moment.\nNow, how can this point be calculated? Let´s imagine a real line; values 0 and 1 represent points on this line, and weights are relative to the sum of all weights, so their sum is 1; the relative weight of the first person sitting at point 0 is \\(60/(60+80)=6/14\\) and the relative weight of the second person sitting at point 1 is \\(80/(60+80)=8/14\\). The centre of gravity value can be calculated as \\(0*6/14+1*8/14=8/14\\).\nLet´s add the third person on the seesaw and put him at point 2 with a weight of 100 kg. So the relative weights are now 60/240 = 6/24, 8/24 and 10/24. See, their total is 1. The centre of gravity value can be calculated as \\(0*6/24+1*8/24+2*10/24=28/24\\approx{1.167}\\). So, if you wanted to put a fulcrum below the seesaw for the line to be horizontal and stable, you would have to put it under the seesaw at this point.\nThis long introduction here tries to explain the (first raw) moment and why it is useful as a characteristic of the location. However, we derive various properties from three different types of moments and powers.\nThe first raw moment has been described so far, which, in probability theory, is also called expected value or simply expectation. We use values x as the points and probabilities P(x) as weights for discrete random variables. Because the sum of the P(x) equals 1, it is already relative weight. Similarly, we use the probability density function f(x) as the relative weight for continuous random variables. Still, since it is a continuous function, we must integrate instead of summing.\n\\[\nE(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n\\]\n\\[ E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nNow, the k-th raw moment is defined as the expected value of the k-th power of the random variable X:\n\\[ \\mu_k^´=E(X^k)= \\sum_{x=-\\infty}^{\\infty}x^k*P(x) \\]\n\\[ \\mu_k^´=E(X^k)=\\int_{x=-\\infty}^{\\infty}x^k*f(x)dx \\]\n\n\nCentral Moments\nWe can use central moments for some properties besides the raw moments. Centralisation is the term used to put the centre of the random variable to equal 0. This leads to the description of the random variable independent of its location.\nWe do this by subtracting the expected value from the values of the random variable: \\(x-E(X)\\). Now, the k-th central moment is defined as the expected value of the k-th power of the centralised random variable X:\n\\[ \\mu_k=E((X-E(X))^k)= \\sum_{x=-\\infty}^{\\infty}(x-E(X))^k*P(x) \\]\n\\[ \\mu_k=E((X-E(X))^k)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^k*f(x)dx \\]\n\n\nStandardised Moments\nThe last type of moment is the standardised (or normalised) moment. Proces of standardisation start with centralisation (to put the expected value equal to 0) and continues with dividing by the square root of the second central moment, which is called standard deviation. This ensures that a standardised random variable’s variance and standard deviation (moment measures of variability) equal 1, thus not influencing other properties we might desire to observe.\nNow, the k-th standardised moment is defined as the expected value of the k-th power of the standardised random variable X:\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*P(x) \\]\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\int_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*f(x)dx \\]"
  },
  {
    "objectID": "LN4_b.html",
    "href": "LN4_b.html",
    "title": "Characteristics of Location",
    "section": "",
    "text": "Characteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?"
  },
  {
    "objectID": "LN4_b.html#expected-value",
    "href": "LN4_b.html#expected-value",
    "title": "Characteristics of Location",
    "section": "Expected Value",
    "text": "Expected Value\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function P(x) or probability density function f(x).\n\\[\n\\mu_1^´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n\\]\n\\[ \\mu_1^´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nThe expected value has several important properties; for now, we stay with two of them:\n\nFor real constant c: \\(E(c)=c\\)\nLinearity of expectations: \\(E(cX)=c*E(X)\\)"
  },
  {
    "objectID": "LN4_b.html#median",
    "href": "LN4_b.html#median",
    "title": "Characteristics of Location",
    "section": "Median",
    "text": "Median\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x0.5 is 50% quantile, which means it is a value satisfying the following equation\n\\[\nF(x_{0.5})=P(X\\leq{x_{0.5}})=0.5\n\\]\n\\[\nx_{0.5}=Q(0.5)=F^{-1}(0.5)\n\\]\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\\[\nF(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5\n\\]\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves."
  },
  {
    "objectID": "LN4_b.html#mode",
    "href": "LN4_b.html#mode",
    "title": "Characteristics of Location",
    "section": "Mode",
    "text": "Mode\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value x, for which probability density function f(x) has a local maximum in the range of the random variable.\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two peaks in its probability density function without looking for the larger of the two maxima.\nApart from mode, distributions can have antimode(s); this term refers to the points x, for which the probability density function f(x) has its local minima. These points are not, however, considered characteristic of location."
  },
  {
    "objectID": "LN4_c.html",
    "href": "LN4_c.html",
    "title": "Characteristics of Variability",
    "section": "",
    "text": "Characteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?"
  },
  {
    "objectID": "LN4_c.html#variance-and-standard-deviation",
    "href": "LN4_c.html#variance-and-standard-deviation",
    "title": "Characteristics of Variability",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable \\(x-E(X)\\). This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\\[E((x-E(X))^1)=\\mu_1\\]This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have “distances to the centre inversely proportional to their weights”.\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\\[\n\\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x)\n\\]\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx \\]\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\\[Var(X)=E(X^2)-[E(X)]^2\\]Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\nSome important properties of the variance are:\n\nNon-negativity \\(Var(X)\\geq{0}\\)\nConstant (real value c) has zero variance \\(Var(c)=0\\)\nFor real constants a and c: \\(Var(a+cX)=c^2Var(X)\\)\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in cm, then the variance of the height would be measured in cm2.\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a typical deviation in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\\[\nS.D.(X)=\\sqrt{Var(X)}\n\\]"
  },
  {
    "objectID": "LN4_c.html#mean-absolute-deviation",
    "href": "LN4_c.html#mean-absolute-deviation",
    "title": "Characteristics of Variability",
    "section": "Mean Absolute Deviation",
    "text": "Mean Absolute Deviation\nAnother possibility to deal with the problem, that the first central moment is equal to 0, is to use absolute deviations instead of the squared deviations:\n\\[\nMAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x)\n\\]\n\\[ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx \\]\nIn probability theory, mean absolute deviation most commonly refer to the deviation from the expected value. This way of measuring variability is a more common sense typical deviation in the probability distribution than standard deviation.\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tigthly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to expected value.\n\nIntermezzo\nIf we were looking for the real valued constant c, for which probability weighted sum of squared deviations from such a constant \\(\\sum_{x=-\\infty}^\\infty (x-c)^2*P(x)\\) is minimal, than it would be expected value \\(c=E(X)\\). So the variance is minimized, if we use expected value as the location characteristic. This has far-reaching consquences e.g. for the linear regression, in which conditional expected values are modeled by linear combination of variables and the optimal constants in the model are found by minimizing residual variance. But do not get disturbed too much here with thoughts of linear regression, it is not scope of this course.\n\n\nBack to MAD\nUsing the same logic as in the intermezzo, what real valued constant c would minimize probability weighted sum of absolute deviations \\(\\sum_{x=-\\infty}^\\infty |x-c|*P(x)\\) ? Maybe surprisingly, the constant in this case is not expected value, but median \\(c=x_{0.5}\\).\nSo by the same logic, in which expected value is connected to the variance, mean absolute deviation should be connected to the median. And by the same logic, when modelling conditional medians by linear combination of variables, the optimal constants are found by minimizing residual mean absolute deviations - this type of modelling is called median (or more generally quantile) regression. Again, do not get disturbed with this.\nSo what, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Does it make your head hurt? Sorry, no one promised you it will be always easy.\nIn this course, we will use expected value as a measure of location when calculating mean absolute deviation, so follow formulas at the beginning of this sub-chapter."
  },
  {
    "objectID": "LN4_c.html#inter-quartile-range",
    "href": "LN4_c.html#inter-quartile-range",
    "title": "Characteristics of Variability",
    "section": "Inter-quartile range",
    "text": "Inter-quartile range\nApart from measuring variability by moment-based approach, we can also use quantile-based approach. We can measure ranges between pre-specified quantiles, and use them as a measure of variability with probabilistic meaning. The one that we will stick with in the course is inter-quartile range\n\\[\nIQR(X)=x_{0.75}-x_{0.25}\n\\]\nInter-quartile range is the range (distance) that is needed to capture middle 50 % of the probability distribution."
  },
  {
    "objectID": "LN4_c.html#entropy",
    "href": "LN4_c.html#entropy",
    "title": "Characteristics of Variability",
    "section": "Entropy",
    "text": "Entropy\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures expected ability to surprise. It starts with negative logarithmic transformation of probability \\(-log_b(P(x))\\). Value with probability 0 can be viewed as impossible, or infinitely surprising and indeed \\(-log_b(0)=Inf\\). On the second part of the scale, value with probability 1 is sure to happen, or totally unsurprising and indeed \\(-log_b(1)=0\\). So the initial transformation gives us potential measure of surprisingness and entropy measures expected value of this\n\\[\nH(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x))\n\\]\nNotice, that introduced measure of entropy is based on the probabilities and hence is suitable for disrecete random variables only. The trouble with continuous random variables is, that \\(f(x)\\geq0\\) and if the value of probability density is above 1, the logarithm is positive and entropy can be negative. So we will skip problematic of measuring entropy of continuous random variables and use it only as a measure for discrete random variables.\nNow, what units is entropy measured in? It depends on the base of logarithm - b. The most natural base, the one used in this course, is 2. Then entropy is measured in bits. But other bases can be used, e.g. base 10 leads to units called dits or base e leads to units called nats.\nThe little trouble with entropy is, that its value depends on the number of possible values x of random variable. So for comparison among random variables with different number of possible values of random variable (k), normalised entropy, called efficiency, can be used\n\\[\n\\eta(X)=\\frac{H(X)}{log_b(k)}\n\\]\nNow, there is a different way to view entropy. Let´s stick to b = 2, that is entropy measured in bits. Then entropy is expected number of Yes/No questions needed to find the result of a random trial.\nAs an demonstration: there are 8 horses in a race. You want to transfer information about the winner of the race. Assume all horses are equally likely to win, than the order of the questions does not matter: If horse A wins, the answer is 1 (coding Yes) and message needs to be only 1 bit long. If horse B wins, the answer is 01 (coding No, Yes) and message need to be 2 bits long."
  },
  {
    "objectID": "LN4_d.html",
    "href": "LN4_d.html",
    "title": "Other Characteristics",
    "section": "",
    "text": "Apart from location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis and the most common way of measuring them are based on standardized moments. These were already introduced, so just to repeat, these are moments based on the k-th powers of standardized random variables, where the standardization leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\nSkewness is the 3rd standardized moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nOperationally, we can distinguish the skewness, or the lean of the distribution into three possibilitites:\n\nPositive skewness - distribution is skewed towards right, so the larger part of the mass is on the left\nNull skewness - distribution is symmetric\nNegative skewness - distribution is skewed towards left, so the larger part of the mass is on the right\n\nKurtosis is the 4th standardized moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nWhile skewness is somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of the centralization - larger the mass is in the centre, the larger kurtosis should be. But nowadays prevailing view is, that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\nDoes it make sense? On the first sight, having larger mass in the centre and in the tails is contrarian, but in many probability distributions, it is actually true. What is lacking in these distributions is the mass somewhere in between centre and the tails.\nOperationally, we can distinguish the kurtosis into three possibilitites by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\nLeptokurtic: \\(\\alpha_4&gt;3\\)\nMesokurtic: \\(\\alpha_4=3\\)\nPlatykurtic : \\(\\alpha_4&lt;3\\)\n\nDue to the direct comparability to the kurtosis of normal distribution, sometimes the excess kurtosis \\(\\alpha_4-3\\) is being used."
  },
  {
    "objectID": "LN5_a.html",
    "href": "LN5_a.html",
    "title": "Random Vectors",
    "section": "",
    "text": "So far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now p-dimensional sets or intervals of real numbers.\nSimply put, if one random event leads to p numbers, we are dealing with the random vector X and the realisation of this random vector x.\nWhile random vectors can have \\(p\\in\\mathbb{N}\\) dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\\[\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n\\]\nRandom vectors have distributions that can be described jointly, marginally or conditionally."
  },
  {
    "objectID": "LN5_a.html#introduction",
    "href": "LN5_a.html#introduction",
    "title": "Random Vectors",
    "section": "",
    "text": "So far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now p-dimensional sets or intervals of real numbers.\nSimply put, if one random event leads to p numbers, we are dealing with the random vector X and the realisation of this random vector x.\nWhile random vectors can have \\(p\\in\\mathbb{N}\\) dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\\[\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n\\]\nRandom vectors have distributions that can be described jointly, marginally or conditionally."
  },
  {
    "objectID": "LN5_a.html#joint-distribution",
    "href": "LN5_a.html#joint-distribution",
    "title": "Random Vectors",
    "section": "Joint distribution",
    "text": "Joint distribution\nThe joint distribution of a random vector is described as an intersection. Translated, the joint cumulative distribution function for a two-dimensional random vector is\n\\[\nF(x_1,x_2)=P(X_1\\leq{x_1}\\cap X_2\\leq{x_2})\n\\]\njoint probability funcion is\n\\[ P(x_1,x_2)=P(X_1={x_1}\\cap X_2={x_2}) \\]\nand joint probability density function is\n\\[ f(x_1,x_2)=f(X_1={x_1}\\cap X_2={x_2}) \\]\nWe are omitting a case of a mixed random vector (continuous and discrete random variables in one random vector).\nRelations between the mentioned descriptions of joint probability distributions are generalisations of the already seen relations in the random variables\n\\[ F(x_1,x_2)=\\sum_{t=-\\infty}^{x_1}\\sum_{u=-\\infty}^{x_2}P(x_1,x_2)\\]\n\\[\nF(x_1,x_2)=\\int_{t=-\\infty}^{x_1}\\int_{u=-\\infty}^{x_2}f(x_1,x_2)dx_2dx_1\n\\]\n\\[\nf(x_1,x_2)=\\frac{\\partial\\partial F(x_1,x_2)}{\\partial x_1 \\partial x_2}\n\\]"
  },
  {
    "objectID": "LN5_a.html#marginal-distribution",
    "href": "LN5_a.html#marginal-distribution",
    "title": "Random Vectors",
    "section": "Marginal distribution",
    "text": "Marginal distribution\nMarginal distribution refers to any subset of the originating random vector independent of the omitted part of that random vector. Say we have a 5-dimensional random vector:\n\\[\n\\textbf{X}=\n\\begin{pmatrix}\nX_1\\\\\nX_2\\\\\nX_3\\\\\nX_4\\\\\nX_5\\\\\n\\end{pmatrix}\n\\]\nThen, all the following random vectors and random variables can be called marginal vectors or variables (this is not a complete enumeration, just examples):\n\\[ \\textbf{X}= \\begin{pmatrix} X_1\\\\ X_2\\\\ X_3\\\\ X_4\\\\\\end{pmatrix},\n\\textbf{X}= \\begin{pmatrix} X_1\\\\ X_4 \\end{pmatrix},\nX_1 \\]\nMarginal distributions can be obtained from the joint distribution. Using the probability function, one needs to sum joint probabilities across the variables he is getting rid of.\nThe idea explained in two dimensions is this: You want, e.g. the probability P(X = 1); this probability is a union of all the joint probabilities containing X = 1, e.g. P(X1 = 1, X2 = 0), P(X1 = 1, X2 = 1), P(X1 = 1, X2 = 2). Because these intersections are disjoint random events, the union is calculated as a simple summation.\n\\[\nP(x_1)=\\sum_{x_2=-\\infty}^\\infty P(x_1, x_2)\n\\]\nThe same logic applies to continuous random variables described by a probability density function but using continuous mathematics. i.e. integration\n\n\\[\nf(x_1)=\\int_{x_2=-\\infty}^\\infty f(x_1, x_2)dx_2\n\\]\nLast but not least comes the relation between marginal and joint cumulative distribution function, which is straightforward\n\\[\nF(x_1)= P(X_1 \\leq x_1)=P(X_1 \\leq x_1 \\cap X_2 &lt; \\infty)= F(x_1,x_2=\\infty)\n\\]\nHaving marginal distributions of the random variables, all their characteristics can be calculated as usual. We will use marginal expected values and marginal variances for the characteristics of a random vector."
  },
  {
    "objectID": "LN5_a.html#joint-characteristics",
    "href": "LN5_a.html#joint-characteristics",
    "title": "Random Vectors",
    "section": "Joint characteristics",
    "text": "Joint characteristics\nJoint characteristics, or just characteristics of a random vector, characterise specific properties of a random vector as a whole. Again, for the needs of this course, we simplify and use only the most common characteristics.\nThe location of a random vector is commonly characterised by a vector of expected values, which is a vector composed of marginal expected values of marginal random variables. In two dimensions:\n\\[\nE(\\textbf{X})= \\begin{pmatrix} E(X_1)\\\\ E(X_2) \\end{pmatrix}\n\\]\nA variance-covariance matrix commonly characterises the variability of a random vector. In this matrix, marginal variances are used together with covariances.\n\nCovariance\nCovariance is a measure of the linear relation between the two random variables. It can take any real value; if it is negative, the two random variables are negatively linearly associated; if it is positive, the two random variables are positively linearly associated; if it is zero, the two random variables are not linearly associated. The word linearly is important here - random variables can have non-linear associations, which can be visible using, e.g. regression or skedastic functions. Linearly can be viewed as on average. This will be discussed later in this chapter.\nMathematically, covariance is calculated from the definition\n\\[\ncov(X_1,X_2)=E[(X_1-E(X_1))*(X_2-E(X_2))]\n\\]\nwhich can be transformed into the evaluation formula\n\\[\ncov(X_1, X_2)=E(X_1X_2)-E(X_1)E(X_2)\n\\]\nTo calculate covariance, one can calculate the expected value of the product of the two random variables and subtract the product of their expected values.\nWhat if you wanted to calculate the covariance of the random variable with itself?\n\\[\ncov(X_1,X_1)=E[(X_1-E(X_1))*(X_1-E(X_1))]=E(((X_1-E(X_1))^2\n)=var(X_1)\\]\nor\n\\[\ncov(X_1,X_1)=E(X_1X_1)-E(X_1)E(X_1)=E(X_1^2)-(E(X_1))^2=Var(X_1)\n\\]\nVariance is the specific case of the covariance; thus, the variance-covariance matrix describes all possible covariances.\nThe last important property of covariance is its symmetry; from the evaluation formula\n\\[\ncov(X_1,X_2)=E(X_1X_2)-E(X_1)E(X_2)=E(X_2X_1)-E(X_2)E(X_1)=cov(X_2,X_1)\n\\]\nThe variance-covariance matrix is thus square and symmetric, having marginal variances on the main diagonal. It is also positive semi-definite, an important property for various statistical techniques using sample variance-covariance matrices.\n\\[\n\\boldsymbol{\\Sigma}= \\begin{pmatrix} Var(X_1) & cov(X_1,X_2) \\\\\ncov(X_2,X_1) & Var(X_2)\n\\end{pmatrix}\n\\]\n\n\nCorrelation\nThe covariance describes the direction of the linear association, but its value is unbounded and depends on the units of the random variables. Thus, we cannot compare values of the covariances across different pairs of random variables. Here comes the standardised version of the covariance that bounds its values to the range [-1; 1]. This standardised covariance is called correlation and is calculated as\n\\[\n\\rho(X_1,X_2)=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\frac{cov(X_1,X_2)}{S.D.(X_1)S.D.(X_2)}\n\\]\nThe standardisation leads to a unitless value, which can be compared across different pairs of random variables. Value 0 still means a linearly independent pair of random variables; value -1 means that one random variable is a negative linear function of the other, and value 1 indicates that one random variable is a positive linear function of the other.\nMost commonly, the correlation is somewhere in between. There are no hard bounds to a strong or weak association, even though you might find some simplifying tables with given bounds in various scientific fields. These tables are usually created by looking at the common correlations in the given field. For example, in social sciences, a correlation of 0.6 is typically referred to as strong simply because it is very unusually high value in the social sciences. On the other hand, in physics, a correlation of 0.6 would be considered weak because the correlation coefficients are typically higher than that in physics.\nCorrelation is also connected to the concept of explained variability in the linear regression analysis. This connection will be described in the chapter on two-dimensional normal distribution.\nThe correlation of the random variable with itself yields a value of 1\n\\[\n\\rho(X_1,X_1)=\\frac{cov(X_1,X_1)}{\\sqrt{Var(X_1)*Var(X_1)}}=\n\\frac{Var(X_1)}{Var(X_1)}=1\n\\]\nand the correlation is again symmetric:\n\\[\n\\rho(X_2,X_1)=\\frac{cov(X_2,X_1)}{\\sqrt{Var(X_2)*Var(X_1)}}=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\rho(X_1,X_2)\n\\]\nThis together means that the correlation matrix is squared and symmetric, having values one on the main diagonal. This matrix is also a positive semidefinite\n\\[\n\\boldsymbol{\\rho}= \\begin{pmatrix}\n1 & \\rho(X_1, X_2) \\\\\n\\rho(X_2, X_1) & 1\n\\end{pmatrix}\n\\]\nTo summarise, a random vector’s two main characteristics are a vector of expected values, which describes the location, and a variance-covariance matrix, which describes variability and linear relations inside the vector. The third commonly used characteristic is a correlation matrix, which recalculates a variance-covariance matrix to a standardised form, describing the linear relations inside the vector."
  },
  {
    "objectID": "LN5_a.html#conditional-distribution",
    "href": "LN5_a.html#conditional-distribution",
    "title": "Random Vectors",
    "section": "Conditional distribution",
    "text": "Conditional distribution\nThe joint distribution represents the intersection, and the marginal distribution represents the independent distribution so that the last distribution will represent the conditional distribution. The notation is changed; instead of x1 and x2, x and y will be used for better clarity.\nIt is calculated in a similar way we saw with the two random events, with the probability of intersection divided by the probability of a condition:\n\\[\nP(x|y)=\\frac{P(x,y)}{P(y)}\n\\]\n\\[ f(x|y)=\\frac{f(x,y)}{f(y)} \\]\nSo, having joint distribution, we can calculate marginal distribution. And having both joint distribution and marginal distribution, we can calculate conditional distribution. If there is an association between the two variables, the conditional probability function or conditional probability density function will be a function of both the variable (x) and the condition (y).\n\nRegression function\nThe regression function in probability theory is a function of the expected value of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) E(X|y). It is calculated the very same way as any expected value, but using conditional probability function or probability density function\n\\[\nE(X|y)=\\sum_{x=-\\infty}^\\infty x*P(x|y)\n\\]\n\\[ E(X|y)=\\int_{x=-\\infty}^\\infty x*f(x|y)dx \\]\nMathematically, we eliminate X but not Y in the result by integrating or summing across values of random variable X. The conditional expected value E(X|y) is usually, but not necessarily, a function of Y in case there is a relation between the two variables. The conditional expected values are always constants if no association exists between X and Y.\n\n\nSkedastic function\nThe skedastic function in probability theory is a function of the variance of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) Var(X|y). It is calculated the very same way as any variance, but using conditional probability function or probability density function\n\\[ E(X^2|y)=\\sum_{x=-\\infty}^\\infty x^2*P(x|y) \\]\n\\[ E(X^2|y)=\\int_{x=-\\infty}^\\infty x^2*f(x|y)dx \\]\n\\[\nVar(X|y)=E(X^2|y)-[E(X|y)]^2\n\\]\nThe conditional variance Var(X|y) is usually, but not necessarily, a function of Y in case there is a relation between the two variables. The conditional variances are always constants if no association exists between X and Y."
  },
  {
    "objectID": "LN5_a.html#independence",
    "href": "LN5_a.html#independence",
    "title": "Random Vectors",
    "section": "Independence",
    "text": "Independence\nWe stumbled upon association and linear independence notions throughout the previous paragraphs. Let´s now formalise it a little.\nIn the same way, we introduced the definition of independence of two random events, we can start here\n\\(P(x|y)=P(x);f(x|y)=f(x);F(x|y)=F(x)\\) for each possible value x and y.\nBasically, we claim that the two random variables are independent if the conditioning on one random variable does not change the probability distribution of the second one.\nFrom this, we can follow with the condition of independence,\n\\[\nP(x,y)=P(x)*P(y)\n\\]\n\\[\nf(x,y)=f(x)*f(y)\n\\]\n\\[\n, soF(x,y)=F(x)*F(y)\n\\]\nThis condition is both necessary and sufficient, meaning that if we can show one of these relations, the two random variables are independent, and if the two random variables are independent, then we can assume these relations.\nCovariance and correlation were introduced as measures of linear association. The regression function was introduced to describe how one random variable’s expected values react to the other’s values. Similarly, the skedastic function describes conditional variances. What are some takeaway points on these descriptions of relations and general independence?\n\nA linear relation can be described as a relation in which the regression function is close to the straight line.\nThe regression function can take any shape, and no linearity is required (!). If the regression function is, on average, increasing, the correlation is positive, and if the regression function is, on average,decreasing, the correlation is negative.\nA higher correlation value is connected to the regression function being close to the straight line and the skedastic function being close to 0.\nIf one random variable is a linear function of the other, then the regression is a straight line, and there is no conditional variability, so the skedastic function is equal to 0.\nIf the two random variables are independent, they are also linearly independent, so covariance and correlation are 0. Also, regression and skedastic functions are constant.\nThe opposite is not necessarily true! There can be non-linear relations such that covariance and correlation equal 0. Two random variables can even be dependent in such a way that regression or skedastic function is constant."
  },
  {
    "objectID": "LN5_a.html#mutual-information",
    "href": "LN5_a.html#mutual-information",
    "title": "Random Vectors",
    "section": "Mutual Information",
    "text": "Mutual Information\nEven though the correlation coefficient is the most commonly used measure of the strength of the relation, it has a few flaws, most notably a connection to linearity and a less obviously hard interpretation of values. However, there are other ways to measure the strength of the association of the two random variables.\nThe one introduced here is connected to the information theory and notion of entropy introduced in the chapter on characteristics of random variables. We will continue using this characteristic only to measure the strength of the association of discrete random vectors.\nStatistically speaking, mutual information is measured as the Kullback-Leibler divergence (type of distance measure) between the joint probability distribution and probability distribution under the condition of independence. Kullback-Leibler divergence of one random variable from the other is\n\\[\nD_{KL}(P||Q)=\\sum_{x\\in{X}}P(x)*log \\left( \\frac{P(x)}{Q(x)} \\right)\n\\]\nUsing joint probability function \\(P(x)=P(x,y)\\) and condition of independence \\(Q(x)=P(x)*P(y)\\), we have definition of mutual information\n\\[\nI(X;Y)=\\sum_{y\\in{Y}} \\sum_{x\\in{X}}P(x,y)*log \\left(\n\\frac{P(x,y)} {P(x)*P(y)}\n\\right)\n\\]\nApart from the definition, mutual entropy can be calculated using various formulas. One of them uses entropies of marginal random variables and joint random vector\n\\[\nI(X;Y)=H(X)+H(Y)-H(X;Y)\n\\]\nThe entropy of the random vector is calculated the same way the entropy of random variables\n\\[\nH(X;Y)=-\\sum_{y\\in{Y}} \\sum_{x \\in {X}}P(x,y)*log_b{P(x,y)}\n\\]\nThe formula for calculating mutual information from the entropies shows that mutual information is the information content of both random variables minus the information content of their joint distribution.\nTranslated, it means that mutual information quantifies how much knowing the value of one random variable reduces uncertainty about the other random variable, measured, e.g. in bits, nats or dits.\n\n\n\nMutual information\n\n\nThe mutual information has a clear advantage in covering all types of relations between the two random variables, not only linear. Also, if mutual information is 0, the two random variables are independent. It is also symmetric: \\(I(X;Y)=I(Y;X)\\).\n\nUncertainty coefficients\nMutual information provides information gain in one variable by observing the second variable in bits (or other units depending on the base of logarithms used). The trouble is that the maximal information content depends on the number of possible values of the random variable, as discussed in the chapter on entropy. So sometimes, the need to use the normalised version that can provide us with relative information gain might be useful. The logic behind the uncertainty coefficient is that it will tell us what information gain is in one random variable by knowing the value of the second random variable provided as the percentage of the entropy, and thus maximal possible information gain, of that first random variable.\n\\[\nU(X|Y)=\\frac{I(X,Y)}{H(X)}\\neq U(Y|X)=\\frac{I(Y,X)}{H(Y)}\n\\]\nUncertainty coefficients are not generally symmetric because the two random variables usually have different entropies.\nTo repeat:\n\nAbsolute information gain, mutual entropy, is symmetric.\nRelative information gain, uncertainty coefficient, is not (generally) symmetric.\n\nTo finish this chapter, let´s introduce one of the possible symmetric uncertainty coefficients. This answers the question, what is an average relative information gain in one variable by knowing the value of the second variable, assuming we do not know which of the two is known?\n\\[\nU(X,Y)=\\frac{H(X)*U(X|Y)+H(Y)*U(Y|X)}{H(X)+H(Y)}=\\frac{2*I(X,Y)}{H(X)+H(y)}\n\\]\nThis symmetric uncertainty coefficient is calculated as an entropy-weighted uncertainty coefficient."
  },
  {
    "objectID": "LN4_a.html#characteristics-of-location",
    "href": "LN4_a.html#characteristics-of-location",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Location",
    "text": "Characteristics of Location\nCharacteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?\n\nExpected Value\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function P(x) or probability density function f(x).\n\\[ \\mu_1^´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x) \\]\n\\[ \\mu_1^´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nThe expected value has several important properties; for now, we stay with two of them:\n\nFor real constant c: \\(E(c)=c\\)\nLinearity of expectations, for real constants a and c: \\(E(a+cX)=a+c*E(X)\\)\n\n\n\nMedian\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x0.5 is 50% quantile, which means it is a value satisfying the following equation\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})=0.5 \\]\n\\[ x_{0.5}=Q(0.5)=F^{-1}(0.5) \\]\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5 \\]\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves.\n\n\nMode\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value x, for which probability density function f(x) has a local maximum in the range of the random variable.\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two peaks in its probability density function without looking for the larger of the two maxima.\nApart from mode, distributions can have antimode(s); this term refers to the points x, for which the probability density function f(x) has its local minima. These points are not, however, considered characteristic of location."
  },
  {
    "objectID": "LN4_a.html#characteristics-of-variability",
    "href": "LN4_a.html#characteristics-of-variability",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Variability",
    "text": "Characteristics of Variability\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\nVariance and standard deviation\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable \\(x-E(X)\\). This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\\[E((x-E(X))^1)=\\mu_1\\]This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have “distances to the centre inversely proportional to their weights”.\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x) \\]\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx \\]\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\\[Var(X)=E(X^2)-[E(X)]^2\\]Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\nSome important properties of the variance are:\n\nNon-negativity \\(Var(X)\\geq{0}\\)\nConstant (real value c) has zero variance \\(Var(c)=0\\)\nFor real constants a and c: \\(Var(a+cX)=c^2Var(X)\\)\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in cm, then the variance of the height would be measured in cm2.\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a typical deviation in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\\[ S.D.(X)=\\sqrt{Var(X)} \\]\n\n\nMean Absolute Deviation\nAnother possibility to deal with the problem that the first central moment is equal to 0 is to use absolute deviations instead of squared deviations:\n\\[ MAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x) \\]\n\\[ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx \\]\nIn probability theory, mean absolute deviation most commonly refers to the deviation from the expected value. This way of measuring variability is a more common sense typical deviation in the probability distribution than standard deviation.\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tightly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to the expected value.\n\nIntermezzo\nIf we were looking for the real-valued constant c, for which the probability-weighted sum of squared deviations from such a constant ∑𝑥=−∞∞(𝑥−𝑐)2∗𝑃(𝑥) is minimal, then it would be the \\(c=E(X)\\). So, the variance is minimised if we use the expected value as the location characteristic. This has far-reaching consequences, e.g. for linear regression, in which conditional expected values are modelled by a linear combination of variables and the optimal constants in the model are found by minimising residual variance. But do not get disturbed too much here with thoughts of linear regression; it is not the scope of this course.\n\n\nBack to MAD\nUsing the same logic as in the intermezzo, what real-valued constant c would minimise the probability-weighted sum of absolute deviations \\(\\sum_{x=-\\infty}^\\infty |x-c|*P(x)\\)? Maybe surprisingly, the constant is not the expected value in this case, but median \\(c=x_{0.5}\\).\nSo, using the same logic, in which the expected value is connected to the variance, the mean absolute deviation should be connected to the median. And by the same reasoning, when modelling conditional medians by a linear combination of variables, the optimal constants are found by minimising residual mean absolute deviations. This type of modelling is called median (or, more generally, quantile) regression. Again, do not get disturbed by this.\nSo, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Even the median absolute deviation shares the same acronym, MAD. Does it make your head hurt? Sorry, no one promised you it would always be easy.\nIn this course, we will use the expected value to measure location when calculating mean absolute deviation, so we will follow the formulas at the beginning of this sub-chapter.\n\n\n\nInter-quartile range\nApart from measuring variability by a moment-based approach, we can also use a quantile-based approach. We can measure ranges between pre-specified quantiles and use them to measure variability with probabilistic meaning. The one that we will stick to within the course is the inter-quartile range\n\\[ IQR(X)=x_{0.75}-x_{0.25} \\]\nThe inter-quartile range is the range (distance) needed to capture the middle 50 % of the probability distribution.\n\n\nEntropy\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures the random variable’s expected ability to surprise or information content.\nIt starts with a negative logarithmic transformation of probability \\(-log_b(P(x))\\). Value with probability 0 can be viewed as impossible or infinitely surprising, and indeed \\(-log_b(0)=Inf\\). On the second part of the scale, a value with probability 1 is sure to happen or totally unsurprising and indeed \\(-log_b(1)=0\\). So, the initial transformation gives us the potential measure of surprisingness, and entropy measures the expected value of this\n\\[ H(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x)) \\]\nNotice that the introduced entropy measure is based on the probabilities and is suitable for only discrete random variables. The trouble with continuous random variables is that 𝑓(𝑥)≥0, and if the probability density value is above 1, the logarithm is positive, and entropy can be negative. So, we will skip the problem of measuring the entropy of continuous random variables and use it only as a measure for discrete random variables.\nNow, what units is entropy measured in? It depends on the base of the logarithm - b. The most common base, the one used in this course, is 2. Then, entropy is measured in bits. However, other bases can be used, e.g., base 10 leads to units called dits, or base e leads to units called nats.\nThe little trouble with entropy is, that its value depends on the number of possible values x of random variable. Maximum entropy can be achieved if all the values of the random variable are equally likely (so-called discrete uniform distribution).\nAssume there are k possible values, each with probability \\(P(x)=\\frac{1}{k}\\), then maximal entropy is\n\\[\nH(X)=-\\sum_{x=- \\infty}^{\\infty}P(x)*log_b P(x)=-k*\\frac{1}{k}*log_b \\frac {1}{k}=log_b(k)\n\\]\nSo for comparison among random variables with different numbers of possible values of random variable (k), normalised entropy, called efficiency, can be used\n\\[ \\eta(X)=\\frac{H(X)}{log_b(k)} \\]"
  },
  {
    "objectID": "LN4_a.html#skewness",
    "href": "LN4_a.html#skewness",
    "title": "Characteristics of Random Variables",
    "section": "Skewness",
    "text": "Skewness\nApart from the location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis, and the most common way of measuring them is based on standardised moments. These were already introduced, so to repeat, these are moments based on the k-th powers of standardised random variables, where the standardisation leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\nMoment measure of skewness is the 3rd standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nOperationally, we can distinguish the skewness or the lean of the distribution into three possibilities:\n\nPositive skewness - distribution is skewed towards the right, so the larger part of the mass is on the left.\nNull skewness - distribution is symmetric.\nNegative skewness - distribution is skewed towards the left, so the larger part of the mass is on the right."
  },
  {
    "objectID": "LN4_a.html#kurtosis",
    "href": "LN4_a.html#kurtosis",
    "title": "Characteristics of Random Variables",
    "section": "Kurtosis",
    "text": "Kurtosis\nThe moment measure of kurtosis is the 4th standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nWhile skewness is a somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of centralisation - the larger the mass is in the centre, the larger the kurtosis should be. But, nowadays, the prevailing view is that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\nDoes it make sense? At first sight, having a larger mass in the centre and the tails is contradictory, but it is true in many probability distributions. What is lacking in these distributions is the mass somewhere in between the centre and the tails.\nOperationally, we can distinguish the kurtosis into three possibilities by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\nLeptokurtic: \\(\\alpha_4&gt;3\\)\nMesokurtic: \\(\\alpha_4=3\\)\nPlatykurtic: \\(\\alpha_4&lt;3\\)\n\nDue to the direct comparability to the normal distribution kurtosis, the excess kurtosis 𝛼4−3 is sometimes used."
  },
  {
    "objectID": "LN4.html",
    "href": "LN4.html",
    "title": "Random Vectors",
    "section": "",
    "text": "So far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now p-dimensional sets or intervals of real numbers.\nSimply put, if one random event leads to p numbers, we are dealing with the random vector X and the realisation of this random vector x.\nWhile random vectors can have \\(p\\in\\mathbb{N}\\) dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\\[\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n\\]\nRandom vectors have distributions that can be described jointly, marginally or conditionally."
  },
  {
    "objectID": "LN4.html#introduction",
    "href": "LN4.html#introduction",
    "title": "Random Vectors",
    "section": "",
    "text": "So far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now p-dimensional sets or intervals of real numbers.\nSimply put, if one random event leads to p numbers, we are dealing with the random vector X and the realisation of this random vector x.\nWhile random vectors can have \\(p\\in\\mathbb{N}\\) dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\\[\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n\\]\nRandom vectors have distributions that can be described jointly, marginally or conditionally."
  },
  {
    "objectID": "LN4.html#moments",
    "href": "LN4.html#moments",
    "title": "Characteristics of Random Variables",
    "section": "Moments",
    "text": "Moments\n\nRaw Moments\nThe moment is a concept known from ancient Greek physics, e.g. Archimedes on the centre of gravity of a lever: “A and B are equally balanced if their distances to the centre are inversely proportional to their weights.” The centre of gravity is generally a point around which the resultant torque due to gravity forces vanishes; the object is in balance.\nLet´s imagine two persons weighing 60 and 80 kg swinging on a seesaw; the first is at point 0, and the second is at point 1. Where should the fulcrum be put so they can be balanced? According to Archimedes, their distance to the centre should be inversely proportional to their weights. So the first one should be at 8/14 of the distance to the centre, and the second one should be at 6/14 of the distance to the centre. The overall distance is 1, and the fulcrum should be at the point of 8/14. This point is the centre of gravity and the (first) moment.\nNow, how can this point be calculated? Let´s imagine a real line; values 0 and 1 represent points on this line, and weights are relative to the sum of all weights, so their sum is 1; the relative weight of the first person sitting at point 0 is \\(60/(60+80)=6/14\\) and the relative weight of the second person sitting at point 1 is \\(80/(60+80)=8/14\\). The centre of gravity value can be calculated as \\(0*6/14+1*8/14=8/14\\).\nLet´s add the third person on the seesaw and put him at point 2 with a weight of 100 kg. So the relative weights are now 60/240 = 6/24, 8/24 and 10/24. See, their total is 1. The centre of gravity value can be calculated as \\(0*6/24+1*8/24+2*10/24=28/24\\approx{1.167}\\). So, if you wanted to put a fulcrum below the seesaw for the line to be horizontal and stable, you would have to put it under the seesaw at this point.\nThis long introduction here tries to explain the (first raw) moment and why it is useful as a characteristic of the location. However, we derive various properties from three different types of moments and powers.\nThe first raw moment has been described so far, which, in probability theory, is also called expected value or simply expectation. We use values x as the points and probabilities P(x) as weights for discrete random variables. Because the sum of the P(x) equals 1, it is already relative weight. Similarly, we use the probability density function f(x) as the relative weight for continuous random variables. Still, since it is a continuous function, we must integrate instead of summing.\n\\[\nE(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n\\]\n\\[ E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nNow, the k-th raw moment is defined as the expected value of the k-th power of the random variable X:\n\\[ \\mu_k^´=E(X^k)= \\sum_{x=-\\infty}^{\\infty}x^k*P(x) \\]\n\\[ \\mu_k^´=E(X^k)=\\int_{x=-\\infty}^{\\infty}x^k*f(x)dx \\]\n\n\nCentral Moments\nWe can use central moments for some properties besides the raw moments. Centralisation is the term used to put the centre of the random variable to equal 0. This leads to the description of the random variable independent of its location.\nWe do this by subtracting the expected value from the values of the random variable: \\(x-E(X)\\). Now, the k-th central moment is defined as the expected value of the k-th power of the centralised random variable X:\n\\[ \\mu_k=E((X-E(X))^k)= \\sum_{x=-\\infty}^{\\infty}(x-E(X))^k*P(x) \\]\n\\[ \\mu_k=E((X-E(X))^k)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^k*f(x)dx \\]\n\n\nStandardised Moments\nThe last type of moment is the standardised (or normalised) moment. Proces of standardisation start with centralisation (to put the expected value equal to 0) and continues with dividing by the square root of the second central moment, which is called standard deviation. This ensures that a standardised random variable’s variance and standard deviation (moment measures of variability) equal 1, thus not influencing other properties we might desire to observe.\nNow, the k-th standardised moment is defined as the expected value of the k-th power of the standardised random variable X:\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*P(x) \\]\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\int_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*f(x)dx \\]"
  },
  {
    "objectID": "LN4.html#characteristics-of-location",
    "href": "LN4.html#characteristics-of-location",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Location",
    "text": "Characteristics of Location\nCharacteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?\n\nExpected Value\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function P(x) or probability density function f(x).\n\\[ \\mu_1^´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x) \\]\n\\[ \\mu_1^´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nThe expected value has several important properties; for now, we stay with two of them:\n\nFor real constant c: \\(E(c)=c\\)\nLinearity of expectations, for real constants a and c: \\(E(a+cX)=a+c*E(X)\\)\n\n\n\nMedian\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x0.5 is 50% quantile, which means it is a value satisfying the following equation\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})=0.5 \\]\n\\[ x_{0.5}=Q(0.5)=F^{-1}(0.5) \\]\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5 \\]\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves.\n\n\nMode\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value x, for which probability density function f(x) has a local maximum in the range of the random variable.\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two peaks in its probability density function without looking for the larger of the two maxima.\nApart from mode, distributions can have antimode(s); this term refers to the points x, for which the probability density function f(x) has its local minima. These points are not, however, considered characteristic of location."
  },
  {
    "objectID": "LN4.html#characteristics-of-variability",
    "href": "LN4.html#characteristics-of-variability",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Variability",
    "text": "Characteristics of Variability\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\nVariance and standard deviation\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable \\(x-E(X)\\). This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\\[E((x-E(X))^1)=\\mu_1\\]This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have “distances to the centre inversely proportional to their weights”.\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x) \\]\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx \\]\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\\[Var(X)=E(X^2)-[E(X)]^2\\]Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\nSome important properties of the variance are:\n\nNon-negativity \\(Var(X)\\geq{0}\\)\nConstant (real value c) has zero variance \\(Var(c)=0\\)\nFor real constants a and c: \\(Var(a+cX)=c^2Var(X)\\)\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in cm, then the variance of the height would be measured in cm2.\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a typical deviation in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\\[ S.D.(X)=\\sqrt{Var(X)} \\]\n\n\nMean Absolute Deviation\nAnother possibility to deal with the problem that the first central moment is equal to 0 is to use absolute deviations instead of squared deviations:\n\\[ MAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x) \\]\n\\[ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx \\]\nIn probability theory, mean absolute deviation most commonly refers to the deviation from the expected value. This way of measuring variability is a more common sense typical deviation in the probability distribution than standard deviation.\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tightly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to the expected value.\n\nIntermezzo\nIf we were looking for the real-valued constant c, for which the probability-weighted sum of squared deviations from such a constant ∑𝑥=−∞∞(𝑥−𝑐)2∗𝑃(𝑥) is minimal, then it would be the \\(c=E(X)\\). So, the variance is minimised if we use the expected value as the location characteristic. This has far-reaching consequences, e.g. for linear regression, in which conditional expected values are modelled by a linear combination of variables and the optimal constants in the model are found by minimising residual variance. But do not get disturbed too much here with thoughts of linear regression; it is not the scope of this course.\n\n\nBack to MAD\nUsing the same logic as in the intermezzo, what real-valued constant c would minimise the probability-weighted sum of absolute deviations \\(\\sum_{x=-\\infty}^\\infty |x-c|*P(x)\\)? Maybe surprisingly, the constant is not the expected value in this case, but median \\(c=x_{0.5}\\).\nSo, using the same logic, in which the expected value is connected to the variance, the mean absolute deviation should be connected to the median. And by the same reasoning, when modelling conditional medians by a linear combination of variables, the optimal constants are found by minimising residual mean absolute deviations. This type of modelling is called median (or, more generally, quantile) regression. Again, do not get disturbed by this.\nSo, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Even the median absolute deviation shares the same acronym, MAD. Does it make your head hurt? Sorry, no one promised you it would always be easy.\nIn this course, we will use the expected value to measure location when calculating mean absolute deviation, so we will follow the formulas at the beginning of this sub-chapter.\n\n\n\nInter-quartile range\nApart from measuring variability by a moment-based approach, we can also use a quantile-based approach. We can measure ranges between pre-specified quantiles and use them to measure variability with probabilistic meaning. The one that we will stick to within the course is the inter-quartile range\n\\[ IQR(X)=x_{0.75}-x_{0.25} \\]\nThe inter-quartile range is the range (distance) needed to capture the middle 50 % of the probability distribution.\n\n\nEntropy\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures the random variable’s expected ability to surprise or information content.\nIt starts with a negative logarithmic transformation of probability \\(-log_b(P(x))\\). Value with probability 0 can be viewed as impossible or infinitely surprising, and indeed \\(-log_b(0)=Inf\\). On the second part of the scale, a value with probability 1 is sure to happen or totally unsurprising and indeed \\(-log_b(1)=0\\). So, the initial transformation gives us the potential measure of surprisingness, and entropy measures the expected value of this\n\\[ H(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x)) \\]\nNotice that the introduced entropy measure is based on the probabilities and is suitable for only discrete random variables. The trouble with continuous random variables is that 𝑓(𝑥)≥0, and if the probability density value is above 1, the logarithm is positive, and entropy can be negative. So, we will skip the problem of measuring the entropy of continuous random variables and use it only as a measure for discrete random variables.\nNow, what units is entropy measured in? It depends on the base of the logarithm - b. The most common base, the one used in this course, is 2. Then, entropy is measured in bits. However, other bases can be used, e.g., base 10 leads to units called dits, or base e leads to units called nats.\nThe little trouble with entropy is, that its value depends on the number of possible values x of random variable. Maximum entropy can be achieved if all the values of the random variable are equally likely (so-called discrete uniform distribution).\nAssume there are k possible values, each with probability \\(P(x)=\\frac{1}{k}\\), then maximal entropy is\n\\[\nH(X)=-\\sum_{x=- \\infty}^{\\infty}P(x)*log_b P(x)=-k*\\frac{1}{k}*log_b \\frac {1}{k}=log_b(k)\n\\]\nSo for comparison among random variables with different numbers of possible values of random variable (k), normalised entropy, called efficiency, can be used\n\\[ \\eta(X)=\\frac{H(X)}{log_b(k)} \\]"
  },
  {
    "objectID": "LN4.html#skewness",
    "href": "LN4.html#skewness",
    "title": "Characteristics of Random Variables",
    "section": "Skewness",
    "text": "Skewness\nApart from the location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis, and the most common way of measuring them is based on standardised moments. These were already introduced, so to repeat, these are moments based on the k-th powers of standardised random variables, where the standardisation leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\nMoment measure of skewness is the 3rd standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nOperationally, we can distinguish the skewness or the lean of the distribution into three possibilities:\n\nPositive skewness - distribution is skewed towards the right, so the larger part of the mass is on the left.\nNull skewness - distribution is symmetric.\nNegative skewness - distribution is skewed towards the left, so the larger part of the mass is on the right."
  },
  {
    "objectID": "LN4.html#kurtosis",
    "href": "LN4.html#kurtosis",
    "title": "Characteristics of Random Variables",
    "section": "Kurtosis",
    "text": "Kurtosis\nThe moment measure of kurtosis is the 4th standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nWhile skewness is a somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of centralisation - the larger the mass is in the centre, the larger the kurtosis should be. But, nowadays, the prevailing view is that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\nDoes it make sense? At first sight, having a larger mass in the centre and the tails is contradictory, but it is true in many probability distributions. What is lacking in these distributions is the mass somewhere in between the centre and the tails.\nOperationally, we can distinguish the kurtosis into three possibilities by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\nLeptokurtic: \\(\\alpha_4&gt;3\\)\nMesokurtic: \\(\\alpha_4=3\\)\nPlatykurtic: \\(\\alpha_4&lt;3\\)\n\nDue to the direct comparability to the normal distribution kurtosis, the excess kurtosis 𝛼4−3 is sometimes used."
  },
  {
    "objectID": "LN1.html",
    "href": "LN1.html",
    "title": "Basics of Probability",
    "section": "",
    "text": "Let´s begin with the interpretation of probability – what does probability mean?\nWhat does it mean when someone says the “probability of dying at 80 is 0.03”?\nWhat does it mean when someone says the “probability of tomorrow’s rain is 0.5”?\nThere must be some meaning. Throughout history, two competing schools of probability thought have always existed. One is known as objectivist, and the second is known as subjectivist.\nFor the first school, the one that dominates this course, probability is an object that we might attempt to discover, but something that exists no matter what we do. Historically, this school has been presented as a physical school, which describes probability as the result of the world’s physics. In a more modern setting, this school is presented more as frequentist, which holds that probability is a limit of relative frequency in a repeated series of random trials.\nFor the second school, the one that has been somewhat suppressed in the 20th century but is on the rise recently, probability is a subject – something that is dependent on the observer. In this school of thought, the probability is an expression of uncertainty.\nWhile this distinction might, for you, now seem a somewhat interesting but useless philosophical quarrel, it eventually leads to two surprisingly different ways of doing statistical inference, which is in scope in the second half of the course."
  },
  {
    "objectID": "LN1.html#interpretation-of-probability",
    "href": "LN1.html#interpretation-of-probability",
    "title": "Basics of Probability",
    "section": "",
    "text": "Let´s begin with the interpretation of probability – what does probability mean?\nWhat does it mean when someone says the “probability of dying at 80 is 0.03”?\nWhat does it mean when someone says the “probability of tomorrow’s rain is 0.5”?\nThere must be some meaning. Throughout history, two competing schools of probability thought have always existed. One is known as objectivist, and the second is known as subjectivist.\nFor the first school, the one that dominates this course, probability is an object that we might attempt to discover, but something that exists no matter what we do. Historically, this school has been presented as a physical school, which describes probability as the result of the world’s physics. In a more modern setting, this school is presented more as frequentist, which holds that probability is a limit of relative frequency in a repeated series of random trials.\nFor the second school, the one that has been somewhat suppressed in the 20th century but is on the rise recently, probability is a subject – something that is dependent on the observer. In this school of thought, the probability is an expression of uncertainty.\nWhile this distinction might, for you, now seem a somewhat interesting but useless philosophical quarrel, it eventually leads to two surprisingly different ways of doing statistical inference, which is in scope in the second half of the course."
  },
  {
    "objectID": "LN1.html#definitions-of-probability",
    "href": "LN1.html#definitions-of-probability",
    "title": "Basics of Probability",
    "section": "Definitions of Probability",
    "text": "Definitions of Probability\nIn the previous part, the focus was on the interpretation of probability. Let´s now discuss the definition of probability. These are hard to separate from the interpretation and, in some cases, are tightly connected.\n\nThe classical definition of probability\nHistorically, formal treatises on probabilities emerged in the 16th and 17th centuries with scholars dealing with the “game of chance”. These games were based chiefly on throwing dice, but others were also based on cards or roulette wheels. With these games, the idea of a fair outcome emerged – that each outcome at one random trial has equivalent probability. With this assumption, it was easy to calculate probabilities by counts and combinatorics theory, which was emerging to solve many probabilistic puzzles.\nWhat emerged in the works of Italian Gerolamo Cardano is now called classical definition and was cemented by Pierre-Simon Laplace. It states that the probability of an event is the ratio of favourable outcomes to all possible outcomes with the principle of difference – no specific outcome is more likely than any other.\n\nIntermezzo - Important Terms\nRandom trial –a trial (experiment) in which results cannot be predicted with certainty and repeatable (at least theoretically) under the same conditions.\nRandom event – is a result of random trial.\nElementary random event – is the smallest possible outcome of a random trial. More formally, it is such a random event that the union of other random events cannot create it. Is indivisible.\nSample space E – is a set of all possible elementary random events.\nRandom events (usually denoted by capital letters from the beginning of the alphabet – A, B, C,…)can thus be viewed as any non-empty subset of sample space.\nSet operations are essential for probability calculations because the definitions of the initial terms are based on the sets.\nIntersection \\(A \\cap B\\) is a set operation, meaning both A and B are true.\nUnion \\(A \\cup B\\) is a set operation, meaning at least one of A and B is true.\nComplement \\(\\bar A\\) is a set operation meaning true if A is not true. Opposite of A.\n\n\nGoing back to the classical definition of probability\nWith new terms, we can define the classical definition of probability as\n\\[ P(A)= \\frac {m}{n} \\]\nWhere A is the union of m (number of favourable) random events and n is the number of all possible random events, where individual events are equally likely and mutually exclusive.\n\n\n\nGeometric definition of probability\nThe classical definition of probability was, among other things, limited by its discrete nature. To overcome this limitation and solve new issuing problems, scholars in the 18th century started thinking about continuous sample spaces. This led to the generalisation called the geometric definition of probability, which can be expressed as\n\\[ P(A)=\\frac {V(A)}{V(E)} \\]\nThe V operator represents continuous measure – in one dimension length, in two dimensions area and in more dimensions simply volume. What remained was the idea that each pair of regions of the same volume was equally likely.\nThe most famous problem solved using the geometric definition of probability was Buffon´s needle: Imagine a floor made of parallel strips of wood, each with the same width t. Now, randomly drop a needle with a length of l onto this floor. How likely is the needle to lie across a line between two strips? This probability, assuming \\(l \\leq t\\), was proved to be\n\\[ P(A)=\\frac {2*l}{\\pi*t} \\]\nMuch later, this problem was turned around, and Buffon´s needle was used to estimate the value of \\(π\\) by substituting relative frequency p for probability P(A):\n\\[ est(\\pi)=\\frac{2*l} {p *t} \\]\nSubstituting relative frequency for probability is a trick available thanks to the laws describing convergences in probability and is the basis of the frequentist definition.\n\n\nFrequentist definition of probability\nThe classical definition of probability stumbled upon a severe limitation – its reliance on the principle of indifference, which might be justifiable in the games of chance but hardly so in other real-world instances. This led to more broad definition of probability.\nHave a series of random trials. Based on this series, we have a series of relative frequencies of observing random event A: pn(A). This series of relative frequencies will converge (in probability) to its limit, the underlying probability of an event A.\n\\[p_n(A) \\rightarrow P(A)\\]We will deal with stochastic convergences later because these laws connect probability theory with statistics.\n\n\nAxiomatic definition\nAround the same time R.A. Fischer or Richard von Misses (brother of not less famous Austrian economist Ludwig von Misses) were championing and rigorously proving frequentist definition and interpretation of probability, a completely new approach emerged: axiomatic definition, work of brilliant Russian mathematician A. N. Kolmogorov. This approach focused on the smallest basic set of rules, axioms, which can serve as a basis for all manipulations with probabilities. Kolmogorov´s axioms, in simplicity, are three:\nNon-negativity – The probability of a random event is not a negative number: \\(P(A) \\geq 0\\)\nNormalisation – The probability of an entire sample space is 1: \\(P(S)=1\\)\nAdditivity – For mutually exclusive random events A and B, the probability of their union is the sum of their probabilities: \\(P(A\\cap B)=0\\rightarrow P(A\\cup B)=P(A)+P(B)\\)\nBased on these three axioms, all the rules for computing with probabilities can be derived. The most essential derivations are:\nThe probability of an empty set is 0: \\(P(\\emptyset )=0\\)\nThe probability of a random event is between 0 and 1: \\(0 \\leq P(A)\\leq 1\\)\nThe probability of a complement is: \\(P(\\bar{A})=1-P(A)\\)\nThe probability of a union is: \\(P(A \\cup B)=P(A)+P(B)-P(A \\cap B)\\)"
  },
  {
    "objectID": "LN1.html#transformations-of-probability",
    "href": "LN1.html#transformations-of-probability",
    "title": "Basics of Probability",
    "section": "Transformations of Probability",
    "text": "Transformations of Probability\nIt is derived from Kolmogorov´s axioms, which state that probability is a number between 0 and 1. However, for statistical modelling and sometimes in the usual language, we need numbers related to the probability but used on a different scale. This is what various transformations of probability ensure.\n\nOdds\nOdds are an everyday linguistic use of probability, often seen in betting, that is a transformation:\n\\[ Odds(A)=\\frac {P(A)}{P(\\bar A)}= \\frac {P(A)}{1-P(A)} \\]\nThe transformation lands on the scale \\((0, \\infty)\\).\nSo, instead of saying that the probability of a home team winning a soccer match is 0.2, this probability is one to four because odds equal 0.2 over 0.8 equals one fourth. Odds can also be used to calculate the Bayes formula.\n\n\nLogit\nExpressing probability on the unbounded scale might be helpful for linear statistical modelling. This is where logit, using natural base logarithm, comes in handy:\n\\[ Logit(A)=log(odds(A))=log \\left( \\frac {P(A)}{1-P(A)} \\right) \\]\n\n\nNegative log transformation\nThis type of transformation is vital in information theory, where we transform probability on the positive scale \\((0, \\infty)\\), expressing the surprisingness levels of random events. The base of the logarithm tells us the information units, e.g. base = 2 leads to the result expressed in bits.\n\\[ -log_2(P(A)) \\]"
  },
  {
    "objectID": "LN1.html#two-random-events",
    "href": "LN1.html#two-random-events",
    "title": "Basics of Probability",
    "section": "Two Random Events",
    "text": "Two Random Events\nConsidering two (or more, of course) random events, we can introduce important statistical concepts – conditionality and (in)dependence.\n\nIndependent random events\nThe two random events, coming from two random trials, are independent if the result of one of the trials does not influence the probabilities of the results of the other trial, and vice versa.\nWe note conditional probability as \\(P(A|B)\\), meaning the probability of A under the condition of B. The fact that the change in condition does not change this probability can be expressed as\n\\[ P(A|B)=P(A|\\bar B)=P(A) \\]\nand vice versa.\nLet´s make a quick detour. What is the probability of the intersection of two random events? Intersection means that both events occur. The product of two probabilities gives the probability of two events occurring, but which? Having random events A and B, we can say that both occur is equivalent to A occurring and B occurring under condition A has occurred. And vice versa, so\n\\[ P(A \\cap B)=P(A)*P(B|A)=P(B)*P(A|B) \\]\nWith this detour, we can posit necessary and sufficient condition of independence:\n\\[ P(A \\cap B)=P(A)*P(B) \\]\nIf two random events are independent, the probability of their intersection is a product of individual probabilities. And if the product of individual probabilities is equal to the probability of intersection, the two random events are independent. This has two critical consequences – first, it simplifies calculation if we assume independence (e.g., standard likelihood function used in estimation theory). Second, it can be used to test the hypothesis of independence when having a set of data.\n\n\nConditional probability\nLast but not least comes an essential formula: conditional probability calculation. Since we introduced that \\(P(A \\cap B)=P(A)*P(B|A)\\), it should be easy to see how to calculate\n\\[ P(B|A)=\\frac{P(A \\cap B)}{P(A)} \\]\nand vice versa.\nThis can be interpreted this way: the probability of B under the condition of A is the probability of both events occurring divided by the probability of the condition P(A). The condition limits the sample space, leading to normalisation such that\n\\[ P(B|A)+P(\\bar B|A)=\\frac {P(A \\cap B)}{P(A)} + \\frac {P(A \\cap \\bar B)}{P(A)}=\\frac{P(A \\cap B)+P(A\\cap \\bar B)}{P(A)}=\\frac {P(A)}{P(A)}=1 \\]\nYes, under condition A, either B or complement to B must happen."
  },
  {
    "objectID": "LN1.html#total-probability-law",
    "href": "LN1.html#total-probability-law",
    "title": "Basics of Probability",
    "section": "Total Probability Law",
    "text": "Total Probability Law\nIn the previous part of the lecture notes, we defined conditional probability. We will continue in the journey to reach two very important results of probability theory - total probability law now and Bayes theorem in the next part.\nLet´s start with the simplest case: we have one random event B followed by the second random event A. Random event A is dependent on the random event B, meaning its conditional probabilities are different based on whether event B or its complement occurred: \\(P(A|B) \\not= P(A|\\bar{B})\\).\nThe first question arises: what is (un)conditional probability of the random event A? If we think a little while about this, the four intersections create elementary sample space:\n\\(E=(A\\cap{B}, A\\cap{\\bar{B}}, \\bar{A}\\cap{B}), \\bar{A}\\cap{\\bar{B}})\\).\nRandom event A is simply union: \\(A=(A\\cap{B})\\cup({A\\cap{\\bar{B}}})\\).\nAnd since the two intersections are elementary, they are disjoint and the probability of such a union:\n\\(P(A)=P(A\\cap{B})+P({A\\cap{\\bar{B}}})\\).\nIn simple terms, the random event A occurs if random event A occurs with B or with the complement to B, and total probability is the summation of the two possibilities. If we apply the formula from the previous section, we can find the usual way total probability law is written in this case:\n\\(P(A)=P(B)*P(A|B)+P(\\bar{B})*P(A|\\bar{B})\\).\nIn the last step, we can generalise. Instead of the random event B and its complement, let´s now assume that the first random trial can end up in k various disjoint random events, which together fully partition the sample space of this random trial:\n\\(E=\\cup_{i=1}^kB_i\\).\nHence, the summation of probabilities of \\(B_i\\) is equal to one:\n\\(\\sum_{i=1}^k(P(B_i))=1\\).\nThen we can follow the same logic as in the first case, where k = 2, and find out the general formula for total probability law:\n\\(P(A)=\\sum_{i=1}^k(P(B_i)*P(A|B_i))\\).\nWe can interpret this formula - total probability is the weighted average of the conditional probabilities \\(P(A|B_i)\\) with weights equal to respective probabilities of conditions \\(P(B_i)\\)."
  },
  {
    "objectID": "LN1.html#bayes-theorem",
    "href": "LN1.html#bayes-theorem",
    "title": "Basics of Probability",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nWe used total probability law to determine event A’s probability independent of the preceding event B. But we can be, and often are, interested in a different question: knowing that event A happened, what is the probability that before it, event B happened?\nThis question was generally answered in the late 18th century independently by two persons: first by English reverend Thomas Bayes, whose work was corrected and published posthumously by his friend Richard Price, and later and more generally by Pierre-Simon Laplace, who put the ground to what is now called Bayesian inference. The last name to mention here is Sir Jeffreys, who put Laplace´s formulation in line with the axiomatic definition of probability.\nIt is important to note that even though Bayes´ theorem is a basis for Bayesian inference (statistic) and was developed as such, it is generally an undisputable and uncontroversial finding in probability theory.\nLet´s start with \\(P(B|A)=\\frac{P(A\\cap B)}{P(A)}\\); then P(A) is given by total probability law, hence:\n\\[ P(B|A)=\\frac{P(A\\cap B)}{P(A\\cap B)+P(A\\cap \\bar{B})} \\]\nWe can interpret it, e.g., in this way: What part of the total probability of event A is attributable to events A and B occurring together? In a more general way, this specific case of Bayes theorem can be written using the notation from the previous section:\n\\[ P(B_{i}|A)=\\frac{P(A\\cap B_{i})}{\\sum_{i=1}^kP(B_i) *P(A|B_i)} \\]\nIn statistics, probabilities \\(P(B_i)\\) are usually called prior probabilities or simply priors to stress that they represent probability before an event A does or does not occur (before data). Probabilities \\(P(A|B_i)\\) are in statistics called likelihoods (probabilities of data). Finally, probabilities \\(P(B_i|A)\\) are called posterior probabilities or simply posteriors because they represent newly acquired probabilities after observing event A (after seeing data)."
  },
  {
    "objectID": "LN2.html",
    "href": "LN2.html",
    "title": "Random Variable",
    "section": "",
    "text": "So far, we have dealt with the topic of random events. But in statistics, it is much more usual to deal with the numbers - how can we get to the theoretical concepts upon which we can build statistical models?\nThe answer is by developing the concept of random variables.\nA random variable X is a measurable function that maps from the sample space of random events to the measurable space, typically a set or interval of real numbers.\n\\[\nP(X\\in{S})=P(\\omega\\in{\\Omega}|X(\\omega)\\in{S})\n\\]\nSimply put, it defines how we get from random events to numbers.\nWe usually use capital letters from the end of the alphabet (X, Y, Z or X1, X2, …) to denote the random variable (function) itself and small letters (x, y, z or x1, x2, …) to represent numerical values of this random variable.\nRandom variables are usually divided into two classes: discrete random variable is such a variable in which the range of possible values is countable (finitely or infinitely), and we can assign a probability to individual values;\nOn the other hand, a continuous random variable is such a variable in which the range is uncountably infinite, most commonly interval, and we cannot assign a probability to individual values.\nThe term absolutely has its mathematical meaning and is usually omitted in textbooks, so from now on, it will be called a continuous random variable."
  },
  {
    "objectID": "LN2.html#cumulative-distribution-function",
    "href": "LN2.html#cumulative-distribution-function",
    "title": "Random Variable",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nThe cumulative distribution function is the most universal way to describe a probability distribution of any random variable. It is defined as\n\\[ F(x)=P(X\\leq{x}) \\]\nThe cumulative distribution function at point x is a probability that a random variable X is lower or equal to the value x.\nThis function has several important properties:\n\nIs non-decreasing \\(x_1\\leq{x_2}\\to{F(x_1)}\\leq{F(x_2)}\\)\nAs any probability, takes values in the range \\([0,1]\\)\nFrom the two properties, we can see the limits: \\(lim_{x\\to-\\infty}F(x)=0\\) and \\(lim_{x\\to\\infty}F(x)=1\\)\nIs right-continuous\n\nThe last point is important for formally distinguishing discrete and continuous random variables.\n\nHow to calculate probabilities\nOnce we have a random variable described by its cumulative distribution function, we can calculate any probability related to the random variable. In the most general way, we can write.\n\\[ P(x_1&lt;X\\leq{x_2})=P(X\\leq{x_2})-P(X\\leq{x_1})=F(x_2)-F(x_1) \\]\nUsing the knowledge of limits of the cumulative distribution function, we can see special cases:\n\\[ P(X\\leq{x_2})=P(-\\infty&lt;X\\leq{x_2})=F(x_2)-F(-\\infty)=F(x_2)-0=F(x_2) \\]\n\\[ P(X&gt;x_1)=P(x_1&lt;X\\leq{\\infty})=F(\\infty)-F(x_1)=1-F(x_1) \\]\nThese cases can also be derived from the cumulative distribution function’s definition and the complement operation.\nThe last special case is the calculation of the point probability.\n\\[ P(X=x)=P(x&lt;X\\leq{x})= P(X\\leq{x})-P(X&lt;x)=F(x)-F(x^-) \\]\nWe can read it this way: the probability of a point x is the value of cumulative distribution at this point minus the value of cumulative distribution function at this point evaluated from the left.\n\n\nDistinguishing discrete and continuous random variables\nThe cumulative distribution function can have points of discontinuity - at these points, values of F(x) and F(x-) are different, and there is a point probability. If the cumulative distribution function is otherwise constant, it is a so-called step function, as it looks like steps. This is the characteristic outlook of the cumulative distribution function of discrete random variables.\nIf the cumulative distribution function contains no points of discontinuity, it is smooth; then the random variable is continuous.\nTheoretically, a mix can occur - the cumulative distribution function can have points of discontinuity and be smoothly increasing over some range - but we will not cover this specific case."
  },
  {
    "objectID": "LN2.html#discrete-random-variables",
    "href": "LN2.html#discrete-random-variables",
    "title": "Random Variable",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nAs already noted, discrete random variables are such random variables that have a countable range of possible values, and we can thus assign specific point probabilities. Its cumulative distribution function is a step function, and the probability of value x can be calculated as the height of the step of the cumulative distribution function at this point x. \\[ P(X=x)=P(x)=(x)-F(x^-) \\]\nThe function assigning probabilities P(x) for all points x is called the probability function (or, in older publications, probability mass function) and is the most common way to describe the probability distribution of discrete random variables.\nSome basic properties are:\n\nIt is a probability \\(0\\leq{P(x)}\\leq1\\)\nOne of the values has to occur:\\(\\sum_{x=-\\infty}^{\\infty}P(x)=1\\)\nThe value of the cumulative distribution function can be calculated by cumulating probabilities (heights of the steps) up to the point x: \\(F(x)=P(X\\leq{x})=\\sum_{i=-\\infty}^{x}P(t)\\)"
  },
  {
    "objectID": "LN2.html#continuous-random-variables",
    "href": "LN2.html#continuous-random-variables",
    "title": "Random Variable",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nContinuous random variables are such random variables that have an uncountably infinite range, usually an interval range, and we thus can not assign specific point probabilities. Probabilities are assigned only to the intervals. Its cumulative distribution function is smooth.\nSo, in this type of random variable, we can not have point probabilities in a classical sense. Still, we can describe point intensity as a speed of increase of cumulative distribution function at point x, thus:\n\\[f(x)=\\frac{\\partial{F(x)}}{\\partial{x}}\\]\nFunction f(x) is called a probability density function. This function is the most common way to display a probability distribution of random variables. Even though it does not represent point probabilities, it still can be used to compare the likeliness of values close to the points of interest.\nThe most important relation between the probability density function and the probability is that the probability of a specific range of values of a random variable is an area under the probability density function in the specified range:\n\\[ P(x_1&lt;X&lt;x_2)=P(x_1&lt;X\\leq{x_2})=P(x_1\\leq{X}&lt;x_2)=P(x_1\\leq{X}\\leq{x_2})=F(x_2)-F(x_1)=\\int_{x_1}^{x_2}f(x)dx \\]\nSome important properties of probability density functions (compare them to properties of probability (mass) functions:\n\nIt is non-negative because the cumulative distribution function is non-decreasing: \\(f(x)\\geq0\\)\nTotal area is 1: \\(\\int_{x=-\\infty}^{\\infty}f(x)=1\\)\n\\(F(x)=P(X\\leq{x})=\\int_{t=-\\infty}^{x}f(t)\\)"
  },
  {
    "objectID": "LN2.html#quantile-function",
    "href": "LN2.html#quantile-function",
    "title": "Random Variable",
    "section": "Quantile Function",
    "text": "Quantile Function\nIn describing a probability distribution of random variables, we have utilised the cumulative distribution function, which is universal, the probability function for discrete random variables, and the probability density function for continuous random variables.\nThere are other ways to describe probability distribution, e.g. via survival function or hazard function. We will skip these, but one other function is important in this course: a quantile function.\n100P% quantile, denoted as xp, is such a value that a random variable can take this value or lower with a given probability P. Hence\n\\[ P(X\\leq{x_p})=P\\to F(x_p)=P \\]\nThe quantile function is closely related to the cumulative distribution function; it is the inverse of it\n\\[ x_p=Q(P)=F^{-1}(P) \\]\nTo rephrase it: when we know the value and want to find the yet unknown probability that the random variable is less or equal to this already known value, we use the cumulative distribution function: \\(x\\to{P}\\).\nAnd when we know the probability that the random variable is less or equal to some yet unknown value, we are solving inverse problem \\(P\\to{x}\\).\nThe cumulative distribution function must be monotonic because we need to find its inverse function. However, the cumulative distribution function is generally only non-decreasing, which means it is not necessarily monotonic.\nThe first obvious problem arises for discrete random variables, in which the cumulative distribution function is generally a step function. We will avoid this problem by simply avoiding using quantile functions for discrete random variables, even though generalisation is available.\nFor continuous random variables, we will assume it is strictly monotonic for 0 &lt; P &lt; 1. We will thus not look for 100% and 0% quantiles.\nMany quantiles have their specific names, usually derived from the number of probabilistically equal portions into which they divide the distribution. The most well-known is median x0.5, which divides the distribution into two halves with the same probability. Others include:\n\nTerciles x0.33 and x0.67\nQuartiles x0.25, x0.5, and x0.75\nQuintiles x0.2, x0.4, x0.6, and x0.8\n….\nPercentiles x0.01,x0.02,…,x0.99"
  },
  {
    "objectID": "LN3.html",
    "href": "LN3.html",
    "title": "Characteristics of Random Variables",
    "section": "",
    "text": "In the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\nWhat is this other information? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\nLocation\nVariability\nSkewness\nKurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later."
  },
  {
    "objectID": "LN3.html#introduction",
    "href": "LN3.html#introduction",
    "title": "Characteristics of Random Variables",
    "section": "",
    "text": "In the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\nWhat is this other information? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\nLocation\nVariability\nSkewness\nKurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later."
  },
  {
    "objectID": "LN3.html#moments",
    "href": "LN3.html#moments",
    "title": "Characteristics of Random Variables",
    "section": "Moments",
    "text": "Moments\n\nRaw Moments\nThe moment is a concept known from ancient Greek physics, e.g. Archimedes on the centre of gravity of a lever: “A and B are equally balanced if their distances to the centre are inversely proportional to their weights.” The centre of gravity is generally a point around which the resultant torque due to gravity forces vanishes; the object is in balance.\nLet´s imagine two persons weighing 60 and 80 kg swinging on a seesaw; the first is at point 0, and the second is at point 1. Where should the fulcrum be put so they can be balanced? According to Archimedes, their distance to the centre should be inversely proportional to their weights. So the first one should be at 8/14 of the distance to the centre, and the second one should be at 6/14 of the distance to the centre. The overall distance is 1, and the fulcrum should be at the point of 8/14. This point is the centre of gravity and the (first) moment.\nNow, how can this point be calculated? Let´s imagine a real line; values 0 and 1 represent points on this line, and weights are relative to the sum of all weights, so their sum is 1; the relative weight of the first person sitting at point 0 is \\(60/(60+80)=6/14\\) and the relative weight of the second person sitting at point 1 is \\(80/(60+80)=8/14\\). The centre of gravity value can be calculated as \\(0*6/14+1*8/14=8/14\\).\nLet´s add the third person on the seesaw and put him at point 2 with a weight of 100 kg. So the relative weights are now 60/240 = 6/24, 8/24 and 10/24. See, their total is 1. The centre of gravity value can be calculated as \\(0*6/24+1*8/24+2*10/24=28/24\\approx{1.167}\\). So, if you wanted to put a fulcrum below the seesaw for the line to be horizontal and stable, you would have to put it under the seesaw at this point.\nThis long introduction here tries to explain the (first raw) moment and why it is useful as a characteristic of the location. However, we derive various properties from three different types of moments and powers.\nThe first raw moment has been described so far, which, in probability theory, is also called expected value or simply expectation. We use values x as the points and probabilities P(x) as weights for discrete random variables. Because the sum of the P(x) equals 1, it is already relative weight. Similarly, we use the probability density function f(x) as the relative weight for continuous random variables. Still, since it is a continuous function, we must integrate instead of summing.\n\\[\nE(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n\\]\n\\[ E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nNow, the k-th raw moment is defined as the expected value of the k-th power of the random variable X:\n\\[ \\mu_k^´=E(X^k)= \\sum_{x=-\\infty}^{\\infty}x^k*P(x) \\]\n\\[ \\mu_k^´=E(X^k)=\\int_{x=-\\infty}^{\\infty}x^k*f(x)dx \\]\n\n\nCentral Moments\nWe can use central moments for some properties besides the raw moments. Centralisation is the term used to put the centre of the random variable to equal 0. This leads to the description of the random variable independent of its location.\nWe do this by subtracting the expected value from the values of the random variable: \\(x-E(X)\\). Now, the k-th central moment is defined as the expected value of the k-th power of the centralised random variable X:\n\\[ \\mu_k=E((X-E(X))^k)= \\sum_{x=-\\infty}^{\\infty}(x-E(X))^k*P(x) \\]\n\\[ \\mu_k=E((X-E(X))^k)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^k*f(x)dx \\]\n\n\nStandardised Moments\nThe last type of moment is the standardised (or normalised) moment. Proces of standardisation start with centralisation (to put the expected value equal to 0) and continues with dividing by the square root of the second central moment, which is called standard deviation. This ensures that a standardised random variable’s variance and standard deviation (moment measures of variability) equal 1, thus not influencing other properties we might desire to observe.\nNow, the k-th standardised moment is defined as the expected value of the k-th power of the standardised random variable X:\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*P(x) \\]\n\\[ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\int_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*f(x)dx \\]"
  },
  {
    "objectID": "LN3.html#characteristics-of-location",
    "href": "LN3.html#characteristics-of-location",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Location",
    "text": "Characteristics of Location\nCharacteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?\n\nExpected Value\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function P(x) or probability density function f(x).\n\\[ \\mu_1^´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x) \\]\n\\[ \\mu_1^´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx \\]\nThe expected value has several important properties; for now, we stay with two of them:\n\nFor real constant c: \\(E(c)=c\\)\nLinearity of expectations, for real constants a and c: \\(E(a+cX)=a+c*E(X)\\)\n\n\n\nMedian\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x0.5 is 50% quantile, which means it is a value satisfying the following equation\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})=0.5 \\]\n\\[ x_{0.5}=Q(0.5)=F^{-1}(0.5) \\]\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\\[ F(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5 \\]\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves.\n\n\nMode\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value x, for which probability density function f(x) has a local maximum in the range of the random variable.\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two peaks in its probability density function without looking for the larger of the two maxima.\nApart from mode, distributions can have antimode(s); this term refers to the points x, for which the probability density function f(x) has its local minima. These points are not, however, considered characteristic of location."
  },
  {
    "objectID": "LN3.html#characteristics-of-variability",
    "href": "LN3.html#characteristics-of-variability",
    "title": "Characteristics of Random Variables",
    "section": "Characteristics of Variability",
    "text": "Characteristics of Variability\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\nVariance and standard deviation\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable \\(x-E(X)\\). This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\\[E((x-E(X))^1)=\\mu_1\\]This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have “distances to the centre inversely proportional to their weights”.\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x) \\]\n\\[ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx \\]\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\\[Var(X)=E(X^2)-[E(X)]^2\\]Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\nSome important properties of the variance are:\n\nNon-negativity \\(Var(X)\\geq{0}\\)\nConstant (real value c) has zero variance \\(Var(c)=0\\)\nFor real constants a and c: \\(Var(a+cX)=c^2Var(X)\\)\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in cm, then the variance of the height would be measured in cm2.\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a typical deviation in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\\[ S.D.(X)=\\sqrt{Var(X)} \\]\n\n\nMean Absolute Deviation\nAnother possibility to deal with the problem that the first central moment is equal to 0 is to use absolute deviations instead of squared deviations:\n\\[ MAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x) \\]\n\\[ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx \\]\nIn probability theory, mean absolute deviation most commonly refers to the deviation from the expected value. This way of measuring variability is a more common sense typical deviation in the probability distribution than standard deviation.\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tightly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to the expected value.\n\nIntermezzo\nIf we were looking for the real-valued constant c, for which the probability-weighted sum of squared deviations from such a constant ∑𝑥=−∞∞(𝑥−𝑐)2∗𝑃(𝑥) is minimal, then it would be the \\(c=E(X)\\). So, the variance is minimised if we use the expected value as the location characteristic. This has far-reaching consequences, e.g. for linear regression, in which conditional expected values are modelled by a linear combination of variables and the optimal constants in the model are found by minimising residual variance. But do not get disturbed too much here with thoughts of linear regression; it is not the scope of this course.\n\n\nBack to MAD\nUsing the same logic as in the intermezzo, what real-valued constant c would minimise the probability-weighted sum of absolute deviations \\(\\sum_{x=-\\infty}^\\infty |x-c|*P(x)\\)? Maybe surprisingly, the constant is not the expected value in this case, but median \\(c=x_{0.5}\\).\nSo, using the same logic, in which the expected value is connected to the variance, the mean absolute deviation should be connected to the median. And by the same reasoning, when modelling conditional medians by a linear combination of variables, the optimal constants are found by minimising residual mean absolute deviations. This type of modelling is called median (or, more generally, quantile) regression. Again, do not get disturbed by this.\nSo, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Even the median absolute deviation shares the same acronym, MAD. Does it make your head hurt? Sorry, no one promised you it would always be easy.\nIn this course, we will use the expected value to measure location when calculating mean absolute deviation, so we will follow the formulas at the beginning of this sub-chapter.\n\n\n\nInter-quartile range\nApart from measuring variability by a moment-based approach, we can also use a quantile-based approach. We can measure ranges between pre-specified quantiles and use them to measure variability with probabilistic meaning. The one that we will stick to within the course is the inter-quartile range\n\\[ IQR(X)=x_{0.75}-x_{0.25} \\]\nThe inter-quartile range is the range (distance) needed to capture the middle 50 % of the probability distribution.\n\n\nEntropy\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures the random variable’s expected ability to surprise or information content.\nIt starts with a negative logarithmic transformation of probability \\(-log_b(P(x))\\). Value with probability 0 can be viewed as impossible or infinitely surprising, and indeed \\(-log_b(0)=Inf\\). On the second part of the scale, a value with probability 1 is sure to happen or totally unsurprising and indeed \\(-log_b(1)=0\\). So, the initial transformation gives us the potential measure of surprisingness, and entropy measures the expected value of this\n\\[ H(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x)) \\]\nNotice that the introduced entropy measure is based on the probabilities and is suitable for only discrete random variables. The trouble with continuous random variables is that 𝑓(𝑥)≥0, and if the probability density value is above 1, the logarithm is positive, and entropy can be negative. So, we will skip the problem of measuring the entropy of continuous random variables and use it only as a measure for discrete random variables.\nNow, what units is entropy measured in? It depends on the base of the logarithm - b. The most common base, the one used in this course, is 2. Then, entropy is measured in bits. However, other bases can be used, e.g., base 10 leads to units called dits, or base e leads to units called nats.\nThe little trouble with entropy is, that its value depends on the number of possible values x of random variable. Maximum entropy can be achieved if all the values of the random variable are equally likely (so-called discrete uniform distribution).\nAssume there are k possible values, each with probability \\(P(x)=\\frac{1}{k}\\), then maximal entropy is\n\\[\nH(X)=-\\sum_{x=- \\infty}^{\\infty}P(x)*log_b P(x)=-k*\\frac{1}{k}*log_b \\frac {1}{k}=log_b(k)\n\\]\nSo for comparison among random variables with different numbers of possible values of random variable (k), normalised entropy, called efficiency, can be used\n\\[ \\eta(X)=\\frac{H(X)}{log_b(k)} \\]"
  },
  {
    "objectID": "LN3.html#skewness",
    "href": "LN3.html#skewness",
    "title": "Characteristics of Random Variables",
    "section": "Skewness",
    "text": "Skewness\nApart from the location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis, and the most common way of measuring them is based on standardised moments. These were already introduced, so to repeat, these are moments based on the k-th powers of standardised random variables, where the standardisation leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\nMoment measure of skewness is the 3rd standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nOperationally, we can distinguish the skewness or the lean of the distribution into three possibilities:\n\nPositive skewness - distribution is skewed towards the right, so the larger part of the mass is on the left.\nNull skewness - distribution is symmetric.\nNegative skewness - distribution is skewed towards the left, so the larger part of the mass is on the right."
  },
  {
    "objectID": "LN3.html#kurtosis",
    "href": "LN3.html#kurtosis",
    "title": "Characteristics of Random Variables",
    "section": "Kurtosis",
    "text": "Kurtosis\nThe moment measure of kurtosis is the 4th standardised moment\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) \\]\n\\[ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx \\]\nWhile skewness is a somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of centralisation - the larger the mass is in the centre, the larger the kurtosis should be. But, nowadays, the prevailing view is that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\nDoes it make sense? At first sight, having a larger mass in the centre and the tails is contradictory, but it is true in many probability distributions. What is lacking in these distributions is the mass somewhere in between the centre and the tails.\nOperationally, we can distinguish the kurtosis into three possibilities by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\nLeptokurtic: \\(\\alpha_4&gt;3\\)\nMesokurtic: \\(\\alpha_4=3\\)\nPlatykurtic: \\(\\alpha_4&lt;3\\)\n\nDue to the direct comparability to the normal distribution kurtosis, the excess kurtosis 𝛼4−3 is sometimes used."
  },
  {
    "objectID": "LN4.html#joint-distribution",
    "href": "LN4.html#joint-distribution",
    "title": "Random Vectors",
    "section": "Joint distribution",
    "text": "Joint distribution\nThe joint distribution of a random vector is described as an intersection. Translated, the joint cumulative distribution function for a two-dimensional random vector is\n\\[\nF(x_1,x_2)=P(X_1\\leq{x_1}\\cap X_2\\leq{x_2})\n\\]\njoint probability funcion is\n\\[ P(x_1,x_2)=P(X_1={x_1}\\cap X_2={x_2}) \\]\nand joint probability density function is\n\\[ f(x_1,x_2)=f(X_1={x_1}\\cap X_2={x_2}) \\]\nWe are omitting a case of a mixed random vector (continuous and discrete random variables in one random vector).\nRelations between the mentioned descriptions of joint probability distributions are generalisations of the already seen relations in the random variables\n\\[ F(x_1,x_2)=\\sum_{t=-\\infty}^{x_1}\\sum_{u=-\\infty}^{x_2}P(x_1,x_2)\\]\n\\[\nF(x_1,x_2)=\\int_{t=-\\infty}^{x_1}\\int_{u=-\\infty}^{x_2}f(x_1,x_2)dx_2dx_1\n\\]\n\\[\nf(x_1,x_2)=\\frac{\\partial\\partial F(x_1,x_2)}{\\partial x_1 \\partial x_2}\n\\]"
  },
  {
    "objectID": "LN4.html#marginal-distribution",
    "href": "LN4.html#marginal-distribution",
    "title": "Random Vectors",
    "section": "Marginal distribution",
    "text": "Marginal distribution\nMarginal distribution refers to any subset of the originating random vector independent of the omitted part of that random vector. Say we have a 5-dimensional random vector:\n\\[\n\\textbf{X}=\n\\begin{pmatrix}\nX_1\\\\\nX_2\\\\\nX_3\\\\\nX_4\\\\\nX_5\\\\\n\\end{pmatrix}\n\\]\nThen, all the following random vectors and random variables can be called marginal vectors or variables (this is not a complete enumeration, just examples):\n\\[ \\textbf{X}= \\begin{pmatrix} X_1\\\\ X_2\\\\ X_3\\\\ X_4\\\\\\end{pmatrix},\n\\textbf{X}= \\begin{pmatrix} X_1\\\\ X_4 \\end{pmatrix},\nX_1 \\]\nMarginal distributions can be obtained from the joint distribution. Using the probability function, one needs to sum joint probabilities across the variables he is getting rid of.\nThe idea explained in two dimensions is this: You want, e.g. the probability P(X = 1); this probability is a union of all the joint probabilities containing X = 1, e.g. P(X1 = 1, X2 = 0), P(X1 = 1, X2 = 1), P(X1 = 1, X2 = 2). Because these intersections are disjoint random events, the union is calculated as a simple summation.\n\\[\nP(x_1)=\\sum_{x_2=-\\infty}^\\infty P(x_1, x_2)\n\\]\nThe same logic applies to continuous random variables described by a probability density function but using continuous mathematics. i.e. integration\n\n\\[\nf(x_1)=\\int_{x_2=-\\infty}^\\infty f(x_1, x_2)dx_2\n\\]\nLast but not least comes the relation between marginal and joint cumulative distribution function, which is straightforward\n\\[\nF(x_1)= P(X_1 \\leq x_1)=P(X_1 \\leq x_1 \\cap X_2 &lt; \\infty)= F(x_1,x_2=\\infty)\n\\]\nHaving marginal distributions of the random variables, all their characteristics can be calculated as usual. We will use marginal expected values and marginal variances for the characteristics of a random vector."
  },
  {
    "objectID": "LN4.html#joint-characteristics",
    "href": "LN4.html#joint-characteristics",
    "title": "Random Vectors",
    "section": "Joint characteristics",
    "text": "Joint characteristics\nJoint characteristics, or just characteristics of a random vector, characterise specific properties of a random vector as a whole. Again, for the needs of this course, we simplify and use only the most common characteristics.\nThe location of a random vector is commonly characterised by a vector of expected values, which is a vector composed of marginal expected values of marginal random variables. In two dimensions:\n\\[\nE(\\textbf{X})= \\begin{pmatrix} E(X_1)\\\\ E(X_2) \\end{pmatrix}\n\\]\nA variance-covariance matrix commonly characterises the variability of a random vector. In this matrix, marginal variances are used together with covariances.\n\nCovariance\nCovariance is a measure of the linear relation between the two random variables. It can take any real value; if it is negative, the two random variables are negatively linearly associated; if it is positive, the two random variables are positively linearly associated; if it is zero, the two random variables are not linearly associated. The word linearly is important here - random variables can have non-linear associations, which can be visible using, e.g. regression or skedastic functions. Linearly can be viewed as on average. This will be discussed later in this chapter.\nMathematically, covariance is calculated from the definition\n\\[\ncov(X_1,X_2)=E[(X_1-E(X_1))*(X_2-E(X_2))]\n\\]\nwhich can be transformed into the evaluation formula\n\\[\ncov(X_1, X_2)=E(X_1X_2)-E(X_1)E(X_2)\n\\]\nTo calculate covariance, one can calculate the expected value of the product of the two random variables and subtract the product of their expected values.\nWhat if you wanted to calculate the covariance of the random variable with itself?\n\\[\ncov(X_1,X_1)=E[(X_1-E(X_1))*(X_1-E(X_1))]=E(((X_1-E(X_1))^2\n)=var(X_1)\\]\nor\n\\[\ncov(X_1,X_1)=E(X_1X_1)-E(X_1)E(X_1)=E(X_1^2)-(E(X_1))^2=Var(X_1)\n\\]\nVariance is the specific case of the covariance; thus, the variance-covariance matrix describes all possible covariances.\nThe last important property of covariance is its symmetry; from the evaluation formula\n\\[\ncov(X_1,X_2)=E(X_1X_2)-E(X_1)E(X_2)=E(X_2X_1)-E(X_2)E(X_1)=cov(X_2,X_1)\n\\]\nThe variance-covariance matrix is thus square and symmetric, having marginal variances on the main diagonal. It is also positive semi-definite, an important property for various statistical techniques using sample variance-covariance matrices.\n\\[\n\\boldsymbol{\\Sigma}= \\begin{pmatrix} Var(X_1) & cov(X_1,X_2) \\\\\ncov(X_2,X_1) & Var(X_2)\n\\end{pmatrix}\n\\]\n\n\nCorrelation\nThe covariance describes the direction of the linear association, but its value is unbounded and depends on the units of the random variables. Thus, we cannot compare values of the covariances across different pairs of random variables. Here comes the standardised version of the covariance that bounds its values to the range [-1; 1]. This standardised covariance is called correlation and is calculated as\n\\[\n\\rho(X_1,X_2)=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\frac{cov(X_1,X_2)}{S.D.(X_1)S.D.(X_2)}\n\\]\nThe standardisation leads to a unitless value, which can be compared across different pairs of random variables. Value 0 still means a linearly independent pair of random variables; value -1 means that one random variable is a negative linear function of the other, and value 1 indicates that one random variable is a positive linear function of the other.\nMost commonly, the correlation is somewhere in between. There are no hard bounds to a strong or weak association, even though you might find some simplifying tables with given bounds in various scientific fields. These tables are usually created by looking at the common correlations in the given field. For example, in social sciences, a correlation of 0.6 is typically referred to as strong simply because it is very unusually high value in the social sciences. On the other hand, in physics, a correlation of 0.6 would be considered weak because the correlation coefficients are typically higher than that in physics.\nCorrelation is also connected to the concept of explained variability in the linear regression analysis. This connection will be described in the chapter on two-dimensional normal distribution.\nThe correlation of the random variable with itself yields a value of 1\n\\[\n\\rho(X_1,X_1)=\\frac{cov(X_1,X_1)}{\\sqrt{Var(X_1)*Var(X_1)}}=\n\\frac{Var(X_1)}{Var(X_1)}=1\n\\]\nand the correlation is again symmetric:\n\\[\n\\rho(X_2,X_1)=\\frac{cov(X_2,X_1)}{\\sqrt{Var(X_2)*Var(X_1)}}=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\rho(X_1,X_2)\n\\]\nThis together means that the correlation matrix is squared and symmetric, having values one on the main diagonal. This matrix is also a positive semidefinite\n\\[\n\\boldsymbol{\\rho}= \\begin{pmatrix}\n1 & \\rho(X_1, X_2) \\\\\n\\rho(X_2, X_1) & 1\n\\end{pmatrix}\n\\]\nTo summarise, a random vector’s two main characteristics are a vector of expected values, which describes the location, and a variance-covariance matrix, which describes variability and linear relations inside the vector. The third commonly used characteristic is a correlation matrix, which recalculates a variance-covariance matrix to a standardised form, describing the linear relations inside the vector."
  },
  {
    "objectID": "LN4.html#conditional-distribution",
    "href": "LN4.html#conditional-distribution",
    "title": "Random Vectors",
    "section": "Conditional distribution",
    "text": "Conditional distribution\nThe joint distribution represents the intersection, and the marginal distribution represents the independent distribution so that the last distribution will represent the conditional distribution. The notation is changed; instead of x1 and x2, x and y will be used for better clarity.\nIt is calculated in a similar way we saw with the two random events, with the probability of intersection divided by the probability of a condition:\n\\[\nP(x|y)=\\frac{P(x,y)}{P(y)}\n\\]\n\\[ f(x|y)=\\frac{f(x,y)}{f(y)} \\]\nSo, having joint distribution, we can calculate marginal distribution. And having both joint distribution and marginal distribution, we can calculate conditional distribution. If there is an association between the two variables, the conditional probability function or conditional probability density function will be a function of both the variable (x) and the condition (y).\n\nRegression function\nThe regression function in probability theory is a function of the expected value of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) E(X|y). It is calculated the very same way as any expected value, but using conditional probability function or probability density function\n\\[\nE(X|y)=\\sum_{x=-\\infty}^\\infty x*P(x|y)\n\\]\n\\[ E(X|y)=\\int_{x=-\\infty}^\\infty x*f(x|y)dx \\]\nMathematically, we eliminate X but not Y in the result by integrating or summing across values of random variable X. The conditional expected value E(X|y) is usually, but not necessarily, a function of Y in case there is a relation between the two variables. The conditional expected values are always constants if no association exists between X and Y.\n\n\nSkedastic function\nThe skedastic function in probability theory is a function of the variance of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) Var(X|y). It is calculated the very same way as any variance, but using conditional probability function or probability density function\n\\[ E(X^2|y)=\\sum_{x=-\\infty}^\\infty x^2*P(x|y) \\]\n\\[ E(X^2|y)=\\int_{x=-\\infty}^\\infty x^2*f(x|y)dx \\]\n\\[\nVar(X|y)=E(X^2|y)-[E(X|y)]^2\n\\]\nThe conditional variance Var(X|y) is usually, but not necessarily, a function of Y in case there is a relation between the two variables. The conditional variances are always constants if no association exists between X and Y."
  },
  {
    "objectID": "LN4.html#independence",
    "href": "LN4.html#independence",
    "title": "Random Vectors",
    "section": "Independence",
    "text": "Independence\nWe stumbled upon association and linear independence notions throughout the previous paragraphs. Let´s now formalise it a little.\nIn the same way, we introduced the definition of independence of two random events, we can start here\n\\(P(x|y)=P(x);f(x|y)=f(x);F(x|y)=F(x)\\) for each possible value x and y.\nBasically, we claim that the two random variables are independent if the conditioning on one random variable does not change the probability distribution of the second one.\nFrom this, we can follow with the condition of independence,\n\\[\nP(x,y)=P(x)*P(y)\n\\]\n\\[\nf(x,y)=f(x)*f(y)\n\\]\n\\[\n, soF(x,y)=F(x)*F(y)\n\\]\nThis condition is both necessary and sufficient, meaning that if we can show one of these relations, the two random variables are independent, and if the two random variables are independent, then we can assume these relations.\nCovariance and correlation were introduced as measures of linear association. The regression function was introduced to describe how one random variable’s expected values react to the other’s values. Similarly, the skedastic function describes conditional variances. What are some takeaway points on these descriptions of relations and general independence?\n\nA linear relation can be described as a relation in which the regression function is close to the straight line.\nThe regression function can take any shape, and no linearity is required (!). If the regression function is, on average, increasing, the correlation is positive, and if the regression function is, on average,decreasing, the correlation is negative.\nA higher correlation value is connected to the regression function being close to the straight line and the skedastic function being close to 0.\nSuppose one random variable is a linear function of the other. In that case, the regression is a straight line, and there is no conditional variability, so the skedastic function is equal to 0.\nIf the two random variables are independent, thequalsey are also linearly independent, so covariance and correlation are 0. Also, regression and skedastic functions are constant.\nThe opposite is not necessarily true! There can be non-linear relations such that covariance and correlation equal 0. Two random variables can even be dependent in such a way that regression or skedastic function is constant."
  },
  {
    "objectID": "LN4.html#mutual-information",
    "href": "LN4.html#mutual-information",
    "title": "Random Vectors",
    "section": "Mutual Information",
    "text": "Mutual Information\nEven though the correlation coefficient is the most commonly used measure of the strength of the relation, it has a few flaws, most notably a connection to linearity and a less obviously hard interpretation of values. However, there are other ways to measure the strength of the association of the two random variables.\nThe one introduced here is connected to the information theory and notion of entropy introduced in the chapter on characteristics of random variables. We will continue using this characteristic only to measure the strength of the association of discrete random vectors.\nStatistically speaking, mutual information is measured as the Kullback-Leibler divergence (type of distance measure) between the joint probability distribution and probability distribution under the condition of independence. Kullback-Leibler divergence of one random variable from the other is\n\\[\nD_{KL}(P||Q)=\\sum_{x\\in{X}}P(x)*log \\left( \\frac{P(x)}{Q(x)} \\right)\n\\]\nUsing joint probability function \\(P(x)=P(x,y)\\) and condition of independence \\(Q(x)=P(x)*P(y)\\), we have definition of mutual information\n\\[\nI(X;Y)=\\sum_{y\\in{Y}} \\sum_{x\\in{X}}P(x,y)*log \\left(\n\\frac{P(x,y)} {P(x)*P(y)}\n\\right)\n\\]\nApart from the definition, mutual entropy can be calculated using various formulas. One of them uses entropies of marginal random variables and joint random vector\n\\[\nI(X;Y)=H(X)+H(Y)-H(X;Y)\n\\]\nThe entropy of the random vector is calculated the same way the entropy of random variables\n\\[\nH(X;Y)=-\\sum_{y\\in{Y}} \\sum_{x \\in {X}}P(x,y)*log_b{P(x,y)}\n\\]\nThe formula for calculating mutual information from the entropies shows that mutual information is the information content of both random variables minus the information content of their joint distribution.\nTranslated, it means that mutual information quantifies how much knowing the value of one random variable reduces uncertainty about the other random variable, measured, e.g. in bits, nats or dits.\n\n\n\nMutual information\n\n\nThe mutual information has a clear advantage in covering all types of relations between the two random variables, not only linear. Also, if mutual information is 0, the two random variables are independent. It is also symmetric: \\(I(X;Y)=I(Y;X)\\).\n\nUncertainty coefficients\nMutual information provides information gain in one variable by observing the second variable in bits (or other units depending on the base of logarithms used). The trouble is that the maximal information content depends on the number of possible values of the random variable, as discussed in the chapter on entropy. So sometimes, the need to use the normalised version that can provide us with relative information gain might be useful. The logic behind the uncertainty coefficient is that it will tell us what information gain is in one random variable by knowing the value of the second random variable provided as the percentage of the entropy, and thus maximal possible information gain, of that first random variable.\n\\[\nU(X|Y)=\\frac{I(X,Y)}{H(X)}\\neq U(Y|X)=\\frac{I(Y,X)}{H(Y)}\n\\]\nUncertainty coefficients are not generally symmetric because the two random variables usually have different entropies.\nTo repeat:\n\nAbsolute information gain, mutual entropy, is symmetric.\nRelative information gain, uncertainty coefficient, is not (generally) symmetric.\n\nTo finish this chapter, let´s introduce one of the possible symmetric uncertainty coefficients. This answers the question, what is an average relative information gain in one variable by knowing the value of the second variable, assuming we do not know which of the two is known?\n\\[\nU(X,Y)=\\frac{H(X)*U(X|Y)+H(Y)*U(Y|X)}{H(X)+H(Y)}=\\frac{2*I(X,Y)}{H(X)+H(y)}\n\\]\nThis symmetric uncertainty coefficient is calculated as an entropy-weighted uncertainty coefficients."
  },
  {
    "objectID": "LN5.html",
    "href": "LN5.html",
    "title": "Selected Discrete Distributions",
    "section": "",
    "text": "Random Variables might be viewed as simple possible data-generating models. We have already introduced them as such and looked at the ways to describe their probability distributions and characteristics.\nMany of these models might be generalised under specific conditions. Given some assumptions about the data-generating process, a particular probability distribution might arise. Because some of these assumptions are legitimate and repeated often, we give the names to the arising probability distributions.\nUsually, one of the assumptions is some quantifiable property, which then enters the probability distribution as a so-called parameter; from the other side, the parameter is a numerical constant driving specific shape distribution.\nIn this part, we will focus on some named discrete probability distributions; in the later part, we will look at some named continuous probability distributions.\nWe will focus on expected value and variance to characterise these probability distributions."
  },
  {
    "objectID": "LN5.html#introduction",
    "href": "LN5.html#introduction",
    "title": "Selected Discrete Distributions",
    "section": "",
    "text": "Random Variables might be viewed as simple possible data-generating models. We have already introduced them as such and looked at the ways to describe their probability distributions and characteristics.\nMany of these models might be generalised under specific conditions. Given some assumptions about the data-generating process, a particular probability distribution might arise. Because some of these assumptions are legitimate and repeated often, we give the names to the arising probability distributions.\nUsually, one of the assumptions is some quantifiable property, which then enters the probability distribution as a so-called parameter; from the other side, the parameter is a numerical constant driving specific shape distribution.\nIn this part, we will focus on some named discrete probability distributions; in the later part, we will look at some named continuous probability distributions.\nWe will focus on expected value and variance to characterise these probability distributions."
  },
  {
    "objectID": "LN5.html#bernoulli-distribution",
    "href": "LN5.html#bernoulli-distribution",
    "title": "Selected Discrete Distributions",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nLet´s begin with the simplest probability distribution. In this probability distribution, we assume one random trial ends by observing a specific random event of interest (often called success) or not. Such a random trial, which can end up with two different possible outcomes, is called Bernoulli´s trial after Swiss mathematician Jacob Bernoulli. As we will later see, the name connected to the first descriptions of probabilistic convergences and the discovery of Euler´s constant e.\nThe probability of observing this random event of success is usually denoted as p or \\(\\pi\\). We will stick to the later notation in this course. Because this is the numerical constant that drives the specific shape of the distribution, it is a parameter of Bernoulli’s random variable.\nThe random variable is thus a random variable describing the probability distribution of a single Bernoulli trial. The probability of success is 𝜋, and the probability of failure is \\(1-\\pi\\). Putting success = 1 and failure = 0, we can write the probability function:\n\\[\nP(x)=\\pi^x(1-\\pi)^{1-x}\n\\] for \\(x\\in \\{0;1\\}\\) and \\(\\pi\\in[0;1]\\).\nExpected value and variance can be derived quite simply:\n\\[\nE(X)=\\sum_{x=0}^1x*P(x)=0*\\pi^0*(1-\\pi)^{0-0}+1*\\pi^1*(1-\\pi)^{1-0}=\\pi\n\\] \\[\nE(X^2)=\\sum_{x=0}^1x^2*P(x)=0^2*\\pi^0*(1-\\pi)^{0-0}+1^2*\\pi^1*(1-\\pi)^{1-0}=\\pi\n\\]\n\\[Var(X)=E(X^2)-[E(X)]^2=\\pi-\\pi^2=\\pi(1-\\pi)\\]While for now, the Bernoulli distribution might seem a little dull and uninteresting, it has a prominent place in the probability theory and statistical modelling because it represents a model for the probability itself."
  },
  {
    "objectID": "LN5.html#bernoulli-distribution-xsim-bepi",
    "href": "LN5.html#bernoulli-distribution-xsim-bepi",
    "title": "Selected Discrete Distributions",
    "section": "Bernoulli distribution \\(X\\sim Be(\\pi)\\)",
    "text": "Bernoulli distribution \\(X\\sim Be(\\pi)\\)\nLet´s begin with the simplest probability distribution. In this probability distribution, we assume one random trial ends by observing a specific random event of interest (often called success) or not. Such a random trial, which can end up with two different possible outcomes, is called Bernoulli´s trial after Swiss mathematician Jacob Bernoulli. As we will later see, the name connected to the first descriptions of probabilistic convergences and the discovery of Euler´s constant e.\nThe probability of observing this random event of success is usually denoted as p or \\(\\pi\\). We will stick to the later notation in this course. Because this is the numerical constant that drives the specific shape of the distribution, it is a parameter of Bernoulli’s random variable.\nThe random variable is thus a random variable describing the probability distribution of a single Bernoulli trial. The probability of success is 𝜋, and the probability of failure is \\(1-\\pi\\). Putting success = 1 and failure = 0, we can write the probability function:\n\\[\nP(x)=\\pi^x(1-\\pi)^{1-x}\n\\] for \\(x\\in \\{0;1\\}\\) and \\(\\pi\\in[0;1]\\).\nExpected value and variance can be derived quite simply:\n\\[\nE(X)=\\sum_{x=0}^1x*P(x)=0*\\pi^0*(1-\\pi)^{0-0}+1*\\pi^1*(1-\\pi)^{1-0}=\\pi\n\\] \\[\nE(X^2)=\\sum_{x=0}^1x^2*P(x)=0^2*\\pi^0*(1-\\pi)^{0-0}+1^2*\\pi^1*(1-\\pi)^{1-0}=\\pi\n\\]\n\\[Var(X)=E(X^2)-[E(X)]^2=\\pi-\\pi^2=\\pi(1-\\pi)\\]While for now, the Bernoulli distribution might seem a little dull and uninteresting, it has a prominent place in the probability theory and statistical modelling because it represents a model for the probability itself."
  },
  {
    "objectID": "LN5.html#binomial-distribution-x-sim-binpi",
    "href": "LN5.html#binomial-distribution-x-sim-binpi",
    "title": "Selected Discrete Distributions",
    "section": "Binomial distribution \\(X \\sim Bi(n,\\pi)\\)",
    "text": "Binomial distribution \\(X \\sim Bi(n,\\pi)\\)\nThe binomial distribution is an extension of the Bernoulli distribution. It starts with the Bernoulli trial (two possible outcomes; success (1) has probability \\(\\pi\\)), but now we have n independentBernoulli trials. The random variable is the number of successes in those trials.\nThe probability function of the Binomial distribution is\n\\[\nP(x)={n\\choose{x}}\\pi^x(1-\\pi)^{n-x}\n\\]\nfor \\(x=0,1,...,n\\); \\(\\pi\\in[0;1]\\) and \\(n\\in \\{0,1,2,...\\}\\)\nThe idea behind this probability function goes like this: first, think of the probability of one specific sequence of failures and successes, e.g. {0, 0, 0, 1, 1}. The probability of this sequence is \\((1-\\pi)*(1-\\pi)*(1-\\pi)*\\pi*\\pi=\\pi^2(1-\\pi)^3\\).\nA more general question is the probability of observing two successes no matter the specific order. There are more possible sequences, each with the same probability. Overall probability is the sum of these probabilities. Hence, we multiply the probability of a sequence by the number of possible sequences, which is given by the binomial coefficient \\(n\\choose{x}\\).\nExpected value and variance are\n\\[\nE(X)=n\\pi\n\\]\n\\[\nVar(X)=n\\pi(1-\\pi)\n\\]\nBinomial distribution might be more formally defined as the summation of n Bernoulli distributions:\n\\[\nY\\sim Be(\\pi)\\to X=\\sum_{i=1}^{n}Y_i \\sim Bi(n,\\pi)\n\\]From the other side, Bernoulli distribution might be viewed as a specific case of the binomial distribution for n = 1."
  },
  {
    "objectID": "LN5.html#hypergeometric-distribution-x-sim-hgnkn",
    "href": "LN5.html#hypergeometric-distribution-x-sim-hgnkn",
    "title": "Selected Discrete Distributions",
    "section": "Hypergeometric distribution \\(X \\sim Hg(N,K,n)\\)",
    "text": "Hypergeometric distribution \\(X \\sim Hg(N,K,n)\\)\nThe binomial distribution represents a number of successes in a series of independent Bernoulli trials with the constant probability \\(\\pi\\) of success in one trial. One process that leads to the Bernoulli trials is random sampling with replacement. In this specific sampling, a unit is sampled from the population and then returned so it can be sampled again later.\nIn the setup of sampling from the population, a more common type is sampling without replacement. This means that once a unit is sampled from the population, it is NOT returned and hence cannot be sampled again. Think about sampling at once. In this type of sampling, the probability of success changes at each sampling step, violating the binomial distribution assumption.\nWe can use hypergeometric distribution if we want to find out the probability distribution of the number of successes in the random sampling without replacement from the finite population. The information needed is the sample size n, population size N, and number of successes in the population K. Probability function is\n\\[\nP(x)=\\frac {\\binom{K}{x} \\binom{N-K}{n-x}}{\\binom{N}{n}}\n\\]\nfor \\(x=\\{max(0,n+K-N),...,min(n,K) \\}\\); \\(N\\in\\{0, 1, 2,...\\}\\), \\(K\\in\\{0, 1, 2, ..., N \\}\\), and \\(n\\in \\{0, 1, 2, ..., N \\}\\).\nThe idea behind this probability function is to calculate the number of all possible samples with x successes and divide it by the number of all possible samples. Assuming that sampling is random, each possible sample has the same probability, and the classical definition of probability applies. Characteristics follow\n\\[\nE(X)=\\frac{K}{N}\n\\]\n\\[\nVar(X)=n\\frac{K}{N}\\left(1-\\frac{K}{N}\\right)\\frac{N-n}{N-1}\n\\]\nIn the characteristics, we can see a resemblance to the binomial distribution. If we imagine sampling, starting at the first step, the probability of success is the same: \\(\\pi=\\frac{K}{N}\\). The expected value is the same for both sampling schemes - with or without replacement. But variance is different by the last fraction \\(\\frac{N-n}{N-1}\\). This fraction is called finite population correction.\nOne can also notice that the variance is the same for the sample size n = 1. That is because, with this sampling size, we talk about the Bernoulli distribution."
  },
  {
    "objectID": "LN5.html#poisson-distribution-x-sim-polambda",
    "href": "LN5.html#poisson-distribution-x-sim-polambda",
    "title": "Selected Discrete Distributions",
    "section": "Poisson distribution \\(X \\sim Po(\\lambda)\\)",
    "text": "Poisson distribution \\(X \\sim Po(\\lambda)\\)\nWith this distribution, we are still looking for a number of successes but call it now events. We will assume that those successes are occurring throughout specified intervals randomly and independently of each other with a constant rate of occurrence. This happens mostly in the time domain, but intervals can represent something else, e.g. distance. Under the mentioned assumption, the number of events follows the probability function.\n\\[\nP(x)=e^{-\\lambda}\\frac{\\lambda^x}{x!}\n\\]\nfor \\(x\\in\\{0, 1, 2, ...\\}\\) and \\(\\lambda \\in (0;\\infty)\\).\nCharacteristics are\n\\[\nE(X)=\\lambda\n\\]\n\\[\nVar(X)=\\lambda\n\\]\nParameter \\(\\lambda\\) is called a rate parameter; it defines the expected number of events in a given interval. \\(\\lambda\\) is always connected to the interval’s length and responds to it one-to-one. E.g. if the rate is five events per hour, but we are interested in the number of events per day, we use \\(\\lambda=24*5=120\\).\n\nDetour: convergence \\(Hg\\to Bi\\to Po\\)\nThe three introduced distributions are tightly connected, and a possible order of approximations goes from the computationally most complex (relying on the largest number of parameters) to the least complex. However, there is no free lunch - approximations are not exact results, so we trade the result’s precision with the complexity of calculations; less obviously, we trade parameters for the assumptions.\nHypergeometric distribution can be approximated by binomial distribution if the ratio between sample and population sizes is small enough (the smaller, the better). With this ratio being small, there is only a small difference between sampling with and without replacement. Usually, the boundary for the approximation given is \\(n/N&lt;0.1\\)\n\\[\nHg(N,K,n)\\xrightarrow {n/N\\to0} Bi\\left(n,\\pi=\\frac{K}{N}\\right)\n\\]\nSimilarly, binomial distribution can be approximated by the Poisson distribution if the number of Bernoulli trials is high enough and the probability of success is low enough. In this case, we stop thinking in a sequence of Bernoulli trials and start thinking about the whole sequence as one interval long enough. The boundaries for the approximation are usually in the literature given as \\(n&gt;30,\\pi&lt;0.1\\).\n\\[\nBi(n,\\pi)\\xrightarrow{n \\to \\infty,\\pi \\to 0}Po(\\lambda=n\\pi)\n\\]\nWith these approximations, expectations remain the same. Still, the shape of the probability distribution is changing, and variance is increasing, leading to overestimated probability in the tails and underestimated probability in the centre of the distribution.\n\\[\nE(X)=n\\frac{K}{N}=n\\pi=\\lambda\n\\]\n\\[\nVar(X)=n\\frac{K}{N}\\left(1-\\frac{K}{N} \\right)\\frac{N-n}{N-1}&lt;n\\pi(1-\\pi)&lt;\\lambda\n\\]"
  },
  {
    "objectID": "LN5.html#geometric-distribution",
    "href": "LN5.html#geometric-distribution",
    "title": "Selected Discrete Distributions",
    "section": "Geometric distribution",
    "text": "Geometric distribution"
  },
  {
    "objectID": "LN5.html#geometric-distribution-xsim-gepi",
    "href": "LN5.html#geometric-distribution-xsim-gepi",
    "title": "Selected Discrete Distributions",
    "section": "Geometric distribution \\(X\\sim Ge(\\pi)\\)",
    "text": "Geometric distribution \\(X\\sim Ge(\\pi)\\)\nLet´s return to the Bernoulli trials. But now, our interest is in the number of failures before observing the first success. This random variable follows a geometric distribution with the probability function.\n\\[\nP(x)=\\pi*(1-\\pi)^x\n\\]\nfor \\(x\\in\\{0,1,2,... \\}\\) and \\(\\pi\\in [0;1]\\)\nThis probability function has a straightforward meaning: what is the probability of observing a sequence \\(\\{ 0,0,...,1 \\}\\), where the number of 0s is the value of the random variable x.\nCharacteristics are\n\\[\nE(X)=\\frac{\\ 1 - \\pi}{\\pi}\n\\]\n\\[\nVar(X)=\\frac{1-\\pi}{\\pi^2}\n\\]\nThis probability distribution can be connected to the binomial distribution. Using the same parameter \\(\\pi\\), we can see that the number of trials in the binomial distribution is,\\(n=x_{ge}\\) and the \\(x_{bi}=0\\) will give us the probability of observing 0 successes in a sequence. This has to be multiplied by one probability of success to have the end of the sequence equal 1. Imagine we want to see five failures before the first success in geometric distribution:\n\\[\nX_{ge} \\sim Ge(\\pi) \\to P(X_{ge}=5)=\\pi*(1-\\pi)^5\n\\]\nUsing binomial distribution:\n\\[\nX_{bi} \\sim Bi(n=x_{ge}=5;\\pi) \\to \\pi*P(0)=\\pi* \\binom{5}{0}\\pi^0(1-\\pi)^{5-0}=\\pi*(1-\\pi)^5\n\\]"
  },
  {
    "objectID": "LN5.html#negative-binomial-distribution-x-sim-nbirpi",
    "href": "LN5.html#negative-binomial-distribution-x-sim-nbirpi",
    "title": "Selected Discrete Distributions",
    "section": "Negative binomial distribution \\(X \\sim NBi(r,\\pi)\\)",
    "text": "Negative binomial distribution \\(X \\sim NBi(r,\\pi)\\)\nSimilarly, as we generalised from the Bernoulli to the binomial distribution, we can generalise from the geometric distribution to the negative binomial distribution. Having a probability of success \\(\\pi\\), we are interested in the number of failures before the r-th success.\nThe probability function has a very similar construction to the one from the binomial distribution - the probability of one sequence, e.g. \\(\\{0, 0, 0,0,1,1 \\}\\) is \\(\\pi^2*(1-\\pi)^3\\). The sequence has to end with success because that´s what we are waiting for. However, more than one sequence has two successes and ends with one (of those two), specifically \\(\\binom{4}{1}\\), because the last position is given, and we combine one success in 4 positions. All these sequences will have the same probability; the overall probability is the summation of these, which means \\(\\binom{4}{1}\\pi^2(1-\\pi)^4\\).\nGeneralising this thought process, wanting to have r successes, we can write probability function as\n\\[\nP(x)=\\binom{x+r-1}{x}\\pi^r(1-\\pi)^x\n\\]\nCharacteristics are\n\\[\nE(x)=r\\frac{1-\\pi}{\\pi}\n\\]\n\\[\nVar(X)=r\\frac{1-\\pi}{\\pi^2}\n\\]\nA negative binomial distribution might be formally defined as a summation of the geometric distributions, the same way the summation of Bernoulli distributions led to a binomial.\n\\[\nY\\sim Ge(\\pi)\\to X=\\sum_{i=1}^{r}Y_i \\sim NBi(r,\\pi)\n\\]\nA geometric distribution might be considered a special negative binomial case.\nThe last relation in this part is the one between binomial and negative binomial distributions. From the example looking for the probability of observing the second success after the third failure, we can translate it to the binomial distribution: ignore the last position of a sequence, which is always given as success, then \\(\\pi\\) is shared, the number of trials is the number of failures and successes minus one, \\(n=x_{NBi}+r-1\\) and we are looking for the probability of \\(x_{Bi}=r-1\\). The last step is multiplying the result by the last position, hence the probability of success \\(\\pi\\).\n\\[\nX_{Bi} \\sim Bi(n=r+x_{NBi}-1=4,\\pi)\\to \\pi*P(r-1=1)= \\binom{4}{1}\\pi^2(1-\\pi)^3\n\\]\nThe only distinction from the formula for a negative binomial distribution is in binomial coefficient, but they are always equal\n\\[\n\\binom{4}{1}=\\frac{4!}{1!3!}=\\binom{4}{3}\n\\]"
  },
  {
    "objectID": "LN6.html",
    "href": "LN6.html",
    "title": "Selected Continuous Distributions",
    "section": "",
    "text": "In this part, we will continue with specific named probability distributions but focus on some continuous probability distributions."
  },
  {
    "objectID": "LN6.html#introduction",
    "href": "LN6.html#introduction",
    "title": "Selected Continuous Distributions",
    "section": "",
    "text": "In this part, we will continue with specific named probability distributions but focus on some continuous probability distributions."
  },
  {
    "objectID": "LN6.html#continuous-uniform-distribution-xsim-uab",
    "href": "LN6.html#continuous-uniform-distribution-xsim-uab",
    "title": "Selected Continuous Distributions",
    "section": "Continuous uniform distribution \\(X\\sim U(a,b)\\)",
    "text": "Continuous uniform distribution \\(X\\sim U(a,b)\\)\nThis is likely the simplest continuous probability distribution. Uniform means equal probabilities for a discrete case or constant probability density function for a continuous case.\nThis distribution has limited real-world use, e.g. as a model of the waiting time for the regularly repeating event. Maybe the more important role of this distribution is its use in generating random numbers. Having generated a random value from the \\(X\\sim U(0,1)\\) distribution, it can be an input to the quantile function of any desired random variable, generating a random value from that random variable.\nFrom the constant probability density function, we can derive that it is\n\\[\nf(x)=\\frac{1}{b-a}\n\\]\nfor \\(x\\in [a,b]\\) and \\(-\\infty&lt;a&lt;b&lt;\\infty\\).\nFor probability calculations, the cumulative distribution function is usually easier to use\n\\[\nF(x)=\\frac{x-a}{b-a}\n\\]\nand the quantile function, the inverse of the cumulative distribution function\n\\[\nQ(P)=a+(b-a)P\n\\]\nThere is not much more to be said about this distribution; maybe the most common special case 𝑋∼𝑈(0,1) is called standard uniform distribution."
  },
  {
    "objectID": "LN6.html#exponential-distribution-xsim-elambda",
    "href": "LN6.html#exponential-distribution-xsim-elambda",
    "title": "Selected Continuous Distributions",
    "section": "Exponential distribution \\(X\\sim E(\\lambda)\\)",
    "text": "Exponential distribution \\(X\\sim E(\\lambda)\\)\nLet´s assume that random events are not regularly repeating but occurring randomly throughout time and independently of each other. The rate of occurrence of these events is the parameter \\(\\lambda\\), the one we have already seen in the Poisson distribution with the same assumptions.\nThe focus of the Poisson distribution is the number of events in a specified interval; the focus of the exponential distribution is time to the first occurrence of the event. With the mentioned assumptions, this time follows the exponential distribution described by the following functions.\n\\[\nf(x)=\\lambda \\exp(-\\lambda x)\n\\]\n\\[\nF(x)=1-\\exp(-\\lambda x)\n\\]\n\\[\nQ(P)=-\\frac{-\\ln (1-P)}{\\lambda}\n\\]\nfor \\(x\\in(0;\\infty)\\) and \\(\\lambda \\in [0,\\infty)\\).\nThe two main moment characteristics of this distribution are\n\\[\nE(X)=\\frac{1}{\\lambda}\n\\]\n\\[\nVar(X)=\\frac{1}{\\lambda^2}\n\\]\nThe exponential and the Poisson distributions are intertwined and serve as the basis for analysing the so-called Poisson process. An exponential distribution is useful in other contexts; for example, it can serve as a model of the right tail of an unknown underlying distribution, which is useful in risk modelling.\nIn some distributions, especially continuous ones, parameters are not set in stone, and different parametrisations of the distribution can be used to shift focus on various properties. Of course, all the functions describing the probability distribution and its characteristics must be adjusted to the parametrisation of choice.\nFor exponential distribution, one might stumble upon using the scale parameter, which is the inverse of the rate parameter \\(\\sigma=\\frac{1}{\\lambda}\\). This parametrisation might be useful in contexts where the focus is not the random events’ occurrence rate but the expected value because \\(E(X)=\\delta\\)."
  },
  {
    "objectID": "LN6.html#normal-gaussian-distribution-xsim-nmusigma",
    "href": "LN6.html#normal-gaussian-distribution-xsim-nmusigma",
    "title": "Selected Continuous Distributions",
    "section": "Normal (Gaussian) distribution \\(X\\sim N(\\mu,\\sigma)\\)",
    "text": "Normal (Gaussian) distribution \\(X\\sim N(\\mu,\\sigma)\\)\nThe normal distribution is pervasive in statistics because of its presence as a limited distribution of various statistics, most importantly summation. Normality assumption also simplifies a lot of calculations due to its properties. This makes it very useful in many contexts regarding estimates, which is the topic of the second large part of this lecture notes.\nThe probability density of the normal distribution is\n\\[\nf(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\nFor \\(x \\in (-\\infty;\\infty)\\) and \\(\\mu \\in (-\\infty, \\infty)\\), \\(\\sigma \\in(0,\\infty)\\).\nIntegrating this density to get the cumulative distribution function is not analytically solvable, so we do not have a simple closed formula for evaluating its value.\n\\[\nF(x)=\\int_{-\\infty}^x \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(t-\\mu)^2}{2\\sigma^2}}dt\n\\]\nWe do not have the closed formula for the quantile function based on the same argument. Characteristics of this random variable are straightforward from the parameters.\n\\[\nE(X)=\\mu\n\\]\n\\[\nVar(X)=\\sigma^2\n\\]\nNowadays, obtaining probabilities and quantiles of normal distribution is an easy task due to the computational power of computers. Historically, it was a little more complicated, though. Numerical evaluation of the integrals was a tiresome task. Still, luckily for the normal distribution, our ancestors could use one property - any linear transformation of the normal distribution is also normally distributed.\nThat means that any normal distribution could be transformed by standardisation (recall standard moments), and the result would be a standard normal distribution with zero expected value and unit variance. This distribution is so prominent that it has its specific notation: \\(Z\\sim N(0,1)\\).\nSo, the need to calculate the integrals was reduced to only one distribution, and results were tabulated for general use. Hence, for \\(X\\sim N(\\mu, \\sigma)\\), we can write\n\\[\nZ=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\n\\]\n\\[\nF(x)= \\Phi \\left(\\frac {x-\\mu}{\\sigma}\\right)=\\Phi(z)\n\\]\n\\[\nx_p=\\mu+\\sigma z_p\n\\]\nWhere \\(\\Phi\\) is a notation for the cumulative distribution function of standard normal distribution \\(Z \\sim N(0,1)\\), and \\(z_p\\) is the 100P% quantile of the standard normal distribution, both to be found in the appropriate tables.\nAnother property of normal distribution worth mentioning is its symmetry around the expected value. For standard normal distribution, it means symmetry around 0, which leads to two relations used to simplify the tables mentioned above:\n\\[\n\\Phi(z)=1-\\Phi(-z)\n\\]\n\\[\nz_p=-z_{1-p}\n\\]\nThe standard normal distribution has become so pervasive that it is still used in statistics, e.g., in the form of z-scores or critical values for constructing rejection regions.\nZ-scores show how many standard deviations the value is from the variable’s mean (or the random variable´s expected value to stay in probability theory)."
  },
  {
    "objectID": "LN6.html#log-normal-galton-distribution-x-sim-lnmusigma",
    "href": "LN6.html#log-normal-galton-distribution-x-sim-lnmusigma",
    "title": "Selected Continuous Distributions",
    "section": "Log-normal (Galton) distribution \\(X \\sim LN(\\mu,\\sigma)\\)",
    "text": "Log-normal (Galton) distribution \\(X \\sim LN(\\mu,\\sigma)\\)\nIf the normal distribution rises as the distribution of a summation, there has to exist the distribution of a product. This is a log-normal distribution. The name suggests its relation to the normal distribution.\n\\[\nX\\sim LN(\\mu,\\sigma) \\to Y=\\log (X) \\sim N(\\mu, \\sigma)\n\\]\n\\[\nY\\sim N(\\mu,\\sigma) \\to X=\\exp (Y) \\sim LN(\\mu, \\sigma)\n\\]\nAgain, the cumulative distribution and quantile function do not have an analytical solution; hence, the relation was used.\n\\[\nZ=\\frac{\\log(x)-\\mu}{\\sigma} \\sim N(0,1)\n\\] \\[x_p=e^{\\mu+\\sigma z_p}\\]\nYou can notice that parameters are the expected value and standard deviation of the log-transformed version of this distribution, not directly equal to them. Characteristics have to be obtained using formulas.\n\\[\nE(X)=e^{\\mu+\\frac {\\sigma^2}{2}}\n\\]\n\\[\nVar(X)=e^{2\\mu+\\sigma^2}*\\left(e^{\\sigma^2} -1\\right)\n\\]\nThis distribution is well-known and often used in statistical modelling for its shape - it is right-skewed and heavy-tailed, meaning it is suitable for the distribution with possible occurrences of extreme values."
  },
  {
    "objectID": "LN7.html",
    "href": "LN7.html",
    "title": "Two Selected Multivariate Distributions",
    "section": "",
    "text": "This part will continue with two specific named multivariate probability distributions."
  },
  {
    "objectID": "LN7.html#introduction",
    "href": "LN7.html#introduction",
    "title": "Two Selected Multivariate Distributions",
    "section": "",
    "text": "This part will continue with two specific named multivariate probability distributions."
  },
  {
    "objectID": "LN7.html#multinomial-distribution-xsim-mnnboldsymbolpi",
    "href": "LN7.html#multinomial-distribution-xsim-mnnboldsymbolpi",
    "title": "Two Selected Multivariate Distributions",
    "section": "Multinomial distribution \\(X\\sim Mn(n,\\boldsymbol{\\pi})\\)",
    "text": "Multinomial distribution \\(X\\sim Mn(n,\\boldsymbol{\\pi})\\)\nIn binomial distribution, Bernoulli´s random trials produced two possible outcomes - 0, 1, and counting how many 1s (successes) we can observe. The multinomial distribution is a generalisation of this idea.\nOne random trial can (has to) end up with exactly one of the k possible outcomes \\(i\\) with given probabilities \\(\\pi_i\\) for \\(i=1,2,...,k\\). If we repeat this random trial n times and observe absolute frequencies \\(x_i\\) of the occurrences of the possible outcomes, the probability distribution of a random vector \\(\\textbf{X}=(X_1,X_2,...,X_k)´\\) is called multinomial. Its probability function is\n\\[\nP\\left(\\textbf{X}= \\left( \\begin{matrix}\nx_1 \\\\\nx_2 \\\\\n... \\\\\nx_k\n\\end{matrix} \\right)=\\textbf{x} \\right) =\n\\frac{n!}{x_1!x_2!...x_k!}\\pi_1^{x_1}\\pi_2^{x_2}...\\pi_3^{x_3}\n\\]\nFor \\(x_i \\in \\{0, 1, 2,...,n\\}; \\sum_{i=1}^kx_i=n\\) and \\(n\\in \\{1,2,... \\}\\), \\(\\pi_i \\in (0;1);\\sum_{i=1}^k \\pi_i=1\\).\nThe characteristics of the random vector are\n\\[\nE(\\textbf{X})= \\left( \\begin{matrix}\nn\\pi_1 \\\\\nn\\pi_1 \\\\\n... \\\\\nn\\pi_k\n\\end{matrix} \\right) = n\\boldsymbol{\\pi}\n\\]\n\\[\n\\boldsymbol{\\Sigma} = \\left( \\begin{matrix}\nn\\pi_1(1-\\pi_1) & -n\\pi_1\\pi_2 &... & -n\\pi_1\\pi_k \\\\\n-n\\pi_2\\pi_1 & n\\pi_2(1-\\pi_2) &...& -n\\pi_2\\pi_k \\\\\n... &... &... &... \\\\\n-n\\pi_k\\pi_1 & -n\\pi_k\\pi_2 & ...& n\\pi_k(1-\\pi_k)\n\\end{matrix}\n\\right)\n\\]\nIf we put k = 2, the distribution is identical to the binomial distribution, only differently written."
  },
  {
    "objectID": "LN7.html#multivariate-normal-distribution-textbfx-sim-nkboldsymbolmuboldsymbolsigma",
    "href": "LN7.html#multivariate-normal-distribution-textbfx-sim-nkboldsymbolmuboldsymbolsigma",
    "title": "Two Selected Multivariate Distributions",
    "section": "Multivariate Normal Distribution \\(\\textbf{X} \\sim N^k(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\)",
    "text": "Multivariate Normal Distribution \\(\\textbf{X} \\sim N^k(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\)\nThe multivariate normal distribution is a generalised version of the normal distribution to k dimensions. Its vector parameters are characteristics at the same time.\n\\[\nE(\\textbf{X})= \\left( \\begin{matrix}  \n\\mu_1 \\\\\n\\mu_2 \\\\\n... \\\\\n\\mu_k\n\\end{matrix}\n\\right)\n\\]\n\\[\n(\\boldsymbol\\Sigma)= \\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} & ... &\\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_2^2 & ... & \\sigma_{2k} \\\\\n...&...&...&... \\\\\n\\sigma_{k1} & \\sigma_{k2} & ... & \\sigma_k^2\n\\end{matrix}\n\\right)\n\\]\nYou can see that the variance of the i-th random variable is noted as \\(\\sigma^2\\), and the covariance between the i-th and j-th random variables is noted as \\(\\sigma_{ij}\\). This notation is fairly common in the older probability theory texts as a general notation for variance and covariance. It is, however, reserved only for the (multivariate) normal distribution in these lecture notes.\nFor now, we will focus on the case of a two-variate (two-dimensional) normal distribution, k = 2.\nOne of the nice properties of a multivariate normal distribution is the stability of the normality of any linear transformation. In the end, it means that every marginal distribution is (multivariate) normal, and every conditional distribution is (multivariate) normal.\nMarginal characteristics are taken from the joint characteristics, and conditional characteristics, regression and skedastic functions are as follows:\n\\[\nE(X_1|x_2)=\\mu_1+\\rho \\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2)\n\\]\n\\[\nVar(X_1|x_2)=\\sigma_1^2\\left(1-\\rho^2\\right)\n\\]\nThe regression function is a linear function of \\(x_2\\), and the formula expresses its reliance on the correlation - if the correlation is negative, it has a negative slope and vice versa.\nThe skedastic function shows the meaning of the correlation coefficient - its squared value tells us how much is the variance of \\(X_1\\) reduced by knowing the value \\(x_2\\). In linear regression analysis, this concept is known as R-squared and measures the fit of the regression function.\nThe last but not least important property to note here is that in a multivariate normal distribution, all the relations are linear, and the correlation coefficient is thus a correct measure of the strength of the association."
  },
  {
    "objectID": "LN7 - kopie.html",
    "href": "LN7 - kopie.html",
    "title": "Stochastic convergences",
    "section": "",
    "text": "We are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process."
  },
  {
    "objectID": "LN7 - kopie.html#introduction",
    "href": "LN7 - kopie.html#introduction",
    "title": "Stochastic convergences",
    "section": "",
    "text": "We are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process."
  },
  {
    "objectID": "LN7 - kopie.html#convergence-in-probability-almost-sure-convergence",
    "href": "LN7 - kopie.html#convergence-in-probability-almost-sure-convergence",
    "title": "Stochastic convergences",
    "section": "Convergence in probability & almost sure convergence",
    "text": "Convergence in probability & almost sure convergence\nStandard deterministic convergence usually claims something about the limit of a sequence for an increasing value of the variable, e.g.\n\\[\n\\frac{1}{n} \\xrightarrow{n\\to\\infty}0\n\\]\n\\[\\lim_{n\\to\\infty}\\left(\\frac{1}{n}\\right)=0\\]\nIn stochastic convergences, our limit is usually put upon some probability, not the variable’s value. For convergence in probability, we start with a sequence of random variables \\(X_n\\) and claim its convergence in probability to its limit \\(X\\):\n\\[\nX_n\\xrightarrow{n\\to\\infty;P}X\n\\]\n\\[\np\\lim(X_n)=X\n\\]\nThe definition of in probability convergence is done after specifying some small positive constant \\(\\varepsilon&gt;0\\)\n\\[\nP(|X_n-X|&lt;\\varepsilon)\\xrightarrow{n\\to\\infty}1\n\\]\nor inversely\n\\[\nP(|X_n-X|&gt;\\varepsilon)\\xrightarrow{n\\to\\infty}0\n\\]\nThe first formulation stresses that the probability of absolute deviation of a sequence from its limit being up to the constant \\(\\varepsilon\\) increases; that is, the sequence converges long-term. The second formulation stresses that the probability of such deviation or larger is becoming smaller. That can be translated as saying that the probability of an unusual outcome decreases.\n\nBernoulli´s law\nThe first use of the in probability convergence is Bernoulli´s first formulation of the weak law of large numbers: Let \\(X_1,X_2,...,X_n\\) be a sequence of independent random variables with Bernoulli distribution, \\(X_i\\sim Be(\\pi)\\).\nLet a sequence of relative frequencies be calculated from these random variables that is, \\(P_n=\\frac{\\sum_{i=1}^{n}(X_i)}{n}\\). This sequence will converge in probability to the \\(\\pi\\).\n\\[\nP_n \\xrightarrow{n\\to\\infty;P}\\pi\n\\]\nThis is akin to saying that the larger the sample size, the closer the relative frequency is to the underlying probability in probability.\n\n\nKnichin´s law\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\).\nLet a sequence of means be calculated from these random variables, that is, \\(\\overline{X_n}=\\frac{\\sum_{i=1}^{n}(X_i)}{n}\\). This sequence will converge in probability to the expected value \\(E(X)\\).\n\\[ \\overline{X_n} \\xrightarrow{n\\to\\infty;P}E(X) \\]\nThis is akin to saying that the larger the sample size, the closer the mean is to the underlying expected value in probability.\n\n\nKolmogorov´s law\nSo far, we have used in probability convergence. However, mathematical development in the early 20th century led to a different type of stochastic convergence, defined and proven for the two previous convergences, called almost sure (a.s.) convergence. This convergence is defined as\n\\[\nP(\\lim_{n\\to\\infty}X_n=X)=1\n\\]\nIn this type of convergence, the probability is not converging to 1; rather, the probability of convergence at some step n is 1. This claim is stronger than the previous one and leads to the so-called strong law of large numbers, also known as Kolmogorov´s law:\n\\[ \\overline{X_n} \\xrightarrow{n\\to\\infty;a.s.}E(X) \\]\nThis fundamental theoretical development, along with the ease of use, basically cemented the frequentist statistic as the default inferential procedure for more than half a century."
  },
  {
    "objectID": "LN7 - kopie.html#convergence-in-distribution",
    "href": "LN7 - kopie.html#convergence-in-distribution",
    "title": "Stochastic convergences",
    "section": "Convergence in distribution",
    "text": "Convergence in distribution\nConvergences in probability or almost surely state the limit value of a sequence of random variables. Convergence in distribution state the limit distribution of values of a sequence of random variables.\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\), and cumulative distribution function \\(F(x)\\).\nFrom this sequence of random variables, let´s have a sequence of cumulative distribution functions \\(F_n(x)\\). Convergence in distribution states that this sequence of \\(F_n(x)\\) converges to a specific underlying cumulative distribution function \\(F(x)\\).\n\\[\nF_n(x)\\xrightarrow{n\\to\\infty;D}F(x)\n\\]\nOne of the results is that the larger the dataset, the closer the empirical cumulative distribution function (cumulative relative frequencies) is to the theoretical cumulative distribution function of a generating process.\n\nde-Moivre-Laplace central limit theorem\nThis theorem states the limit distribution of binomial distribution: Let \\(X_1,X_2,...,X_n\\) be a sequence of independent random variables with Bernoulli distribution, \\(X_i\\sim Be(\\pi)\\).\nLet a sequence of summations be calculated from these random variables, that is, \\(M_n=\\sum_{i=1}^{n}(X_i)\\). This summation follows binomial distribution but also converges in distribution to the normal distribution:\n\\[\nM_n\\sim Bi(n,\\pi)\\xrightarrow{n\\to\\infty;D}N \\left(n\\pi,\\sqrt{n\\pi(1-\\pi)}\\right)\n\\]\nWe can make a similar claim about a sequence of means by linear transformation.\n\\[\\overline{X_n}=P=\\frac{\\sum_{i=1}^{n} X_i}{n}\\xrightarrow{n\\to\\infty;D}N \\left(\\pi,\\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\right)\\]One of the results is that for n large enough, we might approximate the binomial probabilities using the normal distribution. We denote this type of approximative distribution using a double-wave symbol \\(\\approx\\):\n\\[\n\\sum_{i=1}^nX_i \\approx N\\left(n\\pi, \\sqrt{n\\pi(1-\\pi)}  \\right)\n\\]\n\\[\nP_n\\approx N \\left(\\pi,\\sqrt{\\frac{\\pi(1- \\pi)}{n}}  \\right)\n\\]\n\nContinuity correction\nApproximating binomial distribution, which is integer-valued, with a continuous normal distribution, might pose an interesting problem. This problem arises with a question formulation: for binomial distribution, the question of what is the probability of being less or equal to x is the same as the question of what is the probability of being less than x+1; for normal distribution, however, these are two different questions because there is now a probability between x and x+1.\n\\[\nBi:P(X\\leq x)=P(X&lt;x+1)\n\\]\n\\[ N:F(x)\\neq F(x+1) \\]\nThe solution to this problem is straightforward: when the question is about less or equal, we add 1/2; when the question is about less, we subtract 1/2. By this operation, we get the same result no matter how the question is posed:\n\\[\nF(x+0.5)=F(x+1-0.5)\n\\]\nThis is called continuity correction and is applicable any time we want to approximate an integer-valued random variable with a continuous one. If the random variable is discrete but not an integer-valued, we want to meet half the distance between the two values.\nThis applies, for example, to the means of discrete variables (and relative frequency as well), where we can derive continuity correction this way:\n\\[\\frac{\\sum_{i=1}^n X_i \\pm1/2}{n}=\\overline{X} \\pm\\frac{1}{2n}\\]\n\n\n\nLindenberg-Lévy central limit theorem\nSimilarly, as Knichin´s law is a generalisation of Bernoulli´s law, Lindenberg-Lévy’s theorem generalises the de-Moivre-Laplace theorem.\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\).\nLet a sequence of summations be calculated from these random variables, that is, \\(M_n=\\sum_{i=1}^{n}(X_i)\\). This sequence convergences in distribution to the normal distribution:\n\\[\nM_n \\xrightarrow{n\\to\\infty;D}N\\left(nE(X),\\sqrt{nVar(X)}\\right)\n\\]\nAnd by linear transformation, the sequence of arithmetic means also converges to the normal distribution:\n\\[\\overline{X_n}=\\frac{\\sum_{i=1}^n X_i}{n} \\xrightarrow{n\\to\\infty;D}N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)\\]One of the results is that for n large enough, we might use a normal distribution to calculate probabilities of sample means. Again, the approximation notation is commonly used:\n\\[ M_n \\approx N\\left(nE(X),\\sqrt{nVar(X)}\\right) \\]\n\\[\\overline{X_n}\\approx N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)\\]"
  },
  {
    "objectID": "LN8.html",
    "href": "LN8.html",
    "title": "Stochastic convergences",
    "section": "",
    "text": "We are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process."
  },
  {
    "objectID": "LN8.html#introduction",
    "href": "LN8.html#introduction",
    "title": "Stochastic convergences",
    "section": "",
    "text": "We are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process."
  },
  {
    "objectID": "LN8.html#convergence-in-probability-almost-sure-convergence",
    "href": "LN8.html#convergence-in-probability-almost-sure-convergence",
    "title": "Stochastic convergences",
    "section": "Convergence in probability & almost sure convergence",
    "text": "Convergence in probability & almost sure convergence\nStandard deterministic convergence usually claims something about the limit of a sequence for an increasing value of the variable, e.g.\n\\[\n\\frac{1}{n} \\xrightarrow{n\\to\\infty}0\n\\]\n\\[\\lim_{n\\to\\infty}\\left(\\frac{1}{n}\\right)=0\\]\nIn stochastic convergences, our limit is usually put upon some probability, not the variable’s value. For convergence in probability, we start with a sequence of random variables \\(X_n\\) and claim its convergence in probability to its limit \\(X\\):\n\\[\nX_n\\xrightarrow{n\\to\\infty;P}X\n\\]\n\\[\np\\lim(X_n)=X\n\\]\nThe definition of in probability convergence is done after specifying some small positive constant \\(\\varepsilon&gt;0\\)\n\\[\nP(|X_n-X|&lt;\\varepsilon)\\xrightarrow{n\\to\\infty}1\n\\]\nor inversely\n\\[\nP(|X_n-X|&gt;\\varepsilon)\\xrightarrow{n\\to\\infty}0\n\\]\nThe first formulation stresses that the probability of absolute deviation of a sequence from its limit being up to the constant \\(\\varepsilon\\) increases; that is, the sequence converges long-term. The second formulation stresses that the probability of such deviation or larger is becoming smaller. That can be translated as saying that the probability of an unusual outcome decreases.\n\nBernoulli´s law\nThe first use of the in probability convergence is Bernoulli´s first formulation of the weak law of large numbers: Let \\(X_1,X_2,...,X_n\\) be a sequence of independent random variables with Bernoulli distribution, \\(X_i\\sim Be(\\pi)\\).\nLet a sequence of relative frequencies be calculated from these random variables that is, \\(P_n=\\frac{\\sum_{i=1}^{n}(X_i)}{n}\\). This sequence will converge in probability to the \\(\\pi\\).\n\\[\nP_n \\xrightarrow{n\\to\\infty;P}\\pi\n\\]\nThis is akin to saying that the larger the sample size, the closer the relative frequency is to the underlying probability in probability.\n\n\nKnichin´s law\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\).\nLet a sequence of means be calculated from these random variables, that is, \\(\\overline{X_n}=\\frac{\\sum_{i=1}^{n}(X_i)}{n}\\). This sequence will converge in probability to the expected value \\(E(X)\\).\n\\[ \\overline{X_n} \\xrightarrow{n\\to\\infty;P}E(X) \\]\nThis is akin to saying that the larger the sample size, the closer the mean is to the underlying expected value in probability.\n\n\nKolmogorov´s law\nSo far, we have used in probability convergence. However, mathematical development in the early 20th century led to a different type of stochastic convergence, defined and proven for the two previous convergences, called almost sure (a.s.) convergence. This convergence is defined as\n\\[\nP\\left(\\lim_{n\\to\\infty}X_n=X\\right)=1\n\\]\nIn this type of convergence, the probability is not converging to 1; rather, the probability of convergence at some step n is 1. This claim is stronger than the previous one and leads to the so-called strong law of large numbers, also known as Kolmogorov´s law:\n\\[ \\overline{X_n} \\xrightarrow{n\\to\\infty;a.s.}E(X) \\]\nThis fundamental theoretical development, along with the ease of use, basically cemented the frequentist statistic as the default inferential procedure for more than half a century."
  },
  {
    "objectID": "LN8.html#convergence-in-distribution",
    "href": "LN8.html#convergence-in-distribution",
    "title": "Stochastic convergences",
    "section": "Convergence in distribution",
    "text": "Convergence in distribution\nConvergences in probability or almost surely state the limit value of a sequence of random variables. Convergence in distribution state the limit distribution of values of a sequence of random variables.\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\), and cumulative distribution function \\(F(x)\\).\nFrom this sequence of random variables, let´s have a sequence of cumulative distribution functions \\(F_n(x)\\). Convergence in distribution states that this sequence of \\(F_n(x)\\) converges to a specific underlying cumulative distribution function \\(F(x)\\).\n\\[\nF_n(x)\\xrightarrow{n\\to\\infty;D}F(x)\n\\]\nOne of the results is that the larger the dataset, the closer the empirical cumulative distribution function (cumulative relative frequencies) is to the theoretical cumulative distribution function of a generating process.\n\nde Moivre-Laplace central limit theorem\nThis theorem states the limit distribution of binomial distribution: Let \\(X_1,X_2,...,X_n\\) be a sequence of independent random variables with Bernoulli distribution, \\(X_i\\sim Be(\\pi)\\).\nLet a sequence of summations be calculated from these random variables, that is, \\(M_n=\\sum_{i=1}^{n}(X_i)\\). This summation follows binomial distribution but also converges in distribution to the normal distribution:\n\\[\nM_n\\sim Bi(n,\\pi)\\xrightarrow{n\\to\\infty;D}N \\left(n\\pi,\\sqrt{n\\pi(1-\\pi)}\\right)\n\\]\nWe can make a similar claim about a sequence of means by linear transformation.\n\\[\\overline{X_n}=P=\\frac{\\sum_{i=1}^{n} X_i}{n}\\xrightarrow{n\\to\\infty;D}N \\left(\\pi,\\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\right)\\]One of the results is that for n large enough, we might approximate the binomial probabilities using the normal distribution. We denote this type of approximative distribution using a double-wave symbol \\(\\approx\\):\n\\[\n\\sum_{i=1}^nX_i \\approx N\\left(n\\pi, \\sqrt{n\\pi(1-\\pi)}  \\right)\n\\]\n\\[\nP_n\\approx N \\left(\\pi,\\sqrt{\\frac{\\pi(1- \\pi)}{n}}  \\right)\n\\]\n\nContinuity correction\nApproximating binomial distribution, which is integer-valued, with a continuous normal distribution, might pose an interesting problem. This problem arises with a question formulation: for binomial distribution, the question of what is the probability of being less or equal to x is the same as the question of what is the probability of being less than x+1; for normal distribution, however, these are two different questions because there is now a probability between x and x+1.\n\\[\nBi:P(X\\leq x)=P(X&lt;x+1)\n\\]\n\\[ N:F(x)\\neq F(x+1) \\]\nThe solution to this problem is straightforward: when the question is about less or equal, we add 1/2; when the question is about less, we subtract 1/2. By this operation, we get the same result no matter how the question is posed:\n\\[\nN+CC:F(x+0.5)=F(x+1-0.5)\n\\]\nThis is called continuity correction and is applicable any time we want to approximate an integer-valued random variable with a continuous one. If the random variable is discrete but not an integer-valued, we want to meet half the distance between the two values.\nThis applies, for example, to the means of discrete variables (and relative frequency as well), where we can derive continuity correction this way:\n\\[\\frac{\\sum_{i=1}^n X_i \\pm1/2}{n}=\\overline{X} \\pm\\frac{1}{2n}\\]\n\n\n\nLindenberg-Lévy central limit theorem\nSimilarly, as Knichin´s law is a generalisation of Bernoulli´s law, Lindenberg-Lévy’s theorem generalises the de Moivre-Laplace theorem.\nLet \\(X_1,X_2,...,X_n\\) be a sequence of independent identically distributed random variables with any distribution with finite variance, \\(X_i\\sim ?(E(X),Var(X))\\).\nLet a sequence of summations be calculated from these random variables, that is, \\(M_n=\\sum_{i=1}^{n}(X_i)\\). This sequence convergences in distribution to the normal distribution:\n\\[\nM_n \\xrightarrow{n\\to\\infty;D}N\\left(nE(X),\\sqrt{nVar(X)}\\right)\n\\]\nAnd by linear transformation, the sequence of arithmetic means also converges to the normal distribution:\n\\[\\overline{X_n}=\\frac{\\sum_{i=1}^n X_i}{n} \\xrightarrow{n\\to\\infty;D}N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)\\]One of the results is that for n large enough, we might use a normal distribution to calculate probabilities of sample means. Again, the approximation notation is commonly used:\n\\[ M_n \\approx N\\left(nE(X),\\sqrt{nVar(X)}\\right) \\]\n\\[\\overline{X_n}\\approx N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)\\]"
  },
  {
    "objectID": "LN9.html",
    "href": "LN9.html",
    "title": "Sampling",
    "section": "",
    "text": "To open the inferential statistics part of our course, we have to start with the motivation behind it and with the (simplified) sampling theory."
  },
  {
    "objectID": "LN9.html#introduction",
    "href": "LN9.html#introduction",
    "title": "Sampling",
    "section": "",
    "text": "To open the inferential statistics part of our course, we have to start with the motivation behind it and with the (simplified) sampling theory."
  },
  {
    "objectID": "LN9.html#why-inference",
    "href": "LN9.html#why-inference",
    "title": "Sampling",
    "section": "Why inference?",
    "text": "Why inference?\nTo infer means to make a generalising statement. Statistical teaching usually starts with descriptive or exploratory statistics. In this branch, students are taught how to describe the data at hand - how to visualise and present them, and what measures to use to provide typical value or typical spread in the data. Many terms in descriptive statistics are identical to those used in probability theory, e.g., variance or median, which might build a little confusion.\nHere is the deal: we worked with theoretical concepts in probability theory and calculated consequences of what-if scenarios, while in descriptive statistics, you calculate specific values from the data.\nThe issue is that you can´t go any further when describing the data at hand - no generalisation is possible without additional assumptions. But the human brain is a big machine that makes predictions and generalisations constantly; we can´t help it; it is our survival edge.\nInferential statistics is an attempt to satisfy our brain´s hunger for generalisations from the data to the process that generated them using some coherent and logical framework.\nAs an example, because we know that under quite realistic assumptions, a sample mean converges to the expected value for increasing sample size, we might use the mean of the data to guess, to estimate, the expected value of the generating process and to guess uncertainty around this estimate."
  },
  {
    "objectID": "LN9.html#inferential-division",
    "href": "LN9.html#inferential-division",
    "title": "Sampling",
    "section": "Inferential division",
    "text": "Inferential division\nThere are two main inferential schools of thought, those coherent and logical frameworks - frequentist and Bayesian.\n\nBayesian tribe\nFor Bayesians, the probability is a “degree of belief” - the underlying distribution (process) is uncertain, but this uncertainty is formalised by specifying prior probabilities.\nIn the second step, Bayesians are looking forward - if this prior scenario (distribution, parametric value,..) is correct, then the likelihood of observing the data we have at hand is calculated.\nIn the third step, Bayesians update their priors using the Bayes theorem to arrive at posterior probabilities (that data comes from this distribution and not some alternative; that parameters have these values and not some alternative..).\nThe Bayesian approach has a great advantage - one can make a direct probabilistic claim about the generating process, which is an intuitive way of communicating uncertainty. On the other hand, they do not work with long-term error probabilities, which are the basis for formal hypothesis testing.\n\n\nFrequentist tribe\nThe frequentist approach, which we will work on within this course, is different. Frequentists assume that the underlying process is certain but unknown. We look forward similarly - if the scenario is correct, the likelihood of the data at hand is calculated. But there are no priors and no updates.\nBayesians treat data as given; frequentists treat data as random. For frequentists, data are uncertain, not a generating process, and uncertainty of the data can be calculated, but not of the generating process.\nIn this way of thinking, we cannot make any direct probabilistic claim about the generating process, which complicates understanding the frequentist terms like p-value or confidence interval.\nFrequentist probabilistic claims are about the data, and procedures are about controlling long-term error probabilities.\n\n\nUnification?\nThere have been historically many unification attempts. Even the great, almost mythological, father of frequentist hypothesis testing, R. A. Fischer, attempted to make a “Bayesian omelette without breaking frequentist eggs”, that is to use frequentist procedures controlling long-term error probabilities with the result that would give Bayesian, direct probability, claim. Sure, he tried to hide this from the world (because what a shame) by calling it fiduciarity.\nFrom the other tribe, no one lesser than Harold Jeffrey tried to propagate the common use of so-called flat prior probabilities, assigning the same weight to any possibility; this leads, for many probability distributions, to mathematically identical results to the frequentist procedures and thus getting “frequentist omelette without breaking Bayesians eggs”.\nHere are my closing personal two cents: The greatest strength of the Bayesian approach is the possibility of directly involving information on possibilities in the estimates.\nSuppose I believe strongly that height and weight are positively correlated. In that case, I will benefit from putting these beliefs in the prior distribution because my posteriors will be much narrower and estimates more precise.\nTrying to unite the two approaches in such cases does not make sense because I would pretend that I do not know the results in the realms of possibilities.\nOn the other hand, there are many instances in which I have no such knowledge, and I do not want to get involved in the results in this way. I might want to follow the procedure with a given and known error rate; that would lead me to choose frequentist hypothesis testing."
  },
  {
    "objectID": "LN9.html#sampling",
    "href": "LN9.html#sampling",
    "title": "Sampling",
    "section": "Sampling",
    "text": "Sampling\nThe process of obtaining data is, in inferential statistics, called sampling. We might imagine sampling from a given finite population as the simplest case of sampling or sampling from a given probability distribution or infinite population (generating process) as another case.\nPopulation is an interesting term. In its simple use, it means all the units we can sample from. For example, we can sample some countries to observe the relationship between inflation and unemployment rates. In such a case, we could say, in this simplified manner, that our population are all the countries in the world.\nBut what if we had data from all the countries in the world? Would it be enough to describe what we observe in this population, and would all the uncertainty about the relationship between inflation and unemployment rates disappear?\nHardly so. To counter this, there is a concept called superpopulation - the infinite population of all the possible countries that could have existed, from which we observe only our finite population.\nSo, even with all the countries in the world assuming some general relation between unemployment and inflation rates, we still need to quantify the uncertainty arising from the sampling process regarding this relationship.\n\nRandom sampling\nRandom sampling is a process of sampling in which each unit in the population has an equal probability of being sampled. In this case, sampling with or without replacement might change the probability of being in the sample (or even being there multiple times) and some variance measures, as we have seen in the cases of binomial and hypergeometric distributions.\nThere are many possibilities for creating random samples, ranging from simple sampling to systematic, stratified, or cluster sampling procedures.\nStill, as long as the probabilities are equal for all the units, it does not generally matter.\nAnother case is a sampling from an infinite population (or superpopulation) or, even more generally, a data-generating process. For this type of sampling to be considered random, we assume it remains consistent (the same) for each sampling step.\nRandom sampling is very important because it simplifies calculations and fulfils the conditions of independence and identical distribution, which are important background assumptions of stochastical convergences.\n\n\nNon-random sampling\nThe definition of non-random sampling is straightforward negation of random sampling. It is such a sampling in which there is at least one unit in the population with an unequal probability of being sampled. The process is inconsistent at least at one step for an infinite population (superpopulation or data-generating process).\nThe world is full of non-random samples violating thus important assumptions behind stochastic convergences, which often leads to systematic differences between estimates and true values of characteristics - known as sampling bias.\nExamples of non-random sampling are convenience, purposive, snowball or quota sampling.\nThis bias might be unrepairable, but it might be alleviated, e.g. by specifying the target population differently. For example, suppose you sample students by posting an online questionnaire distributed via social networks. In that case, you can hardly assume that all the students of your target institution have an equal chance of answering this. But you might try to claim that your (super)population consists of students on social networks willing to answer questionnaires.\nAlternatively, you might assume that the non-random nature of sampling does not influence the characteristic of your interest, e.g. that for the relation between height and weight, your social questionnaire is a random sample because this relation is the same in the population of all the students and population of students on social networks willing to answer questionnaires.\nThis is still an assumption; you should be honest, at least with yourselves. You might try to test it in a different setting.\n\n\nSampling from a Bernoulli distribution\nIn this type of sampling, we assume our population consists of N units, of which m units have some property coded as 1, and the rest do not possess this property, coded as 0. This means that the probability of seeing the property in one step of the random sampling process is \\(\\pi=\\frac{m}{N}\\). Let´s also denote the sample size as n.\nNow imagine that you randomly sample these n units (or assume they are generated from the data-generating process). Then, you can imagine another random sample, another, another, and so on. These samples, usually called data, are thus random.\nYou can calculate the characteristics of each sample. For sampling from the Bernoulli distribution, the two most important are sample total \\(m=\\sum_{i=1}^{n}x_i\\) and relative frequency \\(p=\\frac{m}{n}\\). From probability theory, you should be able to recall that M follows binomial (sampling with replacement) or hypergeometric (without replacement) distribution.\nCharacteristics of the sample are called statistics. Statistics are functions of values observed in the sample and nothing else.\n\nDetour\nIf data are random, then statistics also have to be random. They are random variables \\(M,P\\) with probability distributions dependent on the parameters of the data-generating process. This makes statistics basic of the inference - they can always be calculated in the observed data and are connected to the (assumed) underlying probability distribution.\nDistributions of statistics are called sampling distributions and will be specifically introduced later. But recall the idea: generating process leads to random data with random statistics.\n\n\n\nSampling from a (continuous) distribution\nWhen sampling from binomial distribution, we have characteristics dependent on the parameter \\(\\pi\\). However, when generalising this idea, we must introduce a random variable with the expected value E(X) and the variance Var(X). (and we might wander further talking about other characteristics).\nSo let´s have a population of size N, with mean \\(\\bar{x}\\) and variance \\(s_x^2\\), or an infinite population, superpopulation, or data-generating process with the expected value E(X) and the variance Var(X). These are identical for our purposes. Let´s also have a sample of size n.\nOur most important sample characteristics - statistics, are sample total m, sample mean \\(\\bar{x}\\) and sample variance \\(s_x^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\). Again, using the same logic introduced in the Detour, these are random variables; hence \\(M,\\bar{X},S_x^2\\), their values can be calculated for any data, and their distributions depend on the population distribution (or data-generating process properties)."
  },
  {
    "objectID": "LN9.html#characteristics-of-statistics",
    "href": "LN9.html#characteristics-of-statistics",
    "title": "Sampling",
    "section": "Characteristics of statistics",
    "text": "Characteristics of statistics\nSince statistics are random variables, they have to have their characteristics.\n\\[\nE(M)=n*E(X);Var(M)=n*Var(X)\n\\]\n\\[\nE(\\bar{X})=E(X);Var(\\bar{X})=\\frac{Var(X)}{n}\n\\]\n\\[\nE(S^2)=Var(X);Var(S^2)\\xrightarrow{n\\to\\infty}0\n\\]\nIf we assume sampling from a finite population without replacement, total and mean variances must be multiplied by a finite correction factor \\(\\frac{N-n}{N-1}\\), known from hypergeometric distribution."
  },
  {
    "objectID": "LN9.html#sampling-distributions",
    "href": "LN9.html#sampling-distributions",
    "title": "Sampling",
    "section": "Sampling distributions",
    "text": "Sampling distributions\nSampling distributions are distributions of statistics.\nIt might be unclear if you had descriptive statistics because their statistics, like sample mean or sample variance, were just numbers and constants describing some property of observed data.\nHowever, in inferential (frequentist) statistics, we think about statistics as random variables - and the value observed is only one of many possible values that could have arisen during the sampling or data-generating process.\nWhat are their distributions? What are sampling distributions? It depends on the distribution of the population or the data-generating process.\n\nSampling distributions from Bernoulli\nIf \\(X_i\\sim Be(\\pi)\\to E(X_i)=\\pi;Var(X_i)=\\pi*(1-\\pi)\\), then\n\\[\nM=\\sum_{i=1}^{n} X_i\\sim Bi(n,\\pi)\n\\]\n\\[\nE(M)=n*\\pi;Var(M)=n*\\pi*(1-\\pi)\n\\]\n\\[\nE(P)=\\pi;Var(P)=\\frac{\\pi*(1-\\pi)}{n}\n\\]\nWe know from probability theory that the sample total will follow a binomial distribution, assuming sampling is with replacement. Assuming sampling is without replacement, it would follow hypergeometric distribution, but we will omit this case here. It might be, however, useful to hold this thought for small samples.\nBy the Moivre-Laplace central limit theorem, we might (and later will) use binomial’s convergence to normal distribution.\n\\[\nM=\\sum_{i=1}^{n}X_i\\approx N\\left(\\mu=n*\\pi;\\sigma=\\sqrt{(n*\\pi*(1-\\pi)}\\right)\n\\]\n\\[\nP=\\frac{\\sum_{i=1}^{n}X_i}{n}\\approx N\\left(\\mu=\\pi;\\sigma=\\sqrt{\\frac{(\\pi*(1-\\pi)}{n}}\\right)\n\\]\n\n\nSampling distributions from normal\nIf \\(X_i\\sim N(\\mu;\\sigma)\\), then\n\\[\nM\\sim N\\left(n*\\mu;\\sqrt{n}*\\sigma\\right)\n\\]\n\\[ \\bar{X}\\sim N\\left(\\mu;\\frac{\\sigma}{\\sqrt{n}}\\right) \\]\n\\[\nS^2\\to\\frac{n-1}{\\sigma^2}S^2\\sim \\chi^2(n-1)\n\\]\n\\[\nE\\left(S^2\\right)=\\sigma^2;Var\\left(S^2\\right)=\\frac{2}{n-1}\\sigma^4\n\\]\nYou can notice that not only do we have an exact distribution of sample total and sample mean, but we also have a distribution of the specific transformation of sample variance. We have exact characteristics of sample variance.\nAll this is derived from the assumption of normal distribution of the underlying (superpopulation, data-generating process) random variable.\nLet´s now focus a little on the transformation of sample variance. You might notice that while sample variance is a statistic because its value can be calculated from the sample only, the provided transformation cannot be considered statistics because it can be calculated only with the knowledge of \\(\\sigma\\).\nThis type of random variable, dependent on the sample values and the parameter value, is called a pivot. The second important property of pivots is that their distribution is independent of the underlying distribution (spare the normality assumption). Pivots are historically extremely important because they connect parameters with the sample so that we can calculate probabilities in a general manner (using tables). This connection is the basis for standard hypothesis testing.\n\nDetour on \\(\\chi^2\\) distribution\nThe pivot introduced in the previous paragraph follows \\(\\chi^2\\) distribution assuming \\(X_i\\sim N(\\mu;\\sigma)\\). How does the \\(\\chi^2\\) distribution arise in general?\nLet´s have a random variable with standard normal distribution: \\(Z_i\\sim N(0;1)\\), and let´s think of the sum of its squared values; then this sum follows a chi-squared distribution with \\(k-1\\) degrees of freedom:\n\\[\n\\sum_{i=1}^{k}Z_i \\sim \\chi^2(k-1).\n\\]\nHow do you get to this formula from the presented pivot?\n\\[\n\\frac{n-1}{\\sigma^2}S^2=\\frac{n-1}{\\sigma^2}*\\frac{\\sum_{i=1}^n \\left( X_i-\\bar{X} \\right) ^2}{n-1}=\n\\sum_{i=1}^{n}\\frac{\\left(X_i-\\bar{X}\\right)^2}{\\sigma^2}=\n\\sum_{i=1}^{n} \\left( \\frac{X_i-\\bar{X}}{\\sigma}  \\right)^2\n\\]\nFrom the assumption that \\(X_i\\sim N(\\mu;\\sigma)\\) follows the sampling distribution of \\(\\bar{X} \\sim N\\left(\\mu;\\frac{\\sigma}{\\sqrt{n}} \\right)\\), by subtracting the two, we have centred normal distribution and by dividing them by \\(\\sigma\\) we have a standard normal distribution, hence \\(\\sum_{i=1}^{n} Z_i^2 \\sim \\chi^2(n-1)\\).\nThe parameter of \\(\\chi^2\\) distribution is called degrees of freedom, usually denoted as df or k. Why such a name? If you go back to the sample distribution, you have n degrees of freedom to begin with - you can manipulate those n values in the sample any way you want. However, since you use sample mean \\(\\bar{X}\\), one of those values has to be dedicated, which means you can choose \\(n-1\\) values any way you want, but the last one has to be such that you observe a specific value of the sample mean \\(\\bar{x}\\) .\n\n\n\nFollowing the distribution of the sample mean\nAs already stated, If \\(X_i\\sim N(\\mu;\\sigma)\\), then\n\\[ \\bar{X}\\sim N\\left(\\mu;\\frac{\\sigma}{\\sqrt{n}}\\right) \\]\nWe can introduce a new pivot or two; the first one is the standardised value of the sample mean, coming from the fact that under normality assumption \\(\\bar{X}\\sim N \\left(\\mu;\\frac{\\sigma}{\\sqrt{n}} \\right)\\):\n\\[\nZ=\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{\\sigma^2}{n}}}=\n\\frac{\\bar{X}-\\mu}{\\sigma}*\\sqrt{n} \\sim N(0;1)\n\\]\nWhile this pivot is theoretically sound and often serves as a basis for introducing basic concepts in inferential statistics, it is not practically useful - say you want to test the hypothesis about expected value \\(\\mu\\) using some observed sample values - to be able to calculate this pivot, you would need to know the value of \\(\\sigma\\), which is even less likely than knowing directly value of \\(\\mu\\) (in which case you would not have to test a hypothesis at the first place).\nThe practical solution to this problem is to use a sample estimate of \\(\\sigma\\), the sample standard deviation S. This is called a plug-in estimator - you are not interested in what value 𝜎 has but need to guess it to estimate \\(\\mu\\) correctly. The pivot is as follows:\n\\[\nT=\\frac{\\bar{X}-\\mu}{\\sqrt{\\frac{S^2}{n}}}=\n\\frac{\\bar{X}-\\mu}{S}*\\sqrt{n} \\sim t(n-1)\n\\]\nThis pivot now does not follow a standard normal distribution but a Student´s distribution with \\(n-1\\) degrees of freedom.\n\nDetour on Student´s t distribution.\nStudent´s t distribution is well known for its discovery by Student, a pseudonym for Guinness Brewery statistician William Gossett. At times before the Great War, Guinness was expanding its exports and needed to make sure that wherever you order your pint of their now-famous ale, it would have the same quality as if ordered in Ireland.\nHis task was to control this quality and devise a solution to the so-called small-sample problem. When testing the quality of the new batch, he wanted to use only a small sample of it. However, he knew using a standard normal distribution for this task was inappropriate, so he had to devise an analytical solution.\nThis solution was published under the pseudonym Student, as the brewery´s policy did not allow him to publish under his name.\nLet´s assume that \\(Z\\sim N(0;1)\\) and \\(G \\sim \\chi^2(k)\\) are independent. Then\n\\[\nT=\\frac{Z}{\\sqrt{\\frac{G}{k}}} \\sim t(k)\n\\]\nHow do we arrive at the pivot above from this theoretical solution? We use already known pivots \\(Z=\\frac{\\bar{X}-\\mu}{\\sigma}*\\sqrt{n}\\sim N(0;1)\\) and \\(G=\\frac{n-1}{\\sigma^2}*S^2 \\sim \\chi^2(n-1)\\).\n\\[ T=\\frac{\\frac{\\left(\\bar{X}-\\mu\\right)}{\\sigma}*\\sqrt{n}}{\\sqrt{\\frac{\\frac{(n-1)}{\\sigma^2}*S^2}{n-1}}}\n= \\frac{\\bar{X}-\\mu}{S}*\\sqrt{n}\n\\sim t(n-1) \\]\nStudent´s t distribution is thus the exact distribution of the pivot assuming the normality of the originating random variable. It is a distribution quite similar to the standard normal - is symmetric and centred around 0, but has so-called heavier tails than the normal - meaning that it generates extreme values with higher probabilities.\nThe lower the number of degrees of freedom, the heavier the tails are, reflecting large uncertainties arising in the variation of the mean in the small samples when using a plug-in estimator of \\(\\sigma\\).\nFor increasing sample sizes, followed by increasing degrees of freedom, the Student´s t distribution converges in distribution to standard normal distribution:\n\\[\nt(k) \\xrightarrow{D;k \\to \\infty}N(0;1)\n\\]\nThis means that for the sample sizes large enough, Student´s t distribution can be substituted by standard normal distribution with a little loss of precision. Assuming normal distribution in the generating process, as we are, the large enough can be translated into a few dozen.\nHistorically, for practical purposes, Student´s t distribution was used for the sample sizes up to 31 because tables usually contained 30 rows of quantiles of this distribution (it is not analytically solvable). For higher sample sizes, standard normal distribution was used.\nNowadays, with the widespread use of computers, there is no reason NOT to use Student´s t distribution."
  },
  {
    "objectID": "LN9.html#sampling-distribution-of-the-sample-mean---general-case",
    "href": "LN9.html#sampling-distribution-of-the-sample-mean---general-case",
    "title": "Sampling",
    "section": "Sampling distribution of the sample mean - general case",
    "text": "Sampling distribution of the sample mean - general case\nSo far, we have followed the assumption of the normality of the data-generating process. But what if the underlying distribution is not normal?\nLuckily, there is a go-around - the famous central limit theorem, specifically Lindenber-Lévy. Just recall that this theorem roughly states that the total and the mean of independent identically distributed random variables converge in distribution to a normal distribution, assuming finite variance.\n\\[\nX \\sim ? E(X);Var(X) \\to \\bar{X} \\xrightarrow{D;n\\to\\infty}N\\left(E(X);\\sqrt {\\frac{Var(X)}{n}} \\right)\n\\]\nSo, for sample sizes large enough, we might assume that the sampling distribution of the sample mean is approximately normal:\n\\[\\bar{X} \\approx N\\left(E(X), \\sqrt{\\frac{Var(X}{n}}  \\right)\\]\nHence, the standardised mean is approximately the standard normal.\n\\[\nZ=\\frac{\\bar{X}-E(X)}{S.D.(X)}*\\sqrt{n} \\approx N(0;1)\n\\]\nUsing a plug-in estimate, substituting population variance with sample variance, the standardised mean follows approximately Student´s t distribution.\n\\[\nT=\\frac{\\bar{X}-E(X)}{S}*\\sqrt{n} \\approx t(n-1)\n\\]\nThe important notion is that the sample size should be large enough. There is no clear boundary - the further away the data-generating process is from normality, especially regarding skewness and kurtosis, the more observations are needed. Sometimes, a few dozen are fine; sometimes, thousands are not enough.\nFor the historical reasons already stated, the boundary of 30 observations is ingrained in the collective memory even though there is no theoretical reason for it to be used.\nThe last note: because we assume the sample size is large enough (for CLT to kick in), we can use standard normal distribution instead of Student´s t.\nThat is why one of the parts of the initial data analysis before hypothesis testing is often normality testing. However, using formal tests is not an appropriate procedure for the reasons connected to the concept of power in hypothesis testing; rather, a graphic comparison of the observed distribution with the normal distribution is much more suitable. Decisions should be made subjectively on whether deviations are large enough given the sample size. These things are complicated, and beware of simplicist procedures; it is what it is."
  },
  {
    "objectID": "LN10.html",
    "href": "LN10.html",
    "title": "Point estimates",
    "section": "",
    "text": "We will introduce the first basic concepts in point estimates in frequentist statistics. To remind you, frequentist statistics is about long-term properties; here, we introduce the long-term properties of the statistics we use to estimate parameters and the most common methods to construct these statistics.\nThe long-term in this setting means under repeated sampling. That is, if you repeat the sample of a given size over and over and over, and you write down values of the statistics across this theoretically infinite batch of samples, what are the properties of these statistics? Recall the previous chapter by introducing the characteristics of statistics and their sampling distributions.\nFirst, we need to distinguish three terms:\nThe estimator T is a statistic that is used to estimate something. This is a random variable, and the properties of this random variable are ofinterest.\nThe estimand is what we want to estimate, e.g., the parameter \\(\\theta\\) of a chosen probability distribution. This is a constant with no probabilistic properties in our frequentist playbook.\nThe estimateis a realisation; it is the estimator’s specific value, t, calculated in the data."
  },
  {
    "objectID": "LN10.html#introduction",
    "href": "LN10.html#introduction",
    "title": "Point estimates",
    "section": "",
    "text": "We will introduce the first basic concepts in point estimates in frequentist statistics. To remind you, frequentist statistics is about long-term properties; here, we introduce the long-term properties of the statistics we use to estimate parameters and the most common methods to construct these statistics.\nThe long-term in this setting means under repeated sampling. That is, if you repeat the sample of a given size over and over and over, and you write down values of the statistics across this theoretically infinite batch of samples, what are the properties of these statistics? Recall the previous chapter by introducing the characteristics of statistics and their sampling distributions.\nFirst, we need to distinguish three terms:\nThe estimator T is a statistic that is used to estimate something. This is a random variable, and the properties of this random variable are ofinterest.\nThe estimand is what we want to estimate, e.g., the parameter \\(\\theta\\) of a chosen probability distribution. This is a constant with no probabilistic properties in our frequentist playbook.\nThe estimateis a realisation; it is the estimator’s specific value, t, calculated in the data."
  },
  {
    "objectID": "LN10.html#basic-properties-of-point-estimators",
    "href": "LN10.html#basic-properties-of-point-estimators",
    "title": "Point estimates",
    "section": "Basic properties of point estimators",
    "text": "Basic properties of point estimators\nWe instinctively want our estimators to be connected to the estimands somehow. We would like them to have some properties, and we should be aware that there is sometimes even a trade-off between these properties. Intuitively, the estimator should be, on average, as close to the estimand as possible. This closeness can be divided into two properties - how far away it is on the long-term average and how far away it is in its individual realisations (estimates).\n\nBias\nThe first intuitive property can be written down this way:\n\\[\nE(T)=\\theta\n\\]\nThis means that the expected value of our estimator should be equal to the parameter we are about to estimate with it. The property is called unbiasedness and is often not achieved. But a softer version, called asymptotic unbiasedness, does exist:\n\\[\nE(\\textbf{T})\\xrightarrow{n\\to\\infty}\\theta\n\\]\nWe can calculate how far away the expectation is from the reality in the quantification of bias:\n\\[\nb(T)=E(T)-\\theta\n\\]\nAnd rewrite both properties as\n\\[\nb(T)=0\n\\]\n\\[\nb(T)\\xrightarrow{n\\to\\infty}0\n\\]\n\n\nConsistency\nBias represents systematic error. The second type of error is random, which we mathematically represent as the variance of the estimator \\(Var(T)\\).\nNow, the consistency property connects the two properties: systematic error, which is zero (unbiasedness) or going down (asymptotic unbiasedness), and random errors, which go down with the increasing number of observations. So, the two necessary and sufficient conditions of consistency are:\n\\[\nb(T)\\xrightarrow{n\\to\\infty}0\n\\]\n\\[ Var(T)\\xrightarrow{n\\to\\infty}0 \\]\nConsistency means that the more data we observe, the better our estimator is on the long-term average.\n\n\nAccuracy\nHow well is our estimator doing on a long-term average? Since there are two sources of the error, we can combine them to create an overall error evaluation called mean squared error:\n\\[\nMSE(T)=E \\left(\\left(T-\\theta\\right)^2\\right)=\\left(b\\left(T\\right)\\right)^2+Var(T)\n\\]\nThe mean squared error tells us what the expected squared error of the estimator is regarding the estimated parameter \\(\\theta\\). It is composed of the squared value of the bias and variance of the estimator. If the estimator is unbiased, then the mean squared error is the estimator’s variance only.\nBecause MSE is, similarly to variance, in squared units of the random variable, it is often considered more useful to communicate its square root called root mean squared error, which is in the units of the random variable:\n\\[\nRMSE(T)=\\sqrt{MSE(T)}\n\\]"
  },
  {
    "objectID": "LN10.html#basic-methods-of-construction",
    "href": "LN10.html#basic-methods-of-construction",
    "title": "Point estimates",
    "section": "Basic methods of construction",
    "text": "Basic methods of construction\nEstimators can be constructed in many ways, ranging from the look outside-of-the-window method to more sophisticated techniques. Generally, the common basic approaches can be divided into two groups: one that matches some theoretical properties of the distribution to similar properties of the data, and the second optimises the selected criterion of fit. From the first group, we will glance at moment-matching and quantile-matching methods; from the second, we will focus on the most commonly applied method - maximum likelihood estimation.\n\nMethod of Moments\nThe method of moments is based on matching theoretical moments of a given probability distribution with empirical moments calculated in the sample. Take, for example, raw moments:\n\\[\n\\mu_k´=\\overline{x_k}\n\\]\nThis method can be adapted to use various types of moments (central, standardised, L-moments, TL-moments,…). Still, in its simple version, you create as many equations as the number of parameters you need to estimate and start from the bottom \\(k=1,2,...\\).\nThis method was used for its simplicity and usually yields consistent estimators. On the other hand, the estimators are often worse in terms of accuracy than other options and have some other disadvantages.\nIt also serves as a teaching introduction to the generalised method of moments applicable under certain conditions or as a first-order solution before proceeding to the maximum likelihood estimate.\n\n\nMethod of Quantiles\nAnother example of the matching method is the method of quantiles, but instead of matching moments, it matches theoretical quantiles \\(x_p\\) with empirical quantiles \\(\\widetilde{x_p}\\) for various values of P.\nThis method is specifically useful when you want to estimate probability distribution with specific theoretical quantiles of interest, like, e.g. 95% quantile of the log-normal distribution for risk estimate.\n\n\nMaximum likelihood method\nThe last method of finding estimators in this short list is probably the most common and powerful. Let´s first introduce the concept of likelihood and distinguish it from the joint probability distribution of the sample.\nWe start with the assumption that our sample comes from a random sampling procedure (or unchanging data-generating process). In such a case, individual values are drawn from a sequence of independent identically distributed random variables, and their joint probability distribution can be written as the product of individual densities which are identical:\n\\[f(\\textbf{x},\\theta)=\\prod_{i=1}^n f(x_i,\\theta)\\]This is a probability function of fixed parameter \\(\\theta\\) and varying data. This answers the question: What are the probabilities of various datasets if the given parameter has this value?\nHowever, it can´t be presented as varying once the dataset is known and fixed. Still, we can switch the idea and ask a different question: if the given dataset looks like this, what are its probabilities under different possible values of the parameter \\(\\theta\\)? That is what the likelihood function presents:\n\\[\nL(x;\\theta)=\\prod_{i=1}^{n}f(x_i,\\theta)\n\\]\nTo repeat: probability density is the probability of varying data under a fixed value of the parameter; likelihood is the probability of fixed data under a varying value of the parameter.\nMaximum likelihood estimators arise from maximising likelihood function; that is, they answer the question: under what value of parameter \\(\\theta\\) arise data at hand with the highest probability?\nUnder some general conditions, the likelihood function is unimodal and its maximum can be found using partial derivations:\n\\[\n\\frac{\\partial L(\\textbf{x},\\theta)}{\\partial \\theta}=0\n\\]\nThe problem with this is two-fold: analytically, it is quite difficult to find partial derivations of the product. Numerically, this product usually goes too quickly to zero, and the procedure cannot distinguish its value and collapses.\nPractically, both of these problems can be solved by using the log-likelihood function, which transforms the product into a sum:\n\\[\nl(\\textbf{x};\\theta)=log\\left(L(\\textbf{x};\\theta)  \\right)=\\sum_{i=1}^{n}log(f(x_i;\\theta))\n\\]\nBecause this transformation is monotonic, we can find the maximum at the same point by partial derivation:\n\\[\n\\frac{\\partial l(\\textbf{x};\\theta)}{\\partial \\theta}=0\n\\]\nMaximum likelihood estimators are often well-behaved with the following properties:\n\n(Asymptotically) unbiased\nConsistent\nAsymptotically best linear unbiased estimators (lowest variance among unbiased estimators)\nAsymptotically having normal distribution.\n\nThe last point is vital for inferences about estimands. One can see that these properties are often asymptotical, i.e. they should be considered with caution. But generally speaking, maximum likelihood estimators are usually the optimal way to go for medium to large samples and typically have good properties even for small sample sizes."
  }
]