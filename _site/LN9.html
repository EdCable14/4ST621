<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adam Cabla">

<title>4ST621 - Sampling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">4ST621</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./Course_Intro.html" rel="" target="" aria-current="page">
 <span class="menu-text">Course Introduction</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./LN9.html">Mathematical Statistics</a></li><li class="breadcrumb-item"><a href="./LN9.html">Sampling</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Course_Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Probability Theory</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Basics of Probability</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random Variable</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Characteristics of Random Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random Vectors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Selected Discrete Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Selected Continuous Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Two Selected Multivariate Distributions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic convergences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Mathematical Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN9.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Sampling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LN10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Point estimates</span></a>
  </div>
</li>
          <li class="sidebar-item">
 <span class="menu-text">LN11.qmd</span>
  </li>
          <li class="sidebar-item">
 <span class="menu-text">LN12.qmd</span>
  </li>
          <li class="sidebar-item">
 <span class="menu-text">LN13.qmd</span>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#why-inference" id="toc-why-inference" class="nav-link" data-scroll-target="#why-inference">Why inference?</a></li>
  <li><a href="#inferential-division" id="toc-inferential-division" class="nav-link" data-scroll-target="#inferential-division">Inferential division</a>
  <ul class="collapse">
  <li><a href="#bayesian-tribe" id="toc-bayesian-tribe" class="nav-link" data-scroll-target="#bayesian-tribe">Bayesian tribe</a></li>
  <li><a href="#frequentist-tribe" id="toc-frequentist-tribe" class="nav-link" data-scroll-target="#frequentist-tribe">Frequentist tribe</a></li>
  <li><a href="#unification" id="toc-unification" class="nav-link" data-scroll-target="#unification">Unification?</a></li>
  </ul></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a>
  <ul class="collapse">
  <li><a href="#random-sampling" id="toc-random-sampling" class="nav-link" data-scroll-target="#random-sampling">Random sampling</a></li>
  <li><a href="#non-random-sampling" id="toc-non-random-sampling" class="nav-link" data-scroll-target="#non-random-sampling">Non-random sampling</a></li>
  <li><a href="#sampling-from-a-bernoulli-distribution" id="toc-sampling-from-a-bernoulli-distribution" class="nav-link" data-scroll-target="#sampling-from-a-bernoulli-distribution">Sampling from a Bernoulli distribution</a></li>
  <li><a href="#sampling-from-a-continuous-distribution" id="toc-sampling-from-a-continuous-distribution" class="nav-link" data-scroll-target="#sampling-from-a-continuous-distribution">Sampling from a (continuous) distribution</a></li>
  </ul></li>
  <li><a href="#characteristics-of-statistics" id="toc-characteristics-of-statistics" class="nav-link" data-scroll-target="#characteristics-of-statistics">Characteristics of statistics</a></li>
  <li><a href="#sampling-distributions" id="toc-sampling-distributions" class="nav-link" data-scroll-target="#sampling-distributions">Sampling distributions</a>
  <ul class="collapse">
  <li><a href="#sampling-distributions-from-bernoulli" id="toc-sampling-distributions-from-bernoulli" class="nav-link" data-scroll-target="#sampling-distributions-from-bernoulli">Sampling distributions from Bernoulli</a></li>
  <li><a href="#sampling-distributions-from-normal" id="toc-sampling-distributions-from-normal" class="nav-link" data-scroll-target="#sampling-distributions-from-normal">Sampling distributions from normal</a></li>
  <li><a href="#following-the-distribution-of-the-sample-mean" id="toc-following-the-distribution-of-the-sample-mean" class="nav-link" data-scroll-target="#following-the-distribution-of-the-sample-mean">Following the distribution of the sample mean</a></li>
  </ul></li>
  <li><a href="#sampling-distribution-of-the-sample-mean---general-case" id="toc-sampling-distribution-of-the-sample-mean---general-case" class="nav-link" data-scroll-target="#sampling-distribution-of-the-sample-mean---general-case">Sampling distribution of the sample mean - general case</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Sampling</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Adam Cabla </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>To open the inferential statistics part of our course, we have to start with the motivation behind it and with the (simplified) sampling theory.</p>
</section>
<section id="why-inference" class="level2">
<h2 class="anchored" data-anchor-id="why-inference">Why inference?</h2>
<p>To infer means to make a generalising statement. Statistical teaching usually starts with descriptive or exploratory statistics. In this branch, students are taught how to describe the data at hand - how to visualise and present them, and what measures to use to provide typical value or typical spread in the data. Many terms in descriptive statistics are identical to those used in probability theory, e.g., variance or median, which might build a little confusion.</p>
<p>Here is the deal: we worked with theoretical concepts in probability theory and calculated consequences of what-if scenarios, while in descriptive statistics, you calculate specific values from the data.</p>
<p>The issue is that you can¬¥t go any further when describing the data at hand - no generalisation is possible without additional assumptions. But the human brain is a big machine that makes predictions and generalisations constantly; we can¬¥t help it; it is our survival edge.</p>
<p>Inferential statistics is an attempt to satisfy our brain¬¥s hunger for generalisations from the data to the process that generated them using some coherent and logical framework.</p>
<p>As an example, because we know that under quite realistic assumptions, a sample mean converges to the expected value for increasing sample size, we might use the mean of the data to guess, to estimate, the expected value of the generating process and to guess uncertainty around this estimate.</p>
</section>
<section id="inferential-division" class="level2">
<h2 class="anchored" data-anchor-id="inferential-division">Inferential division</h2>
<p>There are two main inferential schools of thought, those coherent and logical frameworks - frequentist and Bayesian.</p>
<section id="bayesian-tribe" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-tribe">Bayesian tribe</h3>
<p>For Bayesians, the probability is a ‚Äúdegree of belief‚Äù - the underlying distribution (process) is uncertain, but this uncertainty is formalised by specifying prior probabilities.</p>
<p>In the second step, Bayesians are <em>looking forward -</em> if this prior scenario (distribution, parametric value,..) is correct, then the likelihood of observing the data we have at hand is calculated.</p>
<p>In the third step, Bayesians update their priors using the Bayes theorem to arrive at posterior probabilities (that data comes from this distribution and not some alternative; that parameters have these values and not some alternative..).</p>
<p>The Bayesian approach has a great advantage - one can make a direct probabilistic claim about the generating process, which is an intuitive way of communicating uncertainty. On the other hand, they do not work with long-term error probabilities, which are the basis for formal hypothesis testing.</p>
</section>
<section id="frequentist-tribe" class="level3">
<h3 class="anchored" data-anchor-id="frequentist-tribe">Frequentist tribe</h3>
<p>The frequentist approach, which we will work on within this course, is different. Frequentists assume that the underlying process is certain but unknown. We <em>look forward</em> similarly - if the scenario is correct, the likelihood of the data at hand is calculated. But there are no priors and no updates.</p>
<p>Bayesians treat data as given; frequentists treat data as random. For frequentists, data are uncertain, not a generating process, and uncertainty of the data can be calculated, but not of the generating process.</p>
<p>In this way of thinking, we cannot make any direct probabilistic claim about the generating process, which complicates understanding the frequentist terms like p-value or confidence interval.</p>
<p>Frequentist probabilistic claims are about the data, and procedures are about controlling long-term error probabilities.</p>
</section>
<section id="unification" class="level3">
<h3 class="anchored" data-anchor-id="unification">Unification?</h3>
<p>There have been historically many unification attempts. Even the great, almost mythological, father of frequentist hypothesis testing, R. A. Fischer, attempted to make a ‚ÄúBayesian omelette without breaking frequentist eggs‚Äù, that is to use frequentist procedures controlling long-term error probabilities with the result that would give Bayesian, direct probability, claim. Sure, he tried to hide this from the world (because what a shame) by calling it <em>fiduciarity</em>.</p>
<p>From the other tribe, no one lesser than Harold Jeffrey tried to propagate the common use of so-called flat prior probabilities, assigning the same weight to any possibility; this leads, for many probability distributions, to mathematically identical results to the frequentist procedures and thus getting ‚Äúfrequentist omelette without breaking Bayesians eggs‚Äù.</p>
<p>Here are my closing personal two cents: The greatest strength of the Bayesian approach is the possibility of directly involving information on possibilities in the estimates.</p>
<p>Suppose I believe strongly that height and weight are positively correlated. In that case, I will benefit from putting these beliefs in the prior distribution because my posteriors will be much narrower and estimates more precise.</p>
<p>Trying to unite the two approaches in such cases does not make sense because I would pretend that I do not know the results in the realms of possibilities.</p>
<p>On the other hand, there are many instances in which I have no such knowledge, and I do not want to get involved in the results in this way. I might want to follow the procedure with a given and known error rate; that would lead me to choose frequentist hypothesis testing.</p>
</section>
</section>
<section id="sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling">Sampling</h2>
<p>The process of obtaining data is, in inferential statistics, called sampling. We might imagine sampling from a given finite population as the simplest case of sampling or sampling from a given probability distribution or infinite population (generating process) as another case.</p>
<p>Population is an interesting term. In its simple use, it means <em>all the units we can sample from</em>. For example, we can sample some countries to observe the relationship between inflation and unemployment rates. In such a case, we could say, in this simplified manner, that our population are all the countries in the world.</p>
<p>But what if we had data from all the countries in the world? Would it be enough to describe what we observe in this <em>population</em>, and would all the uncertainty about the relationship between inflation and unemployment rates disappear?</p>
<p>Hardly so. To counter this, there is a concept called <em>superpopulation</em> - the infinite population of all the possible countries that could have existed, from which we observe only our finite <em>population</em>.</p>
<p>So, even with all the countries in the world assuming some general relation between unemployment and inflation rates, we still need to quantify the uncertainty arising from the sampling process regarding this relationship.</p>
<section id="random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="random-sampling">Random sampling</h3>
<p>Random sampling is a process of sampling in which each unit in the population has an equal probability of being sampled. In this case, sampling with or without replacement might change the probability of being in the sample (or even being there multiple times) and some variance measures, as we have seen in the cases of binomial and hypergeometric distributions.</p>
<p>There are many possibilities for creating random samples, ranging from simple sampling to systematic, stratified, or cluster sampling procedures.</p>
<p>Still, as long as the probabilities are equal for all the units, it does not generally matter.</p>
<p>Another case is a sampling from an infinite population (or superpopulation) or, even more generally, a data-generating process. For this type of <em>sampling</em> to be considered random, we assume it remains consistent (the same) for each sampling step.</p>
<p>Random sampling is very important because it simplifies calculations and fulfils the conditions of independence and identical distribution, which are important background assumptions of stochastical convergences.</p>
</section>
<section id="non-random-sampling" class="level3">
<h3 class="anchored" data-anchor-id="non-random-sampling">Non-random sampling</h3>
<p>The definition of non-random sampling is straightforward negation of random sampling. It is such a sampling in which there is at least one unit in the population with an unequal probability of being sampled. The process is inconsistent at least at one step for an infinite population (superpopulation or data-generating process).</p>
<p>The world is full of non-random samples violating thus important assumptions behind stochastic convergences, which often leads to systematic differences between estimates and true values of characteristics - known as sampling bias.</p>
<p>Examples of non-random sampling are convenience, purposive, snowball or quota sampling.</p>
<p>This bias might be unrepairable, but it might be alleviated, e.g.&nbsp;by specifying the target population differently. For example, suppose you sample students by posting an online questionnaire distributed via social networks. In that case, you can hardly assume that all the students of your target institution have an equal chance of answering this. But you might try to claim that your (super)population consists of students on social networks willing to answer questionnaires.</p>
<p>Alternatively, you might assume that the non-random nature of sampling does not influence the characteristic of your interest, e.g.&nbsp;that for the relation between height and weight, your social questionnaire is a random sample because this relation is the same in the population of all the students and population of students on social networks willing to answer questionnaires.</p>
<p>This is still an assumption; you should be honest, at least with yourselves. You might try to test it in a different setting.</p>
</section>
<section id="sampling-from-a-bernoulli-distribution" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-a-bernoulli-distribution">Sampling from a Bernoulli distribution</h3>
<p>In this type of sampling, we assume our population consists of <em>N</em> units, of which <em>m</em> units have some property coded as 1, and the rest do not possess this property, coded as 0. This means that the probability of seeing the property in one step of the random sampling process is <span class="math inline">\(\pi=\frac{m}{N}\)</span>. Let¬¥s also denote the sample size as <em>n</em>.</p>
<p>Now imagine that you randomly sample these <em>n</em> units (or assume they are generated from the data-generating process). Then, you can imagine another random sample, another, another, and so on. These samples, usually called data, are thus random.</p>
<p>You can calculate the characteristics of each sample. For sampling from the Bernoulli distribution, the two most important are sample total <span class="math inline">\(m=\sum_{i=1}^{n}x_i\)</span> and relative frequency <span class="math inline">\(p=\frac{m}{n}\)</span>. From probability theory, you should be able to recall that <em>M</em> follows binomial (sampling with replacement) or hypergeometric (without replacement) distribution.</p>
<p>Characteristics of the sample are called statistics. Statistics are functions of values observed in the sample and nothing else.</p>
<section id="detour" class="level4">
<h4 class="anchored" data-anchor-id="detour">Detour</h4>
<p>If data are random, then statistics also have to be random. They are random variables <span class="math inline">\(M,P\)</span> with probability distributions dependent on the parameters of the data-generating process. This makes statistics basic of the inference - they can always be calculated in the observed data and are connected to the (assumed) underlying probability distribution.</p>
<p>Distributions of statistics are called sampling distributions and will be specifically introduced later. But recall the idea: generating process leads to random data with random statistics.</p>
</section>
</section>
<section id="sampling-from-a-continuous-distribution" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-a-continuous-distribution">Sampling from a (continuous) distribution</h3>
<p>When sampling from binomial distribution, we have characteristics dependent on the parameter <span class="math inline">\(\pi\)</span>. However, when generalising this idea, we must introduce a random variable with the expected value <em>E</em>(<em>X</em>) and the variance <em>Var</em>(<em>X</em>). (and we might wander further talking about other characteristics).</p>
<p>So let¬¥s have a population of size <em>N</em>, with mean <span class="math inline">\(\bar{x}\)</span> and variance <span class="math inline">\(s_x^2\)</span>, or an infinite population, superpopulation, or data-generating process with the expected value <em>E</em>(<em>X</em>) and the variance <em>Var</em>(<em>X</em>). These are identical for our purposes. Let¬¥s also have a sample of size <em>n</em>.</p>
<p>Our most important sample characteristics - statistics, are sample total <em>m</em>, sample mean <span class="math inline">\(\bar{x}\)</span> and sample variance <span class="math inline">\(s_x^2=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}\)</span>. Again, using the same logic introduced in the Detour, these are random variables; hence <span class="math inline">\(M,\bar{X},S_x^2\)</span>, their values can be calculated for any data, and their distributions depend on the population distribution (or data-generating process properties).</p>
</section>
</section>
<section id="characteristics-of-statistics" class="level2">
<h2 class="anchored" data-anchor-id="characteristics-of-statistics">Characteristics of statistics</h2>
<p>Since statistics are random variables, they have to have their characteristics.</p>
<p><span class="math display">\[
E(M)=n*E(X);Var(M)=n*Var(X)
\]</span></p>
<p><span class="math display">\[
E(\bar{X})=E(X);Var(\bar{X})=\frac{Var(X)}{n}
\]</span></p>
<p><span class="math display">\[
E(S^2)=Var(X);Var(S^2)\xrightarrow{n\to\infty}0
\]</span></p>
<p>If we assume sampling from a finite population without replacement, total and mean variances must be multiplied by a finite correction factor <span class="math inline">\(\frac{N-n}{N-1}\)</span>, known from hypergeometric distribution.</p>
</section>
<section id="sampling-distributions" class="level2">
<h2 class="anchored" data-anchor-id="sampling-distributions">Sampling distributions</h2>
<p>Sampling distributions are distributions of statistics.</p>
<p>It might be unclear if you had descriptive statistics because their statistics, like sample mean or sample variance, were just numbers and constants describing some property of observed data.</p>
<p>However, in inferential (frequentist) statistics, we think about statistics as random variables - and the value observed is only one of many possible values that could have arisen during the sampling or data-generating process.</p>
<p>What are their distributions? What are sampling distributions? It depends on the distribution of the population or the data-generating process.</p>
<section id="sampling-distributions-from-bernoulli" class="level3">
<h3 class="anchored" data-anchor-id="sampling-distributions-from-bernoulli">Sampling distributions from Bernoulli</h3>
<p>If <span class="math inline">\(X_i\sim Be(\pi)\to E(X_i)=\pi;Var(X_i)=\pi*(1-\pi)\)</span>, then</p>
<p><span class="math display">\[
M=\sum_{i=1}^{n} X_i\sim Bi(n,\pi)
\]</span></p>
<p><span class="math display">\[
E(M)=n*\pi;Var(M)=n*\pi*(1-\pi)
\]</span></p>
<p><span class="math display">\[
E(P)=\pi;Var(P)=\frac{\pi*(1-\pi)}{n}
\]</span></p>
<p>We know from probability theory that the sample total will follow a binomial distribution, assuming sampling is with replacement. Assuming sampling is without replacement, it would follow hypergeometric distribution, but we will omit this case here. It might be, however, useful to hold this thought for small samples.</p>
<p>By the Moivre-Laplace central limit theorem, we might (and later will) use binomial‚Äôs convergence to normal distribution.</p>
<p><span class="math display">\[
M=\sum_{i=1}^{n}X_i\approx N\left(\mu=n*\pi;\sigma=\sqrt{(n*\pi*(1-\pi)}\right)
\]</span></p>
<p><span class="math display">\[
P=\frac{\sum_{i=1}^{n}X_i}{n}\approx N\left(\mu=\pi;\sigma=\sqrt{\frac{(\pi*(1-\pi)}{n}}\right)
\]</span></p>
</section>
<section id="sampling-distributions-from-normal" class="level3">
<h3 class="anchored" data-anchor-id="sampling-distributions-from-normal">Sampling distributions from normal</h3>
<p>If <span class="math inline">\(X_i\sim N(\mu;\sigma)\)</span>, then</p>
<p><span class="math display">\[
M\sim N\left(n*\mu;\sqrt{n}*\sigma\right)
\]</span></p>
<p><span class="math display">\[ \bar{X}\sim N\left(\mu;\frac{\sigma}{\sqrt{n}}\right) \]</span></p>
<p><span class="math display">\[
S^2\to\frac{n-1}{\sigma^2}S^2\sim \chi^2(n-1)
\]</span></p>
<p><span class="math display">\[
E\left(S^2\right)=\sigma^2;Var\left(S^2\right)=\frac{2}{n-1}\sigma^4
\]</span></p>
<p>You can notice that not only do we have an exact distribution of sample total and sample mean, but we also have a distribution of the specific transformation of sample variance. We have exact characteristics of sample variance.</p>
<p>All this is derived from the assumption of normal distribution of the underlying (superpopulation, data-generating process) random variable.</p>
<p>Let¬¥s now focus a little on the transformation of sample variance. You might notice that while sample variance is a statistic because its value can be calculated from the sample only, the provided transformation cannot be considered statistics because it can be calculated only with the knowledge of <span class="math inline">\(\sigma\)</span>.</p>
<p>This type of random variable, dependent on the sample values and the parameter value, is called a <em>pivot</em>. The second important property of pivots is that their distribution is <strong>independent</strong> of the underlying distribution (spare the normality assumption). Pivots are historically extremely important because they connect parameters with the sample so that we can calculate probabilities in a general manner (using tables). This connection is the basis for standard hypothesis testing.</p>
<section id="detour-on-chi2-distribution" class="level4">
<h4 class="anchored" data-anchor-id="detour-on-chi2-distribution">Detour on <span class="math inline">\(\chi^2\)</span> distribution</h4>
<p>The pivot introduced in the previous paragraph follows <span class="math inline">\(\chi^2\)</span> distribution assuming <span class="math inline">\(X_i\sim N(\mu;\sigma)\)</span>. How does the <span class="math inline">\(\chi^2\)</span> distribution arise in general?</p>
<p>Let¬¥s have a random variable with standard normal distribution: <span class="math inline">\(Z_i\sim N(0;1)\)</span>, and let¬¥s think of the sum of its squared values; then this sum follows a chi-squared distribution with <span class="math inline">\(k-1\)</span> degrees of freedom:</p>
<p><span class="math display">\[
\sum_{i=1}^{k}Z_i \sim \chi^2(k-1).
\]</span></p>
<p>How do you get to this formula from the presented pivot?</p>
<p><span class="math display">\[
\frac{n-1}{\sigma^2}S^2=\frac{n-1}{\sigma^2}*\frac{\sum_{i=1}^n \left( X_i-\bar{X} \right) ^2}{n-1}=
\sum_{i=1}^{n}\frac{\left(X_i-\bar{X}\right)^2}{\sigma^2}=
\sum_{i=1}^{n} \left( \frac{X_i-\bar{X}}{\sigma}  \right)^2
\]</span></p>
<p>From the assumption that <span class="math inline">\(X_i\sim N(\mu;\sigma)\)</span> follows the sampling distribution of <span class="math inline">\(\bar{X} \sim N\left(\mu;\frac{\sigma}{\sqrt{n}} \right)\)</span>, by subtracting the two, we have centred normal distribution and by dividing them by <span class="math inline">\(\sigma\)</span> we have a standard normal distribution, hence <span class="math inline">\(\sum_{i=1}^{n} Z_i^2 \sim \chi^2(n-1)\)</span>.</p>
<p>The parameter of <span class="math inline">\(\chi^2\)</span> distribution is called degrees of freedom, usually denoted as <em>df</em> or <em>k</em>. Why such a name? If you go back to the sample distribution, you have <em>n</em> degrees of freedom to begin with - you can manipulate those <em>n</em> values in the sample any way you want. However, since you use sample mean <span class="math inline">\(\bar{X}\)</span>, one of those values has to be <em>dedicated</em>, which means you can choose <span class="math inline">\(n-1\)</span> values any way you want, but the last one has to be such that you observe a specific value of the sample mean <span class="math inline">\(\bar{x}\)</span> .</p>
</section>
</section>
<section id="following-the-distribution-of-the-sample-mean" class="level3">
<h3 class="anchored" data-anchor-id="following-the-distribution-of-the-sample-mean">Following the distribution of the sample mean</h3>
<p>As already stated, If <span class="math inline">\(X_i\sim N(\mu;\sigma)\)</span>, then</p>
<p><span class="math display">\[ \bar{X}\sim N\left(\mu;\frac{\sigma}{\sqrt{n}}\right) \]</span></p>
<p>We can introduce a new pivot or two; the first one is the standardised value of the sample mean, coming from the fact that under normality assumption <span class="math inline">\(\bar{X}\sim N \left(\mu;\frac{\sigma}{\sqrt{n}} \right)\)</span>:</p>
<p><span class="math display">\[
Z=\frac{\bar{X}-\mu}{\sqrt{\frac{\sigma^2}{n}}}=
\frac{\bar{X}-\mu}{\sigma}*\sqrt{n} \sim N(0;1)
\]</span></p>
<p>While this pivot is theoretically sound and often serves as a basis for introducing basic concepts in inferential statistics, it is not practically useful - say you want to test the hypothesis about expected value <span class="math inline">\(\mu\)</span> using some observed sample values - to be able to calculate this pivot, you would need to know the value of <span class="math inline">\(\sigma\)</span>, which is even less likely than knowing directly value of <span class="math inline">\(\mu\)</span> (in which case you would not have to test a hypothesis at the first place).</p>
<p>The practical solution to this problem is to use a sample estimate of <span class="math inline">\(\sigma\)</span>, the sample standard deviation <em>S</em>. This is called a plug-in estimator - you are not interested in what value ùúé has but need to guess it to estimate <span class="math inline">\(\mu\)</span> correctly. The pivot is as follows:</p>
<p><span class="math display">\[
T=\frac{\bar{X}-\mu}{\sqrt{\frac{S^2}{n}}}=
\frac{\bar{X}-\mu}{S}*\sqrt{n} \sim t(n-1)
\]</span></p>
<p>This pivot now does not follow a standard normal distribution but a Student¬¥s distribution with <span class="math inline">\(n-1\)</span> degrees of freedom.</p>
<section id="detour-on-students-t-distribution." class="level4">
<h4 class="anchored" data-anchor-id="detour-on-students-t-distribution.">Detour on Student¬¥s <em>t</em> distribution.</h4>
<p>Student¬¥s <em>t</em> distribution is well known for its discovery by <em>Student</em>, a pseudonym for Guinness Brewery statistician William Gossett. At times before the Great War, Guinness was expanding its exports and needed to make sure that wherever you order your pint of their now-famous ale, it would have the same quality as if ordered in Ireland.</p>
<p>His task was to control this quality and devise a solution to the so-called small-sample problem. When testing the quality of the new batch, he wanted to use only a small sample of it. However, he knew using a standard normal distribution for this task was inappropriate, so he had to devise an analytical solution.</p>
<p>This solution was published under the pseudonym <em>Student</em>, as the brewery¬¥s policy did not allow him to publish under his name.</p>
<p>Let¬¥s assume that <span class="math inline">\(Z\sim N(0;1)\)</span> and <span class="math inline">\(G \sim \chi^2(k)\)</span> are independent. Then</p>
<p><span class="math display">\[
T=\frac{Z}{\sqrt{\frac{G}{k}}} \sim t(k)
\]</span></p>
<p>How do we arrive at the pivot above from this theoretical solution? We use already known pivots <span class="math inline">\(Z=\frac{\bar{X}-\mu}{\sigma}*\sqrt{n}\sim N(0;1)\)</span> and <span class="math inline">\(G=\frac{n-1}{\sigma^2}*S^2 \sim \chi^2(n-1)\)</span>.</p>
<p><span class="math display">\[ T=\frac{\frac{\left(\bar{X}-\mu\right)}{\sigma}*\sqrt{n}}{\sqrt{\frac{\frac{(n-1)}{\sigma^2}*S^2}{n-1}}}
= \frac{\bar{X}-\mu}{S}*\sqrt{n}
\sim t(n-1) \]</span></p>
<p>Student¬¥s <em>t</em> distribution is thus the exact distribution of the pivot assuming the normality of the originating random variable. It is a distribution quite similar to the standard normal - is symmetric and centred around 0, but has so-called heavier tails than the normal - meaning that it generates extreme values with higher probabilities.</p>
<p>The lower the number of degrees of freedom, the heavier the tails are, reflecting large uncertainties arising in the variation of the mean in the small samples when using a plug-in estimator of <span class="math inline">\(\sigma\)</span>.</p>
<p>For increasing sample sizes, followed by increasing degrees of freedom, the Student¬¥s <em>t</em> distribution converges in distribution to standard normal distribution:</p>
<p><span class="math display">\[
t(k) \xrightarrow{D;k \to \infty}N(0;1)
\]</span></p>
<p>This means that for the sample sizes large enough, Student¬¥s t distribution can be substituted by standard normal distribution with a little loss of precision. Assuming normal distribution in the generating process, as we are, the <em>large enough</em> can be translated into a few dozen.</p>
<p>Historically, for practical purposes, Student¬¥s <em>t</em> distribution was used for the sample sizes up to 31 because tables usually contained 30 rows of quantiles of this distribution (it is not analytically solvable). For higher sample sizes, standard normal distribution was used.</p>
<p>Nowadays, with the widespread use of computers, there is no reason NOT to use Student¬¥s <em>t</em> distribution.</p>
</section>
</section>
</section>
<section id="sampling-distribution-of-the-sample-mean---general-case" class="level2">
<h2 class="anchored" data-anchor-id="sampling-distribution-of-the-sample-mean---general-case">Sampling distribution of the sample mean - general case</h2>
<p>So far, we have followed the assumption of the normality of the data-generating process. But what if the underlying distribution is not normal?</p>
<p>Luckily, there is a go-around - the famous central limit theorem, specifically Lindenber-L√©vy. Just recall that this theorem roughly states that the total and the mean of independent identically distributed random variables converge in distribution to a normal distribution, assuming finite variance.</p>
<p><span class="math display">\[
X \sim ? E(X);Var(X) \to \bar{X} \xrightarrow{D;n\to\infty}N\left(E(X);\sqrt {\frac{Var(X)}{n}} \right)
\]</span></p>
<p>So, for sample sizes large enough, we might assume that the sampling distribution of the sample mean is approximately normal:</p>
<p><span class="math display">\[\bar{X} \approx N\left(E(X), \sqrt{\frac{Var(X}{n}}  \right)\]</span></p>
<p>Hence, the standardised mean is approximately the standard normal.</p>
<p><span class="math display">\[
Z=\frac{\bar{X}-E(X)}{S.D.(X)}*\sqrt{n} \approx N(0;1)
\]</span></p>
<p>Using a plug-in estimate, substituting population variance with sample variance, the standardised mean follows approximately Student¬¥s <em>t</em> distribution.</p>
<p><span class="math display">\[
T=\frac{\bar{X}-E(X)}{S}*\sqrt{n} \approx t(n-1)
\]</span></p>
<p>The important notion is that the sample size should be <em>large enough.</em> There is no clear boundary - the further away the data-generating process is from normality, especially regarding skewness and kurtosis, the more observations are needed. Sometimes, a few dozen are fine; sometimes, thousands are not enough.</p>
<p>For the historical reasons already stated, the boundary of 30 observations is ingrained in the collective memory even though there is no theoretical reason for it to be used.</p>
<p>The last note: because we assume the sample size is large enough (for CLT to kick in), we can use standard normal distribution instead of Student¬¥s <em>t</em>.</p>
<p>That is why one of the parts of the initial data analysis before hypothesis testing is often normality testing. However, using formal tests is not an appropriate procedure for the reasons connected to the concept of power in hypothesis testing; rather, a graphic comparison of the observed distribution with the normal distribution is much more suitable. Decisions should be made subjectively on whether deviations are large enough given the sample size. These things are complicated, and beware of simplicist procedures; <em>it is what it is</em>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>