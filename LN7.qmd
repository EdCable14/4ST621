---
title: "Two Selected Multivariate Distributions"
author: "Adam Cabla"
---

## Introduction

This part will continue with two specific named multivariate probability distributions.

## Multinomial distribution $X\sim Mn(n,\boldsymbol{\pi})$

In binomial distribution, Bernoulli´s random trials produced two possible outcomes - 0, 1, and counting how many 1s (*successes*) we can observe. The multinomial distribution is a generalisation of this idea.

One random trial can (has to) end up with exactly one of the *k* possible outcomes $i$ with given probabilities $\pi_i$ for $i=1,2,...,k$. If we repeat this random trial *n* times and observe absolute frequencies $x_i$ of the occurrences of the possible outcomes, the probability distribution of a random vector $\textbf{X}=(X_1,X_2,...,X_k)´$ is called multinomial. Its probability function is

$$
P\left(\textbf{X}= \left( \begin{matrix}
x_1 \\
x_2 \\
... \\
x_k 
\end{matrix} \right)=\textbf{x} \right) =
\frac{n!}{x_1!x_2!...x_k!}\pi_1^{x_1}\pi_2^{x_2}...\pi_3^{x_3}
$$

For $x_i \in \{0, 1, 2,...,n\}; \sum_{i=1}^kx_i=n$ and $n\in \{1,2,... \}$, $\pi_i \in (0;1);\sum_{i=1}^k \pi_i=1$.

The characteristics of the random vector are

$$
E(\textbf{X})= \left( \begin{matrix}
n\pi_1 \\
n\pi_1 \\
... \\
n\pi_k
\end{matrix} \right) = n\boldsymbol{\pi}
$$

$$
\boldsymbol{\Sigma} = \left( \begin{matrix}
n\pi_1(1-\pi_1) & -n\pi_1\pi_2 &... & -n\pi_1\pi_k \\
-n\pi_2\pi_1 & n\pi_2(1-\pi_2) &...& -n\pi_2\pi_k \\
... &... &... &... \\
-n\pi_k\pi_1 & -n\pi_k\pi_2 & ...& n\pi_k(1-\pi_k)
\end{matrix}
\right)
$$

If we put *k* = 2, the distribution is identical to the binomial distribution, only differently written.

## Multivariate Normal Distribution $\textbf{X} \sim N^k(\boldsymbol{\mu},\boldsymbol{\Sigma})$

The multivariate normal distribution is a generalised version of the normal distribution to *k* dimensions. Its vector parameters are characteristics at the same time.

$$
E(\textbf{X})= \left( \begin{matrix}  
\mu_1 \\
\mu_2 \\
... \\
\mu_k
\end{matrix}
\right)
$$

$$
(\boldsymbol\Sigma)= \left(
\begin{matrix}
\sigma_1^2 & \sigma_{12} & ... &\sigma_{1k} \\
\sigma_{21} & \sigma_2^2 & ... & \sigma_{2k} \\
...&...&...&... \\
\sigma_{k1} & \sigma_{k2} & ... & \sigma_k^2
\end{matrix}
\right)
$$

You can see that the variance of the *i*-th random variable is noted as $\sigma^2$, and the covariance between the *i*-th and *j-*th random variables is noted as $\sigma_{ij}$. This notation is fairly common in the older probability theory texts as a general notation for variance and covariance. It is, however, reserved only for the (multivariate) normal distribution in these lecture notes.

For now, we will focus on the case of a two-variate (two-dimensional) normal distribution, *k* = 2.

One of the nice properties of a multivariate normal distribution is the stability of the normality of any linear transformation. In the end, it means that every marginal distribution is (multivariate) normal, and every conditional distribution is (multivariate) normal.

Marginal characteristics are taken from the joint characteristics, and conditional characteristics, regression and skedastic functions are as follows:

$$
E(X_1|x_2)=\mu_1+\rho \frac{\sigma_1}{\sigma_2}(x_2-\mu_2)
$$

$$
Var(X_1|x_2)=\sigma_1^2\left(1-\rho^2\right)
$$

The regression function is a linear function of $x_2$, and the formula expresses its reliance on the correlation - if the correlation is negative, it has a negative slope and vice versa.

The skedastic function shows the meaning of the correlation coefficient - its squared value tells us how much is the variance of $X_1$ reduced by knowing the value $x_2$. In linear regression analysis, this concept is known as *R*-*squared* and measures the fit of the regression function.

The last but not least important property to note here is that in a multivariate normal distribution, all the relations are linear, and the correlation coefficient is thus a correct measure of the strength of the association.
