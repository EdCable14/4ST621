---
title: "Sampling"
author: "Adam Cabla"
---

## Introduction

To open the inferential statistics part of our course, we have to start with the motivation behind it and with the (simplified) sampling theory.

## Why inference?

To infer means to make a generalising statement. Statistical teaching usually starts with descriptive or exploratory statistics. In this branch, students are taught how to describe the data at hand - how to visualise and present them, and what measures to use to provide typical value or typical spread in the data. Many terms in descriptive statistics are identical to those used in probability theory, e.g., variance or median, which might build a little confusion.

Here is the deal: we worked with theoretical concepts in probability theory and calculated consequences of what-if scenarios, while in descriptive statistics, you calculate specific values from the data.

The issue is that you can´t go any further when describing the data at hand - no generalisation is possible without additional assumptions. But the human brain is a big machine that makes predictions and generalisations constantly; we can´t help it; it is our survival edge.

Inferential statistics is an attempt to satisfy our brain´s hunger for generalisations from the data to the process that generated them using some coherent and logical framework.

As an example, because we know that under quite realistic assumptions, a sample mean converges to the expected value for increasing sample size, we might use the mean of the data to guess, to estimate, the expected value of the generating process and to guess uncertainty around this estimate.

## Inferential division

There are two main inferential schools of thought, those coherent and logical frameworks - frequentist and Bayesian.

### Bayesian tribe

For Bayesians, the probability is a "degree of belief" - the underlying distribution (process) is uncertain, but this uncertainty is formalised by specifying prior probabilities.

In the second step, Bayesians are *looking forward -* if this prior scenario (distribution, parametric value,..) is correct, then the likelihood of observing the data we have at hand is calculated.

In the third step, Bayesians update their priors using the Bayes theorem to arrive at posterior probabilities (that data comes from this distribution and not some alternative; that parameters have these values and not some alternative..).

The Bayesian approach has a great advantage - one can make a direct probabilistic claim about the generating process, which is an intuitive way of communicating uncertainty. On the other hand, they do not work with long-term error probabilities, which are the basis for formal hypothesis testing.

### Frequentist tribe

The frequentist approach, which we will work on within this course, is different. Frequentists assume that the underlying process is certain but unknown. We *look forward* similarly - if the scenario is correct, the likelihood of the data at hand is calculated. But there are no priors and no updates.

Bayesians treat data as given; frequentists treat data as random. For frequentists, data are uncertain, not a generating process, and uncertainty of the data can be calculated, but not of the generating process.

In this way of thinking, we cannot make any direct probabilistic claim about the generating process, which complicates understanding the frequentist terms like p-value or confidence interval.

Frequentist probabilistic claims are about the data, and procedures are about controlling long-term error probabilities.

### Unification?

There have been historically many unification attempts. Even the great, almost mythological, father of frequentist hypothesis testing, R. A. Fischer, attempted to make a "Bayesian omelette without breaking frequentist eggs", that is to use frequentist procedures controlling long-term error probabilities with the result that would give Bayesian, direct probability, claim. Sure, he tried to hide this from the world (because what a shame) by calling it *fiduciarity*.

From the other tribe, no one lesser than Harold Jeffrey tried to propagate the common use of so-called flat prior probabilities, assigning the same weight to any possibility; this leads, for many probability distributions, to mathematically identical results to the frequentist procedures and thus getting "frequentist omelette without breaking Bayesians eggs".

Here are my closing personal two cents: The greatest strength of the Bayesian approach is the possibility of directly involving information on possibilities in the estimates.

Suppose I believe strongly that height and weight are positively correlated. In that case, I will benefit from putting these beliefs in the prior distribution because my posteriors will be much narrower and estimates more precise.

Trying to unite the two approaches in such cases does not make sense because I would pretend that I do not know the results in the realms of possibilities.

On the other hand, there are many instances in which I have no such knowledge, and I do not want to get involved in the results in this way. I might want to follow the procedure with a given and known error rate; that would lead me to choose frequentist hypothesis testing.

## Sampling

The process of obtaining data is, in inferential statistics, called sampling. We might imagine sampling from a given finite population as the simplest case of sampling or sampling from a given probability distribution or infinite population (generating process) as another case.

Population is an interesting term. In its simple use, it means *all the units we can sample from*. For example, we can sample some countries to observe the relationship between inflation and unemployment rates. In such a case, we could say, in this simplified manner, that our population are all the countries in the world.

But what if we had data from all the countries in the world? Would it be enough to describe what we observe in this *population*, and would all the uncertainty about the relationship between inflation and unemployment rates disappear?

Hardly so. To counter this, there is a concept called *superpopulation* - the infinite population of all the possible countries that could have existed, from which we observe only our finite *population*.

So, even with all the countries in the world assuming some general relation between unemployment and inflation rates, we still need to quantify the uncertainty arising from the sampling process regarding this relationship.

### Random sampling

Random sampling is a process of sampling in which each unit in the population has an equal probability of being sampled. In this case, sampling with or without replacement might change the probability of being in the sample (or even being there multiple times) and some variance measures, as we have seen in the cases of binomial and hypergeometric distributions.

There are many possibilities for creating random samples, ranging from simple sampling to systematic, stratified, or cluster sampling procedures.

Still, as long as the probabilities are equal for all the units, it does not generally matter.

Another case is a sampling from an infinite population (or superpopulation) or, even more generally, a data-generating process. For this type of *sampling* to be considered random, we assume it remains consistent (the same) for each sampling step.

Random sampling is very important because it simplifies calculations and fulfils the conditions of independence and identical distribution, which are important background assumptions of stochastical convergences.

### Non-random sampling

The definition of non-random sampling is straightforward negation of random sampling. It is such a sampling in which there is at least one unit in the population with an unequal probability of being sampled. The process is inconsistent at least at one step for an infinite population (superpopulation or data-generating process).

The world is full of non-random samples violating thus important assumptions behind stochastic convergences, which often leads to systematic differences between estimates and true values of characteristics - known as sampling bias.

Examples of non-random sampling are convenience, purposive, snowball or quota sampling.

This bias might be unrepairable, but it might be alleviated, e.g. by specifying the target population differently. For example, suppose you sample students by posting an online questionnaire distributed via social networks. In that case, you can hardly assume that all the students of your target institution have an equal chance of answering this. But you might try to claim that your (super)population consists of students on social networks willing to answer questionnaires.

Alternatively, you might assume that the non-random nature of sampling does not influence the characteristic of your interest, e.g. that for the relation between height and weight, your social questionnaire is a random sample because this relation is the same in the population of all the students and population of students on social networks willing to answer questionnaires.

This is still an assumption; you should be honest, at least with yourselves. You might try to test it in a different setting.

### Sampling from a Bernoulli distribution

In this type of sampling, we assume our population consists of *N* units, of which *m* units have some property coded as 1, and the rest do not possess this property, coded as 0. This means that the probability of seeing the property in one step of the random sampling process is $\pi=\frac{m}{N}$. Let´s also denote the sample size as *n*.

Now imagine that you randomly sample these *n* units (or assume they are generated from the data-generating process). Then, you can imagine another random sample, another, another, and so on. These samples, usually called data, are thus random.

You can calculate the characteristics of each sample. For sampling from the Bernoulli distribution, the two most important are sample total $m=\sum_{i=1}^{n}x_i$ and relative frequency $p=\frac{m}{n}$. From probability theory, you should be able to recall that *M* follows binomial (sampling with replacement) or hypergeometric (without replacement) distribution.

Characteristics of the sample are called statistics. Statistics are functions of values observed in the sample and nothing else.

#### Detour

If data are random, then statistics also have to be random. They are random variables $M,P$ with probability distributions dependent on the parameters of the data-generating process. This makes statistics basic of the inference - they can always be calculated in the observed data and are connected to the (assumed) underlying probability distribution.

Distributions of statistics are called sampling distributions and will be specifically introduced later. But recall the idea: generating process leads to random data with random statistics.

### Sampling from a (continuous) distribution

When sampling from binomial distribution, we have characteristics dependent on the parameter $\pi$. However, when generalising this idea, we must introduce a random variable with the expected value *E*(*X*) and the variance *Var*(*X*). (and we might wander further talking about other characteristics).

So let´s have a population of size *N*, with mean $\bar{x}$ and variance $s_x^2$, or an infinite population, superpopulation, or data-generating process with the expected value *E*(*X*) and the variance *Var*(*X*). These are identical for our purposes. Let´s also have a sample of size *n*.

Our most important sample characteristics - statistics, are sample total *m*, sample mean $\bar{x}$ and sample variance $s_x^2=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}$. Again, using the same logic introduced in the Detour, these are random variables; hence $M,\bar{X},S_x^2$, their values can be calculated for any data, and their distributions depend on the population distribution (or data-generating process properties).

## Characteristics of statistics

Since statistics are random variables, they have to have their characteristics.

$$
E(M)=n*E(X);Var(M)=n*Var(X)
$$

$$
E(\bar{X})=E(X);Var(\bar{X})=\frac{Var(X)}{n}
$$

$$
E(S^2)=Var(X);Var(S^2)\xrightarrow{n\to\infty}0
$$

If we assume sampling from a finite population without replacement, total and mean variances must be multiplied by a finite correction factor $\frac{N-n}{N-1}$, known from hypergeometric distribution.

## Sampling distributions

Sampling distributions are distributions of statistics.

It might be unclear if you had descriptive statistics because their statistics, like sample mean or sample variance, were just numbers and constants describing some property of observed data.

However, in inferential (frequentist) statistics, we think about statistics as random variables - and the value observed is only one of many possible values that could have arisen during the sampling or data-generating process.

What are their distributions? What are sampling distributions? It depends on the distribution of the population or the data-generating process.

### Sampling distributions from Bernoulli

If $X_i\sim Be(\pi)\to E(X_i)=\pi;Var(X_i)=\pi*(1-\pi)$, then

$$
M=\sum_{i=1}^{n} X_i\sim Bi(n,\pi)
$$

$$
E(M)=n*\pi;Var(M)=n*\pi*(1-\pi)
$$

$$
E(P)=\pi;Var(P)=\frac{\pi*(1-\pi)}{n}
$$

We know from probability theory that the sample total will follow a binomial distribution, assuming sampling is with replacement. Assuming sampling is without replacement, it would follow hypergeometric distribution, but we will omit this case here. It might be, however, useful to hold this thought for small samples.

By the Moivre-Laplace central limit theorem, we might (and later will) use binomial's convergence to normal distribution.

$$
M=\sum_{i=1}^{n}X_i\approx N\left(\mu=n*\pi;\sigma=\sqrt{(n*\pi*(1-\pi)}\right)
$$

$$
P=\frac{\sum_{i=1}^{n}X_i}{n}\approx N\left(\mu=\pi;\sigma=\sqrt{\frac{(\pi*(1-\pi)}{n}}\right)
$$

### Sampling distributions from normal

If $X_i\sim N(\mu;\sigma)$, then

$$
M\sim N\left(n*\mu;\sqrt{n}*\sigma\right)
$$

$$ \bar{X}\sim N\left(\mu;\frac{\sigma}{\sqrt{n}}\right) $$

$$
S^2\to\frac{n-1}{\sigma^2}S^2\sim \chi^2(n-1)
$$

$$
E\left(S^2\right)=\sigma^2;Var\left(S^2\right)=\frac{2}{n-1}\sigma^4
$$

You can notice that not only do we have an exact distribution of sample total and sample mean, but we also have a distribution of the specific transformation of sample variance. We have exact characteristics of sample variance.

All this is derived from the assumption of normal distribution of the underlying (superpopulation, data-generating process) random variable.

Let´s now focus a little on the transformation of sample variance. You might notice that while sample variance is a statistic because its value can be calculated from the sample only, the provided transformation cannot be considered statistics because it can be calculated only with the knowledge of $\sigma$.

This type of random variable, dependent on the sample values and the parameter value, is called a *pivot*. The second important property of pivots is that their distribution is **independent** of the underlying distribution (spare the normality assumption). Pivots are historically extremely important because they connect parameters with the sample so that we can calculate probabilities in a general manner (using tables). This connection is the basis for standard hypothesis testing.

#### Detour on $\chi^2$ distribution

The pivot introduced in the previous paragraph follows $\chi^2$ distribution assuming $X_i\sim N(\mu;\sigma)$. How does the $\chi^2$ distribution arise in general?

Let´s have a random variable with standard normal distribution: $Z_i\sim N(0;1)$, and let´s think of the sum of its squared values; then this sum follows a chi-squared distribution with $k-1$ degrees of freedom:

$$
\sum_{i=1}^{k}Z_i \sim \chi^2(k-1).
$$

How do you get to this formula from the presented pivot?

$$
\frac{n-1}{\sigma^2}S^2=\frac{n-1}{\sigma^2}*\frac{\sum_{i=1}^n \left( X_i-\bar{X} \right) ^2}{n-1}=
\sum_{i=1}^{n}\frac{\left(X_i-\bar{X}\right)^2}{\sigma^2}=
\sum_{i=1}^{n} \left( \frac{X_i-\bar{X}}{\sigma}  \right)^2
$$

From the assumption that $X_i\sim N(\mu;\sigma)$ follows the sampling distribution of $\bar{X} \sim N\left(\mu;\frac{\sigma}{\sqrt{n}} \right)$, by subtracting the two, we have centred normal distribution and by dividing them by $\sigma$ we have a standard normal distribution, hence $\sum_{i=1}^{n} Z_i^2 \sim \chi^2(n-1)$.

The parameter of $\chi^2$ distribution is called degrees of freedom, usually denoted as *df* or *k*. Why such a name? If you go back to the sample distribution, you have *n* degrees of freedom to begin with - you can manipulate those *n* values in the sample any way you want. However, since you use sample mean $\bar{X}$, one of those values has to be *dedicated*, which means you can choose $n-1$ values any way you want, but the last one has to be such that you observe a specific value of the sample mean $\bar{x}$ .

### Following the distribution of the sample mean

As already stated, If $X_i\sim N(\mu;\sigma)$, then

$$ \bar{X}\sim N\left(\mu;\frac{\sigma}{\sqrt{n}}\right) $$

We can introduce a new pivot or two; the first one is the standardised value of the sample mean, coming from the fact that under normality assumption $\bar{X}\sim N \left(\mu;\frac{\sigma}{\sqrt{n}} \right)$:

$$
Z=\frac{\bar{X}-\mu}{\sqrt{\frac{\sigma^2}{n}}}=
\frac{\bar{X}-\mu}{\sigma}*\sqrt{n} \sim N(0;1)
$$

While this pivot is theoretically sound and often serves as a basis for introducing basic concepts in inferential statistics, it is not practically useful - say you want to test the hypothesis about expected value $\mu$ using some observed sample values - to be able to calculate this pivot, you would need to know the value of $\sigma$, which is even less likely than knowing directly value of $\mu$ (in which case you would not have to test a hypothesis at the first place).

The practical solution to this problem is to use a sample estimate of $\sigma$, the sample standard deviation *S*. This is called a plug-in estimator - you are not interested in what value 𝜎 has but need to guess it to estimate $\mu$ correctly. The pivot is as follows:

$$
T=\frac{\bar{X}-\mu}{\sqrt{\frac{S^2}{n}}}=
\frac{\bar{X}-\mu}{S}*\sqrt{n} \sim t(n-1)
$$

This pivot now does not follow a standard normal distribution but a Student´s distribution with $n-1$ degrees of freedom.

#### Detour on Student´s *t* distribution.

Student´s *t* distribution is well known for its discovery by *Student*, a pseudonym for Guinness Brewery statistician William Gossett. At times before the Great War, Guinness was expanding its exports and needed to make sure that wherever you order your pint of their now-famous ale, it would have the same quality as if ordered in Ireland.

His task was to control this quality and devise a solution to the so-called small-sample problem. When testing the quality of the new batch, he wanted to use only a small sample of it. However, he knew using a standard normal distribution for this task was inappropriate, so he had to devise an analytical solution.

This solution was published under the pseudonym *Student*, as the brewery´s policy did not allow him to publish under his name.

Let´s assume that $Z\sim N(0;1)$ and $G \sim \chi^2(k)$ are independent. Then

$$
T=\frac{Z}{\sqrt{\frac{G}{k}}} \sim t(k)
$$

How do we arrive at the pivot above from this theoretical solution? We use already known pivots $Z=\frac{\bar{X}-\mu}{\sigma}*\sqrt{n}\sim N(0;1)$ and $G=\frac{n-1}{\sigma^2}*S^2 \sim \chi^2(n-1)$.

$$ T=\frac{\frac{\left(\bar{X}-\mu\right)}{\sigma}*\sqrt{n}}{\sqrt{\frac{\frac{(n-1)}{\sigma^2}*S^2}{n-1}}} 
= \frac{\bar{X}-\mu}{S}*\sqrt{n}
 \sim t(n-1) $$

Student´s *t* distribution is thus the exact distribution of the pivot assuming the normality of the originating random variable. It is a distribution quite similar to the standard normal - is symmetric and centred around 0, but has so-called heavier tails than the normal - meaning that it generates extreme values with higher probabilities.

The lower the number of degrees of freedom, the heavier the tails are, reflecting large uncertainties arising in the variation of the mean in the small samples when using a plug-in estimator of $\sigma$.

For increasing sample sizes, followed by increasing degrees of freedom, the Student´s *t* distribution converges in distribution to standard normal distribution:

$$
t(k) \xrightarrow{D;k \to \infty}N(0;1)
$$

This means that for the sample sizes large enough, Student´s t distribution can be substituted by standard normal distribution with a little loss of precision. Assuming normal distribution in the generating process, as we are, the *large enough* can be translated into a few dozen.

Historically, for practical purposes, Student´s *t* distribution was used for the sample sizes up to 31 because tables usually contained 30 rows of quantiles of this distribution (it is not analytically solvable). For higher sample sizes, standard normal distribution was used.

Nowadays, with the widespread use of computers, there is no reason NOT to use Student´s *t* distribution.

## Sampling distribution of the sample mean - general case

So far, we have followed the assumption of the normality of the data-generating process. But what if the underlying distribution is not normal?

Luckily, there is a go-around - the famous central limit theorem, specifically Lindenber-Lévy. Just recall that this theorem roughly states that the total and the mean of independent identically distributed random variables converge in distribution to a normal distribution, assuming finite variance.

$$
X \sim ? E(X);Var(X) \to \bar{X} \xrightarrow{D;n\to\infty}N\left(E(X);\sqrt {\frac{Var(X)}{n}} \right)
$$

So, for sample sizes large enough, we might assume that the sampling distribution of the sample mean is approximately normal:

$$\bar{X} \approx N\left(E(X), \sqrt{\frac{Var(X}{n}}  \right)$$

Hence, the standardised mean is approximately the standard normal.

$$
Z=\frac{\bar{X}-E(X)}{S.D.(X)}*\sqrt{n} \approx N(0;1)
$$

Using a plug-in estimate, substituting population variance with sample variance, the standardised mean follows approximately Student´s *t* distribution.

$$
T=\frac{\bar{X}-E(X)}{S}*\sqrt{n} \approx t(n-1)
$$

The important notion is that the sample size should be *large enough.* There is no clear boundary - the further away the data-generating process is from normality, especially regarding skewness and kurtosis, the more observations are needed. Sometimes, a few dozen are fine; sometimes, thousands are not enough.

For the historical reasons already stated, the boundary of 30 observations is ingrained in the collective memory even though there is no theoretical reason for it to be used.

The last note: because we assume the sample size is large enough (for CLT to kick in), we can use standard normal distribution instead of Student´s *t*.

That is why one of the parts of the initial data analysis before hypothesis testing is often normality testing. However, using formal tests is not an appropriate procedure for the reasons connected to the concept of power in hypothesis testing; rather, a graphic comparison of the observed distribution with the normal distribution is much more suitable. Decisions should be made subjectively on whether deviations are large enough given the sample size. These things are complicated, and beware of simplicist procedures; *it is what it is*.
