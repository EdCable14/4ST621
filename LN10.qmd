---
title: "Point estimates"
author: "Adam Cabla"
---

## Introduction

We will introduce the first basic concepts in point estimates in frequentist statistics. To remind you, frequentist statistics is about long-term properties; here, we introduce the long-term properties of the statistics we use to estimate parameters and the most common methods to construct these statistics.

The *long-term* in this setting means *under repeated sampling*. That is, if you repeat the sample of a given size over and over and over, and you write down values of the statistics across this theoretically infinite batch of samples, what are the properties of these statistics? Recall the previous chapter by introducing the characteristics of statistics and their sampling distributions.

First, we need to distinguish three terms:

***The estimator T*** is a statistic that is used to estimate something. This is a random variable, and the properties of this random variable are ofinterest.

***The estimand*** is what we want to estimate, e.g., the parameter $\theta$ of a chosen probability distribution. This is a constant with no probabilistic properties in our frequentist playbook.

***The estimate***is a realisation; it is the estimator's specific value, ***t***, calculated in the data.

## Basic properties of point estimators

We instinctively want our estimators to be connected to the estimands somehow. We would like them to have some properties, and we should be aware that there is sometimes even a trade-off between these properties. Intuitively, the estimator should be, on average, as close to the estimand as possible. This closeness can be divided into two properties - how far away it is on the long-term average and how far away it is in its individual realisations (estimates).

### Bias

The first intuitive property can be written down this way:

$$
E(T)=\theta
$$

This means that the expected value of our estimator should be equal to the parameter we are about to estimate with it. The property is called unbiasedness and is often not achieved. But a softer version, called asymptotic unbiasedness, does exist:

$$
E(\textbf{T})\xrightarrow{n\to\infty}\theta
$$

We can calculate how far away the expectation is from the reality in the quantification of bias:

$$
b(T)=E(T)-\theta
$$

And rewrite both properties as

$$
b(T)=0
$$

$$
b(T)\xrightarrow{n\to\infty}0
$$

### Consistency

Bias represents *systematic error*. The second type of error is *random,* which we mathematically represent as the variance of the estimator $Var(T)$.

Now, the consistency property connects the two properties: systematic error, which is zero (unbiasedness) or going down (asymptotic unbiasedness), and random errors, which go down with the increasing number of observations. So, the two necessary and sufficient conditions of consistency are:

$$
b(T)\xrightarrow{n\to\infty}0
$$

$$ Var(T)\xrightarrow{n\to\infty}0 $$

Consistency means that the more data we observe, the better our ***estimator*** is on the long-term average.

### Accuracy

How well is our estimator doing on a long-term average? Since there are two sources of the error, we can combine them to create an overall error evaluation called *mean squared* error:

$$
MSE(T)=E \left(\left(T-\theta\right)^2\right)=\left(b\left(T\right)\right)^2+Var(T)
$$

The mean squared error tells us what the expected squared error of the estimator is regarding the estimated parameter $\theta$. It is composed of the squared value of the bias and variance of the estimator. If the estimator is unbiased, then the mean squared error is the estimator's variance only.

Because MSE is, similarly to variance, in squared units of the random variable, it is often considered more useful to communicate its square root called *root mean squared error,* which is in the units of the random variable:

$$
RMSE(T)=\sqrt{MSE(T)}
$$

## Basic methods of construction

Estimators can be constructed in many ways, ranging from the *look outside-of-the-window* method to more sophisticated techniques. Generally, the common basic approaches can be divided into two groups: one that matches some theoretical properties of the distribution to similar properties of the data, and the second optimises the selected criterion of fit. From the first group, we will glance at moment-matching and quantile-matching methods; from the second, we will focus on the most commonly applied method - maximum likelihood estimation.

### Method of Moments

The method of moments is based on matching theoretical moments of a given probability distribution with empirical moments calculated in the sample. Take, for example, raw moments:

$$
\mu_k´=\overline{x_k}
$$

This method can be adapted to use various types of moments (central, standardised, L-moments, TL-moments,...). Still, in its simple version, you create as many equations as the number of parameters you need to estimate and start from the bottom $k=1,2,...$.

This method was used for its simplicity and usually yields consistent estimators. On the other hand, the estimators are often worse in terms of accuracy than other options and have some other disadvantages.

It also serves as a teaching introduction to the generalised method of moments applicable under certain conditions or as a first-order solution before proceeding to the maximum likelihood estimate.

### Method of Quantiles

Another example of the matching method is the method of quantiles, but instead of matching moments, it matches theoretical quantiles $x_p$ with empirical quantiles $\widetilde{x_p}$ for various values of *P*.

This method is specifically useful when you want to estimate probability distribution with specific theoretical quantiles of interest, like, e.g. 95% quantile of the log-normal distribution for risk estimate.

### Maximum likelihood method

The last method of finding estimators in this short list is probably the most common and powerful. Let´s first introduce the concept of likelihood and distinguish it from the joint probability distribution of the sample.

We start with the assumption that our sample comes from a random sampling procedure (or unchanging data-generating process). In such a case, individual values are drawn from a sequence of independent identically distributed random variables, and their joint probability distribution can be written as the product of individual densities which are identical:

$$f(\textbf{x},\theta)=\prod_{i=1}^n f(x_i,\theta)$$This is a probability function of fixed parameter $\theta$ and varying data. This answers the question: What are the probabilities of various datasets if the given parameter has this value?

However, it can´t be presented as varying once the dataset is known and fixed. Still, we can switch the idea and ask a different question: if the given dataset looks like this, what are its probabilities under different possible values of the parameter $\theta$? That is what the likelihood function presents:

$$
L(x;\theta)=\prod_{i=1}^{n}f(x_i,\theta)
$$

To repeat: probability density is the probability of varying data under a fixed value of the parameter; likelihood is the probability of fixed data under a varying value of the parameter.

Maximum likelihood estimators arise from maximising likelihood function; that is, they answer the question: under what value of parameter $\theta$ arise data at hand with the highest probability?

Under some general conditions, the likelihood function is unimodal and its maximum can be found using partial derivations:

$$
\frac{\partial L(\textbf{x},\theta)}{\partial \theta}=0
$$

The problem with this is two-fold: analytically, it is quite difficult to find partial derivations of the product. Numerically, this product usually goes too quickly to zero, and the procedure cannot distinguish its value and collapses.

Practically, both of these problems can be solved by using the log-likelihood function, which transforms the product into a sum:

$$
l(\textbf{x};\theta)=log\left(L(\textbf{x};\theta)  \right)=\sum_{i=1}^{n}log(f(x_i;\theta))
$$

Because this transformation is monotonic, we can find the maximum at the same point by partial derivation:

$$
\frac{\partial l(\textbf{x};\theta)}{\partial \theta}=0
$$

Maximum likelihood estimators are often well-behaved with the following properties:

-   (Asymptotically) unbiased

-   Consistent

-   Asymptotically best linear unbiased estimators (lowest variance among unbiased estimators)

-   Asymptotically having normal distribution.

The last point is vital for inferences about estimands. One can see that these properties are often asymptotical, i.e. they should be considered with caution. But generally speaking, maximum likelihood estimators are usually the optimal way to go for medium to large samples and typically have good properties even for small sample sizes.
