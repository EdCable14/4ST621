{"title":"Characteristics of Variability","markdown":{"yaml":{"title":"Characteristics of Variability","author":"Adam Cabla"},"headingText":"Variance and standard deviation","containsRefs":false,"markdown":"\n\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\n\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable $x-E(X)$. This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\n$$E((x-E(X))^1)=\\mu_1$$This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have \"*distances to the* *centre* *inversely proportional to their weights\".*\n\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\n$$\n\\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x)\n$$\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx $$\n\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\n$$Var(X)=E(X^2)-[E(X)]^2$$Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\n\nSome important properties of the variance are:\n\n-   Non-negativity $Var(X)\\geq{0}$\n\n-   Constant (real value *c*) has zero variance $Var(c)=0$\n\n-   For real constants *a* and *c:* $Var(a+cX)=c^2Var(X)$\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in *cm*, then the variance of the height would be measured in *cm*^2^.\n\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a *typical deviation* in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\n$$\nS.D.(X)=\\sqrt{Var(X)}\n$$\n\n## Mean Absolute Deviation\n\nAnother possibility to deal with the problem, that the first central moment is equal to 0, is to use absolute deviations instead of the squared deviations:\n\n$$\nMAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x)\n$$\n\n$$ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx $$\n\nIn probability theory, mean absolute deviation most commonly refer to the deviation from the expected value. This way of measuring variability is a more *common sense typical deviation* in the probability distribution than standard deviation.\n\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tigthly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to expected value.\n\n### Intermezzo\n\nIf we were looking for the real valued constant *c,* for which probability weighted sum of squared deviations from such a constant $\\sum_{x=-\\infty}^\\infty (x-c)^2*P(x)$ is minimal, than it would be expected value $c=E(X)$. So the variance is minimized, if we use expected value as the location characteristic. This has far-reaching consquences e.g. for the linear regression, in which conditional expected values are modeled by linear combination of variables and the *optimal* constants in the model are found by minimizing residual variance. But do not get disturbed too much here with thoughts of linear regression, it is not scope of this course.\n\n### Back to MAD\n\nUsing the same logic as in the intermezzo, what real valued constant *c* would minimize probability weighted sum of absolute deviations $\\sum_{x=-\\infty}^\\infty |x-c|*P(x)$ ? Maybe surprisingly, the constant in this case is not expected value, but median $c=x_{0.5}$.\n\nSo by the same logic, in which expected value is connected to the variance, mean absolute deviation should be connected to the median. And by the same logic, when modelling conditional medians by linear combination of variables, the *optimal* constants are found by minimizing residual mean absolute deviations - this type of modelling is called median (or more generally quantile) regression. Again, do not get disturbed with this.\n\nSo what, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Does it make your head hurt? Sorry, no one promised you it will be always easy.\n\nIn this course, we will use expected value as a measure of location when calculating mean absolute deviation, so follow formulas at the beginning of this sub-chapter.\n\n## Inter-quartile range\n\nApart from measuring variability by moment-based approach, we can also use quantile-based approach. We can measure ranges between pre-specified quantiles, and use them as a measure of variability with probabilistic meaning. The one that we will stick with in the course is inter-quartile range\n\n$$\nIQR(X)=x_{0.75}-x_{0.25}\n$$\n\nInter-quartile range is the range (distance) that is needed to capture middle 50 % of the probability distribution.\n\n## Entropy\n\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures *expected ability to surprise*. It starts with negative logarithmic transformation of probability $-log_b(P(x))$. Value with probability 0 can be viewed as impossible, or *infinitely* *surprising* and indeed $-log_b(0)=Inf$. On the second part of the scale, value with probability 1 is sure to happen, or *totally unsurprising* and indeed $-log_b(1)=0$. So the initial transformation gives us potential measure of *surprisingness* and entropy measures expected value of this\n\n$$\nH(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x))\n$$\n\nNotice, that introduced measure of entropy is based on the probabilities and hence is suitable for disrecete random variables only. The trouble with continuous random variables is, that $f(x)\\geq0$ and if the value of probability density is above 1, the logarithm is positive and entropy can be negative. So we will skip problematic of measuring entropy of continuous random variables and use it only as a measure for discrete random variables.\n\nNow, what units is entropy measured in? It depends on the base of logarithm - *b*. The most natural base, the one used in this course, is 2. Then entropy is measured in *bits*. But other bases can be used, e.g. base 10 leads to units called *dits* or base *e* leads to units called *nats*.\n\nThe little trouble with entropy is, that its value depends on the number of possible values *x* of random variable. So for comparison among random variables with different number of possible values of random variable (*k*)*,* normalised entropy, called efficiency, can be used\n\n$$\n\\eta(X)=\\frac{H(X)}{log_b(k)}\n$$\n\nNow, there is a different way to view entropy. LetÂ´s stick to *b* = 2, that is entropy measured in bits. Then entropy is expected number of Yes/No questions needed to find the result of a random trial.\n\nAs an demonstration: there are 8 horses in a race. You want to transfer information about the winner of the race. Assume all horses are equally likely to win, than the order of the questions does not matter: If horse A wins, the answer is 1 (coding Yes) and message needs to be only 1 bit long. If horse B wins, the answer is 01 (coding No, Yes) and message need to be 2 bits long.\n","srcMarkdownNoYaml":"\n\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\n## Variance and standard deviation\n\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable $x-E(X)$. This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\n$$E((x-E(X))^1)=\\mu_1$$This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have \"*distances to the* *centre* *inversely proportional to their weights\".*\n\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\n$$\n\\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x)\n$$\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx $$\n\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\n$$Var(X)=E(X^2)-[E(X)]^2$$Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\n\nSome important properties of the variance are:\n\n-   Non-negativity $Var(X)\\geq{0}$\n\n-   Constant (real value *c*) has zero variance $Var(c)=0$\n\n-   For real constants *a* and *c:* $Var(a+cX)=c^2Var(X)$\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in *cm*, then the variance of the height would be measured in *cm*^2^.\n\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a *typical deviation* in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\n$$\nS.D.(X)=\\sqrt{Var(X)}\n$$\n\n## Mean Absolute Deviation\n\nAnother possibility to deal with the problem, that the first central moment is equal to 0, is to use absolute deviations instead of the squared deviations:\n\n$$\nMAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x)\n$$\n\n$$ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx $$\n\nIn probability theory, mean absolute deviation most commonly refer to the deviation from the expected value. This way of measuring variability is a more *common sense typical deviation* in the probability distribution than standard deviation.\n\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tigthly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to expected value.\n\n### Intermezzo\n\nIf we were looking for the real valued constant *c,* for which probability weighted sum of squared deviations from such a constant $\\sum_{x=-\\infty}^\\infty (x-c)^2*P(x)$ is minimal, than it would be expected value $c=E(X)$. So the variance is minimized, if we use expected value as the location characteristic. This has far-reaching consquences e.g. for the linear regression, in which conditional expected values are modeled by linear combination of variables and the *optimal* constants in the model are found by minimizing residual variance. But do not get disturbed too much here with thoughts of linear regression, it is not scope of this course.\n\n### Back to MAD\n\nUsing the same logic as in the intermezzo, what real valued constant *c* would minimize probability weighted sum of absolute deviations $\\sum_{x=-\\infty}^\\infty |x-c|*P(x)$ ? Maybe surprisingly, the constant in this case is not expected value, but median $c=x_{0.5}$.\n\nSo by the same logic, in which expected value is connected to the variance, mean absolute deviation should be connected to the median. And by the same logic, when modelling conditional medians by linear combination of variables, the *optimal* constants are found by minimizing residual mean absolute deviations - this type of modelling is called median (or more generally quantile) regression. Again, do not get disturbed with this.\n\nSo what, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Does it make your head hurt? Sorry, no one promised you it will be always easy.\n\nIn this course, we will use expected value as a measure of location when calculating mean absolute deviation, so follow formulas at the beginning of this sub-chapter.\n\n## Inter-quartile range\n\nApart from measuring variability by moment-based approach, we can also use quantile-based approach. We can measure ranges between pre-specified quantiles, and use them as a measure of variability with probabilistic meaning. The one that we will stick with in the course is inter-quartile range\n\n$$\nIQR(X)=x_{0.75}-x_{0.25}\n$$\n\nInter-quartile range is the range (distance) that is needed to capture middle 50 % of the probability distribution.\n\n## Entropy\n\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures *expected ability to surprise*. It starts with negative logarithmic transformation of probability $-log_b(P(x))$. Value with probability 0 can be viewed as impossible, or *infinitely* *surprising* and indeed $-log_b(0)=Inf$. On the second part of the scale, value with probability 1 is sure to happen, or *totally unsurprising* and indeed $-log_b(1)=0$. So the initial transformation gives us potential measure of *surprisingness* and entropy measures expected value of this\n\n$$\nH(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x))\n$$\n\nNotice, that introduced measure of entropy is based on the probabilities and hence is suitable for disrecete random variables only. The trouble with continuous random variables is, that $f(x)\\geq0$ and if the value of probability density is above 1, the logarithm is positive and entropy can be negative. So we will skip problematic of measuring entropy of continuous random variables and use it only as a measure for discrete random variables.\n\nNow, what units is entropy measured in? It depends on the base of logarithm - *b*. The most natural base, the one used in this course, is 2. Then entropy is measured in *bits*. But other bases can be used, e.g. base 10 leads to units called *dits* or base *e* leads to units called *nats*.\n\nThe little trouble with entropy is, that its value depends on the number of possible values *x* of random variable. So for comparison among random variables with different number of possible values of random variable (*k*)*,* normalised entropy, called efficiency, can be used\n\n$$\n\\eta(X)=\\frac{H(X)}{log_b(k)}\n$$\n\nNow, there is a different way to view entropy. LetÂ´s stick to *b* = 2, that is entropy measured in bits. Then entropy is expected number of Yes/No questions needed to find the result of a random trial.\n\nAs an demonstration: there are 8 horses in a race. You want to transfer information about the winner of the race. Assume all horses are equally likely to win, than the order of the questions does not matter: If horse A wins, the answer is 1 (coding Yes) and message needs to be only 1 bit long. If horse B wins, the answer is 01 (coding No, Yes) and message need to be 2 bits long.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN4_c.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Characteristics of Variability","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}