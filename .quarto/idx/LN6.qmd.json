{"title":"Selected Continuous Distributions","markdown":{"yaml":{"title":"Selected Continuous Distributions","author":"Adam Cabla"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nIn this part, we will continue with specific named probability distributions but focus on some continuous probability distributions.\n\n## Continuous uniform distribution $X\\sim U(a,b)$\n\nThis is likely the simplest continuous probability distribution. Uniform means equal probabilities for a discrete case or constant probability density function for a continuous case.\n\nThis distribution has limited real-world use, e.g. as a model of the waiting time for the regularly repeating event. Maybe the more important role of this distribution is its use in generating random numbers. Having generated a random value from the $X\\sim U(0,1)$ distribution, it can be an input to the quantile function of any desired random variable, generating a random value from that random variable.\n\nFrom the constant probability density function, we can derive that it is\n\n$$\nf(x)=\\frac{1}{b-a}\n$$\n\nfor $x\\in [a,b]$ and $-\\infty<a<b<\\infty$.\n\nFor probability calculations, the cumulative distribution function is usually easier to use\n\n$$\nF(x)=\\frac{x-a}{b-a}\n$$\n\nand the quantile function, the inverse of the cumulative distribution function\n\n$$\nQ(P)=a+(b-a)P\n$$\n\nThere is not much more to be said about this distribution; maybe the most common special case 𝑋∼𝑈(0,1) is called standard uniform distribution.\n\n## Exponential distribution $X\\sim E(\\lambda)$\n\nLet´s assume that random events are not regularly repeating but occurring randomly throughout time and independently of each other. The rate of occurrence of these events is the parameter $\\lambda$, the one we have already seen in the Poisson distribution with the same assumptions.\n\nThe focus of the Poisson distribution is the number of *events* in a specified interval; the focus of the exponential distribution is time to the first occurrence of the *event*. With the mentioned assumptions, this time follows the exponential distribution described by the following functions.\n\n$$\nf(x)=\\lambda \\exp(-\\lambda x)\n$$\n\n$$\nF(x)=1-\\exp(-\\lambda x)\n$$\n\n$$\nQ(P)=-\\frac{-\\ln (1-P)}{\\lambda} \n$$\n\nfor $x\\in(0;\\infty)$ and $\\lambda \\in [0,\\infty)$.\n\nThe two main moment characteristics of this distribution are\n\n$$\nE(X)=\\frac{1}{\\lambda}\n$$\n\n$$\nVar(X)=\\frac{1}{\\lambda^2}\n$$\n\nThe exponential and the Poisson distributions are intertwined and serve as the basis for analysing the so-called Poisson process. An exponential distribution is useful in other contexts; for example, it can serve as a model of the right tail of an unknown underlying distribution, which is useful in risk modelling.\n\nIn some distributions, especially continuous ones, parameters are not set in stone, and different parametrisations of the distribution can be used to shift focus on various properties. Of course, all the functions describing the probability distribution and its characteristics must be adjusted to the parametrisation of choice.\n\nFor exponential distribution, one might stumble upon using the scale parameter, which is the inverse of the rate parameter $\\sigma=\\frac{1}{\\lambda}$. This parametrisation might be useful in contexts where the focus is not the random events' occurrence rate but the expected value because $E(X)=\\delta$.\n\n## Normal (Gaussian) distribution $X\\sim N(\\mu,\\sigma)$\n\nThe normal distribution is pervasive in statistics because of its presence as a limited distribution of various statistics, most importantly summation. Normality assumption also simplifies a lot of calculations due to its properties. This makes it very useful in many contexts regarding estimates, which is the topic of the second large part of this lecture notes.\n\nThe probability density of the normal distribution is\n\n$$\nf(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\nFor $x \\in (-\\infty;\\infty)$ and $\\mu \\in (-\\infty, \\infty)$, $\\sigma \\in(0,\\infty)$.\n\nIntegrating this density to get the cumulative distribution function is not analytically solvable, so we do not have a simple closed formula for evaluating its value.\n\n$$\nF(x)=\\int_{-\\infty}^x \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(t-\\mu)^2}{2\\sigma^2}}dt\n$$\n\nWe do not have the closed formula for the quantile function based on the same argument. Characteristics of this random variable are straightforward from the parameters.\n\n$$\nE(X)=\\mu\n$$\n\n$$\nVar(X)=\\sigma^2\n$$\n\nNowadays, obtaining probabilities and quantiles of normal distribution is an easy task due to the computational power of computers. Historically, it was a little more complicated, though. Numerical evaluation of the integrals was a tiresome task. Still, luckily for the normal distribution, our ancestors could use one property - any linear transformation of the normal distribution is also normally distributed.\n\nThat means that any normal distribution could be transformed by standardisation (recall standard moments), and the result would be a standard normal distribution with zero expected value and unit variance. This distribution is so prominent that it has its specific notation: $Z\\sim N(0,1)$.\n\nSo, the need to calculate the integrals was reduced to only one distribution, and results were tabulated for general use. Hence, for $X\\sim N(\\mu, \\sigma)$, we can write\n\n$$\nZ=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\n$$\n\n$$\nF(x)= \\Phi \\left(\\frac {x-\\mu}{\\sigma}\\right)=\\Phi(z)\n$$\n\n$$\nx_p=\\mu+\\sigma z_p\n$$\n\nWhere $\\Phi$ is a notation for the cumulative distribution function of standard normal distribution $Z \\sim N(0,1)$, and $z_p$ is the 100*P*% quantile of the standard normal distribution, both to be found in the appropriate tables.\n\nAnother property of normal distribution worth mentioning is its symmetry around the expected value. For standard normal distribution, it means symmetry around 0, which leads to two relations used to simplify the tables mentioned above:\n\n$$\n\\Phi(z)=1-\\Phi(-z)\n$$\n\n$$\nz_p=-z_{1-p}\n$$\n\nThe standard normal distribution has become so pervasive that it is still used in statistics, e.g., in the form of *z*-*scores* or *critical values* for constructing *rejection regions*.\n\n*Z*-*scores* show how many standard deviations the value is from the variable's mean (or the random variable´s expected value to stay in probability theory).\n\n## Log-normal (Galton) distribution $X \\sim LN(\\mu,\\sigma)$\n\nIf the normal distribution rises as the distribution of a summation, there has to exist the distribution of a product. This is a log-normal distribution. The name suggests its relation to the normal distribution.\n\n$$\nX\\sim LN(\\mu,\\sigma) \\to Y=\\log (X) \\sim N(\\mu, \\sigma)\n$$\n\n$$\nY\\sim N(\\mu,\\sigma) \\to X=\\exp (Y) \\sim LN(\\mu, \\sigma)\n$$\n\nAgain, the cumulative distribution and quantile function do not have an analytical solution; hence, the relation was used.\n\n$$\nZ=\\frac{\\log(x)-\\mu}{\\sigma} \\sim N(0,1)\n$$ $$x_p=e^{\\mu+\\sigma z_p}$$\n\nYou can notice that parameters are the expected value and standard deviation of the log-transformed version of this distribution, not directly equal to them. Characteristics have to be obtained using formulas.\n\n$$\nE(X)=e^{\\mu+\\frac {\\sigma^2}{2}}\n$$\n\n$$\nVar(X)=e^{2\\mu+\\sigma^2}*\\left(e^{\\sigma^2} -1\\right)\n$$\n\nThis distribution is well-known and often used in statistical modelling for its shape - it is right-skewed and heavy-tailed, meaning it is suitable for the distribution with possible occurrences of extreme values.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nIn this part, we will continue with specific named probability distributions but focus on some continuous probability distributions.\n\n## Continuous uniform distribution $X\\sim U(a,b)$\n\nThis is likely the simplest continuous probability distribution. Uniform means equal probabilities for a discrete case or constant probability density function for a continuous case.\n\nThis distribution has limited real-world use, e.g. as a model of the waiting time for the regularly repeating event. Maybe the more important role of this distribution is its use in generating random numbers. Having generated a random value from the $X\\sim U(0,1)$ distribution, it can be an input to the quantile function of any desired random variable, generating a random value from that random variable.\n\nFrom the constant probability density function, we can derive that it is\n\n$$\nf(x)=\\frac{1}{b-a}\n$$\n\nfor $x\\in [a,b]$ and $-\\infty<a<b<\\infty$.\n\nFor probability calculations, the cumulative distribution function is usually easier to use\n\n$$\nF(x)=\\frac{x-a}{b-a}\n$$\n\nand the quantile function, the inverse of the cumulative distribution function\n\n$$\nQ(P)=a+(b-a)P\n$$\n\nThere is not much more to be said about this distribution; maybe the most common special case 𝑋∼𝑈(0,1) is called standard uniform distribution.\n\n## Exponential distribution $X\\sim E(\\lambda)$\n\nLet´s assume that random events are not regularly repeating but occurring randomly throughout time and independently of each other. The rate of occurrence of these events is the parameter $\\lambda$, the one we have already seen in the Poisson distribution with the same assumptions.\n\nThe focus of the Poisson distribution is the number of *events* in a specified interval; the focus of the exponential distribution is time to the first occurrence of the *event*. With the mentioned assumptions, this time follows the exponential distribution described by the following functions.\n\n$$\nf(x)=\\lambda \\exp(-\\lambda x)\n$$\n\n$$\nF(x)=1-\\exp(-\\lambda x)\n$$\n\n$$\nQ(P)=-\\frac{-\\ln (1-P)}{\\lambda} \n$$\n\nfor $x\\in(0;\\infty)$ and $\\lambda \\in [0,\\infty)$.\n\nThe two main moment characteristics of this distribution are\n\n$$\nE(X)=\\frac{1}{\\lambda}\n$$\n\n$$\nVar(X)=\\frac{1}{\\lambda^2}\n$$\n\nThe exponential and the Poisson distributions are intertwined and serve as the basis for analysing the so-called Poisson process. An exponential distribution is useful in other contexts; for example, it can serve as a model of the right tail of an unknown underlying distribution, which is useful in risk modelling.\n\nIn some distributions, especially continuous ones, parameters are not set in stone, and different parametrisations of the distribution can be used to shift focus on various properties. Of course, all the functions describing the probability distribution and its characteristics must be adjusted to the parametrisation of choice.\n\nFor exponential distribution, one might stumble upon using the scale parameter, which is the inverse of the rate parameter $\\sigma=\\frac{1}{\\lambda}$. This parametrisation might be useful in contexts where the focus is not the random events' occurrence rate but the expected value because $E(X)=\\delta$.\n\n## Normal (Gaussian) distribution $X\\sim N(\\mu,\\sigma)$\n\nThe normal distribution is pervasive in statistics because of its presence as a limited distribution of various statistics, most importantly summation. Normality assumption also simplifies a lot of calculations due to its properties. This makes it very useful in many contexts regarding estimates, which is the topic of the second large part of this lecture notes.\n\nThe probability density of the normal distribution is\n\n$$\nf(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$\n\nFor $x \\in (-\\infty;\\infty)$ and $\\mu \\in (-\\infty, \\infty)$, $\\sigma \\in(0,\\infty)$.\n\nIntegrating this density to get the cumulative distribution function is not analytically solvable, so we do not have a simple closed formula for evaluating its value.\n\n$$\nF(x)=\\int_{-\\infty}^x \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{- \\frac{(t-\\mu)^2}{2\\sigma^2}}dt\n$$\n\nWe do not have the closed formula for the quantile function based on the same argument. Characteristics of this random variable are straightforward from the parameters.\n\n$$\nE(X)=\\mu\n$$\n\n$$\nVar(X)=\\sigma^2\n$$\n\nNowadays, obtaining probabilities and quantiles of normal distribution is an easy task due to the computational power of computers. Historically, it was a little more complicated, though. Numerical evaluation of the integrals was a tiresome task. Still, luckily for the normal distribution, our ancestors could use one property - any linear transformation of the normal distribution is also normally distributed.\n\nThat means that any normal distribution could be transformed by standardisation (recall standard moments), and the result would be a standard normal distribution with zero expected value and unit variance. This distribution is so prominent that it has its specific notation: $Z\\sim N(0,1)$.\n\nSo, the need to calculate the integrals was reduced to only one distribution, and results were tabulated for general use. Hence, for $X\\sim N(\\mu, \\sigma)$, we can write\n\n$$\nZ=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)\n$$\n\n$$\nF(x)= \\Phi \\left(\\frac {x-\\mu}{\\sigma}\\right)=\\Phi(z)\n$$\n\n$$\nx_p=\\mu+\\sigma z_p\n$$\n\nWhere $\\Phi$ is a notation for the cumulative distribution function of standard normal distribution $Z \\sim N(0,1)$, and $z_p$ is the 100*P*% quantile of the standard normal distribution, both to be found in the appropriate tables.\n\nAnother property of normal distribution worth mentioning is its symmetry around the expected value. For standard normal distribution, it means symmetry around 0, which leads to two relations used to simplify the tables mentioned above:\n\n$$\n\\Phi(z)=1-\\Phi(-z)\n$$\n\n$$\nz_p=-z_{1-p}\n$$\n\nThe standard normal distribution has become so pervasive that it is still used in statistics, e.g., in the form of *z*-*scores* or *critical values* for constructing *rejection regions*.\n\n*Z*-*scores* show how many standard deviations the value is from the variable's mean (or the random variable´s expected value to stay in probability theory).\n\n## Log-normal (Galton) distribution $X \\sim LN(\\mu,\\sigma)$\n\nIf the normal distribution rises as the distribution of a summation, there has to exist the distribution of a product. This is a log-normal distribution. The name suggests its relation to the normal distribution.\n\n$$\nX\\sim LN(\\mu,\\sigma) \\to Y=\\log (X) \\sim N(\\mu, \\sigma)\n$$\n\n$$\nY\\sim N(\\mu,\\sigma) \\to X=\\exp (Y) \\sim LN(\\mu, \\sigma)\n$$\n\nAgain, the cumulative distribution and quantile function do not have an analytical solution; hence, the relation was used.\n\n$$\nZ=\\frac{\\log(x)-\\mu}{\\sigma} \\sim N(0,1)\n$$ $$x_p=e^{\\mu+\\sigma z_p}$$\n\nYou can notice that parameters are the expected value and standard deviation of the log-transformed version of this distribution, not directly equal to them. Characteristics have to be obtained using formulas.\n\n$$\nE(X)=e^{\\mu+\\frac {\\sigma^2}{2}}\n$$\n\n$$\nVar(X)=e^{2\\mu+\\sigma^2}*\\left(e^{\\sigma^2} -1\\right)\n$$\n\nThis distribution is well-known and often used in statistical modelling for its shape - it is right-skewed and heavy-tailed, meaning it is suitable for the distribution with possible occurrences of extreme values.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN6.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Selected Continuous Distributions","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}