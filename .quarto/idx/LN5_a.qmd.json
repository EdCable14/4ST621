{"title":"Random Vectors","markdown":{"yaml":{"title":"Random Vectors","author":"Adam Cabla"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nSo far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\n\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now *p*-dimensional sets or intervals of real numbers.\n\nSimply put, if one random event leads to *p* numbers, we are dealing with the random vector **X** and the realisation of this random vector **x**.\n\nWhile random vectors can have $p\\in\\mathbb{N}$ dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\n$$\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n$$\n\nRandom vectors have distributions that can be described jointly, marginally or conditionally.\n\n## Joint distribution\n\nThe joint distribution of a random vector is described as an intersection. Translated, the joint cumulative distribution function for a two-dimensional random vector is\n\n$$\nF(x_1,x_2)=P(X_1\\leq{x_1}\\cap X_2\\leq{x_2})\n$$\n\njoint probability funcion is\n\n$$ P(x_1,x_2)=P(X_1={x_1}\\cap X_2={x_2}) $$\n\nand joint probability density function is\n\n$$ f(x_1,x_2)=f(X_1={x_1}\\cap X_2={x_2}) $$\n\nWe are omitting a case of a mixed random vector (continuous and discrete random variables in one random vector).\n\nRelations between the mentioned descriptions of joint probability distributions are generalisations of the already seen relations in the random variables\n\n$$ F(x_1,x_2)=\\sum_{t=-\\infty}^{x_1}\\sum_{u=-\\infty}^{x_2}P(x_1,x_2)$$\n\n$$\nF(x_1,x_2)=\\int_{t=-\\infty}^{x_1}\\int_{u=-\\infty}^{x_2}f(x_1,x_2)dx_2dx_1\n$$\n\n$$\nf(x_1,x_2)=\\frac{\\partial\\partial F(x_1,x_2)}{\\partial x_1 \\partial x_2}\n$$\n\n## Marginal distribution\n\nMarginal distribution refers to any subset of the originating random vector independent of the omitted part of that random vector. Say we have a 5-dimensional random vector:\n\n$$\n\\textbf{X}=\n\\begin{pmatrix}\nX_1\\\\\nX_2\\\\\nX_3\\\\\nX_4\\\\\nX_5\\\\\n\\end{pmatrix}\n$$\n\nThen, all the following random vectors and random variables can be called marginal vectors or variables (this is not a complete enumeration, just examples):\n\n$$ \\textbf{X}= \\begin{pmatrix} X_1\\\\ X_2\\\\ X_3\\\\ X_4\\\\\\end{pmatrix},\n\\textbf{X}= \\begin{pmatrix} X_1\\\\ X_4 \\end{pmatrix},\nX_1 $$\n\nMarginal distributions can be obtained from the joint distribution. Using the probability function, one needs to sum joint probabilities across the variables he is *getting rid of*.\n\nThe idea explained in two dimensions is this: You want, e.g. the probability P(*X* = 1); this probability is a union of all the joint probabilities containing *X* = 1, e.g. P(*X*~1~ = 1, *X*~2~ = 0), P(*X*~1~ = 1, *X*~2~ = 1), P(*X*~1~ = 1, *X*~2~ = 2). Because these intersections are disjoint random events, the union is calculated as a simple summation.\n\n$$\nP(x_1)=\\sum_{x_2=-\\infty}^\\infty P(x_1, x_2)\n$$\n\nThe same logic applies to continuous random variables described by a probability density function but using continuous mathematics. i.e. integration\n\n\\\n$$\nf(x_1)=\\int_{x_2=-\\infty}^\\infty f(x_1, x_2)dx_2\n$$\n\nLast but not least comes the relation between marginal and joint cumulative distribution function, which is straightforward\n\n$$\nF(x_1)= P(X_1 \\leq x_1)=P(X_1 \\leq x_1 \\cap X_2 < \\infty)= F(x_1,x_2=\\infty)\n$$\n\nHaving marginal distributions of the random variables, all their characteristics can be calculated as usual. We will use marginal expected values and marginal variances for the characteristics of a random vector.\n\n## Joint characteristics\n\nJoint characteristics, or just characteristics of a random vector, characterise specific properties of a random vector as a whole. Again, for the needs of this course, we simplify and use only the most common characteristics.\n\nThe location of a random vector is commonly characterised by a vector of expected values, which is a vector composed of marginal expected values of marginal random variables. In two dimensions:\n\n$$\nE(\\textbf{X})= \\begin{pmatrix} E(X_1)\\\\ E(X_2) \\end{pmatrix}\n$$\n\nA variance-covariance matrix commonly characterises the variability of a random vector. In this matrix, marginal variances are used together with covariances.\n\n### Covariance\n\nCovariance is a measure of the linear relation between the two random variables. It can take any real value; if it is negative, the two random variables are negatively linearly associated; if it is positive, the two random variables are positively linearly associated; if it is zero, the two random variables are not linearly associated. The word linearly is important here - random variables can have non-linear associations, which can be visible using, e.g. regression or skedastic functions. Linearly can be viewed as *on average*. This will be discussed later in this chapter.\n\nMathematically, covariance is calculated from the definition\n\n$$\ncov(X_1,X_2)=E[(X_1-E(X_1))*(X_2-E(X_2))]\n$$\n\nwhich can be transformed into the evaluation formula\n\n$$\ncov(X_1, X_2)=E(X_1X_2)-E(X_1)E(X_2)\n$$\n\nTo calculate covariance, one can calculate the expected value of the product of the two random variables and subtract the product of their expected values.\n\nWhat if you wanted to calculate the covariance of the random variable with itself?\n\n$$\ncov(X_1,X_1)=E[(X_1-E(X_1))*(X_1-E(X_1))]=E(((X_1-E(X_1))^2\n)=var(X_1)$$\n\nor\n\n$$\ncov(X_1,X_1)=E(X_1X_1)-E(X_1)E(X_1)=E(X_1^2)-(E(X_1))^2=Var(X_1)\n$$\n\nVariance is the specific case of the covariance; thus, the variance-covariance matrix describes all possible covariances.\n\nThe last important property of covariance is its symmetry; from the evaluation formula\n\n$$\ncov(X_1,X_2)=E(X_1X_2)-E(X_1)E(X_2)=E(X_2X_1)-E(X_2)E(X_1)=cov(X_2,X_1)\n$$\n\nThe variance-covariance matrix is thus square and symmetric, having marginal variances on the main diagonal. It is also positive semi-definite, an important property for various statistical techniques using sample variance-covariance matrices.\n\n$$\n\\boldsymbol{\\Sigma}= \\begin{pmatrix} Var(X_1) & cov(X_1,X_2) \\\\\ncov(X_2,X_1) & Var(X_2)\n \\end{pmatrix}\n$$\n\n### Correlation\n\nThe covariance describes the direction of the linear association, but its value is unbounded and depends on the units of the random variables. Thus, we cannot compare values of the covariances across different pairs of random variables. Here comes the standardised version of the covariance that bounds its values to the range \\[-1; 1\\]. This standardised covariance is called correlation and is calculated as\n\n$$\n\\rho(X_1,X_2)=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\frac{cov(X_1,X_2)}{S.D.(X_1)S.D.(X_2)}\n$$\n\nThe standardisation leads to a unitless value, which can be compared across different pairs of random variables. Value 0 still means a linearly independent pair of random variables; value -1 means that one random variable is a negative linear function of the other, and value 1 indicates that one random variable is a positive linear function of the other.\n\nMost commonly, the correlation is somewhere in between. There are no hard bounds to a strong or weak association, even though you might find some simplifying tables with given bounds in various scientific fields. These tables are usually created by looking at the common correlations in the given field. For example, in social sciences, a correlation of 0.6 is typically referred to as strong simply because it is very unusually high value in the social sciences. On the other hand, in physics, a correlation of 0.6 would be considered weak because the correlation coefficients are typically higher than that in physics.\n\nCorrelation is also connected to the concept of *explained variability* in the linear regression analysis. This connection will be described in the chapter on two-dimensional normal distribution.\n\nThe correlation of the random variable with itself yields a value of 1\n\n$$\n\\rho(X_1,X_1)=\\frac{cov(X_1,X_1)}{\\sqrt{Var(X_1)*Var(X_1)}}=\n\\frac{Var(X_1)}{Var(X_1)}=1\n$$\n\nand the correlation is again symmetric:\n\n$$\n\\rho(X_2,X_1)=\\frac{cov(X_2,X_1)}{\\sqrt{Var(X_2)*Var(X_1)}}=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\rho(X_1,X_2)\n$$\n\nThis together means that the correlation matrix is squared and symmetric, having values one on the main diagonal. This matrix is also a positive semidefinite\n\n$$\n\\boldsymbol{\\rho}= \\begin{pmatrix} \n1 & \\rho(X_1, X_2) \\\\\n\\rho(X_2, X_1) & 1\n\\end{pmatrix}\n$$\n\nTo summarise, a random vector's two main characteristics are a vector of expected values, which describes the location, and a variance-covariance matrix, which describes variability and linear relations inside the vector. The third commonly used characteristic is a correlation matrix, which recalculates a variance-covariance matrix to a standardised form, describing the linear relations inside the vector.\n\n## Conditional distribution\n\nThe joint distribution represents the intersection, and the marginal distribution represents the *independent* distribution so that the last distribution will represent the *conditional* distribution. The notation is changed; instead of *x*~1~ and *x*~2~, *x* and *y* will be used for better clarity.\n\nIt is calculated in a similar way we saw with the two random events, with the probability of intersection divided by the probability of a condition:\n\n$$\nP(x|y)=\\frac{P(x,y)}{P(y)}\n$$\n\n$$ f(x|y)=\\frac{f(x,y)}{f(y)} $$\n\nSo, having joint distribution, we can calculate marginal distribution. And having both joint distribution and marginal distribution, we can calculate conditional distribution. If there is an association between the two variables, the conditional probability function or conditional probability density function will be a function of both the variable (*x*) and the condition (*y*).\n\n### Regression function\n\nThe regression function in probability theory is a function of the expected value of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) E(*X*\\|*y*). It is calculated the very same way as any expected value, but using conditional probability function or probability density function\n\n$$\nE(X|y)=\\sum_{x=-\\infty}^\\infty x*P(x|y)\n$$\n\n$$ E(X|y)=\\int_{x=-\\infty}^\\infty x*f(x|y)dx $$\n\nMathematically, we eliminate X but not Y in the result by integrating or summing across values of random variable X. The conditional expected value *E*(*X*\\|*y*) is usually, but not necessarily, a function of *Y* in case there is a relation between the two variables. The conditional expected values are always constants if no association exists between X and Y.\n\n### Skedastic function\n\nThe skedastic function in probability theory is a function of the variance of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) *Var*(*X*\\|*y*). It is calculated the very same way as any variance, but using conditional probability function or probability density function\n\n$$ E(X^2|y)=\\sum_{x=-\\infty}^\\infty x^2*P(x|y) $$\n\n$$ E(X^2|y)=\\int_{x=-\\infty}^\\infty x^2*f(x|y)dx $$\n\n$$\nVar(X|y)=E(X^2|y)-[E(X|y)]^2\n$$\n\nThe conditional variance *Var*(*X*\\|*y*) is usually, but not necessarily, a function of *Y* in case there is a relation between the two variables. The conditional variances are always constants if no association exists between X and Y.\n\n## Independence\n\nWe stumbled upon association and linear independence notions throughout the previous paragraphs. Let´s now formalise it a little.\n\nIn the same way, we introduced the definition of independence of two random events, we can start here\n\n$P(x|y)=P(x);f(x|y)=f(x);F(x|y)=F(x)$ for each possible value *x* and *y*.\n\nBasically, we claim that the two random variables are independent if the conditioning on one random variable does not change the probability distribution of the second one.\n\nFrom this, we can follow with the condition of independence,\n\n$$\nP(x,y)=P(x)*P(y)\n$$\n\n$$\nf(x,y)=f(x)*f(y)\n$$\n\n$$\n, soF(x,y)=F(x)*F(y)\n$$\n\nThis condition is both necessary and sufficient, meaning that if we can show one of these relations, the two random variables are independent, and if the two random variables are independent, then we can assume these relations.\n\nCovariance and correlation were introduced as measures of linear association. The regression function was introduced to describe how one random variable's expected values react to the other's values. Similarly, the skedastic function describes conditional variances. What are some takeaway points on these descriptions of relations and general independence?\n\n-   A linear relation can be described as a relation in which the regression function is close to the straight line.\n\n-   The regression function can take any shape, and no linearity is required (!). If the regression function is, *on* *average*, increasing, the correlation is positive, and if the regression function is, *on* *average*,decreasing, the correlation is negative.\n\n-   A higher correlation value is connected to the regression function being close to the straight line and the skedastic function being close to 0.\n\n-   Suppose one random variable is a linear function of the other. In that case, the regression is a straight line, and there is no conditional variability, so the skedastic function is equal to 0.\n\n-   If the two random variables are independent, thequalsey are also linearly independent, so covariance and correlation are 0. Also, regression and skedastic functions are constant.\n\n-   The opposite is not necessarily true! There can be non-linear relations such that covariance and correlation equal 0. Two random variables can even be dependent in such a way that regression or skedastic function is constant.\n\n## Mutual Information\n\nEven though the correlation coefficient is the most commonly used measure of the strength of the relation, it has a few flaws, most notably a connection to linearity and a less obviously hard interpretation of values. However, there are other ways to measure the strength of the association of the two random variables.\n\nThe one introduced here is connected to the information theory and notion of entropy introduced in the chapter on characteristics of random variables. We will continue using this characteristic only to measure the strength of the association of discrete random vectors.\n\nStatistically speaking, mutual information is measured as the Kullback-Leibler divergence (type of distance measure) between the joint probability distribution and probability distribution under the condition of independence. Kullback-Leibler divergence of one random variable from the other is\n\n$$\nD_{KL}(P||Q)=\\sum_{x\\in{X}}P(x)*log \\left( \\frac{P(x)}{Q(x)} \\right)\n$$\n\nUsing joint probability function $P(x)=P(x,y)$ and condition of independence $Q(x)=P(x)*P(y)$, we have definition of mutual information\n\n$$\nI(X;Y)=\\sum_{y\\in{Y}} \\sum_{x\\in{X}}P(x,y)*log \\left( \n\\frac{P(x,y)} {P(x)*P(y)}\n \\right)\n$$\n\nApart from the definition, mutual entropy can be calculated using various formulas. One of them uses entropies of marginal random variables and joint random vector\n\n$$\nI(X;Y)=H(X)+H(Y)-H(X;Y)\n$$\n\nThe entropy of the random vector is calculated the same way the entropy of random variables\n\n$$\nH(X;Y)=-\\sum_{y\\in{Y}} \\sum_{x \\in {X}}P(x,y)*log_b{P(x,y)}\n$$\n\nThe formula for calculating mutual information from the entropies shows that mutual information is the information content of both random variables minus the information content of their joint distribution.\n\nTranslated, it means that mutual information quantifies how much knowing the value of one random variable reduces uncertainty about the other random variable, measured, e.g. in bits, nats or dits.\n\n![Mutual information](_site/Pics/MI.png)\n\nThe mutual information has a clear advantage in covering all types of relations between the two random variables, not only linear. Also, if mutual information is 0, the two random variables are independent. It is also symmetric: $I(X;Y)=I(Y;X)$.\n\n### Uncertainty coefficients\n\nMutual information provides information gain in one variable by observing the second variable in bits (or other units depending on the base of logarithms used). The trouble is that the maximal information content depends on the number of possible values of the random variable, as discussed in the chapter on entropy. So sometimes, the need to use the normalised version that can provide us with relative information gain might be useful. The logic behind the uncertainty coefficient is that it will tell us what information gain is in one random variable by knowing the value of the second random variable provided as the percentage of the entropy, and thus maximal possible information gain, of that first random variable.\n\n$$\nU(X|Y)=\\frac{I(X,Y)}{H(X)}\\neq U(Y|X)=\\frac{I(Y,X)}{H(Y)}\n$$\n\nUncertainty coefficients are not generally symmetric because the two random variables usually have different entropies.\n\nTo repeat:\n\n-   Absolute information gain, mutual entropy, is symmetric.\n\n-   Relative information gain, uncertainty coefficient, is not (generally) symmetric.\n\nTo finish this chapter, let´s introduce one of the possible symmetric uncertainty coefficients. This answers the question, what is an average relative information gain in one variable by knowing the value of the second variable, assuming we do not know which of the two is known?\n\n$$\nU(X,Y)=\\frac{H(X)*U(X|Y)+H(Y)*U(Y|X)}{H(X)+H(Y)}=\\frac{2*I(X,Y)}{H(X)+H(y)}\n$$\n\nThis symmetric uncertainty coefficient is calculated as an entropy-weighted uncertainty coefficients.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nSo far, we have moved in one dimension, from one random trial through the possible random events stemming from that trial to the random variable mapping probability distribution on the real-valued axis from those random events.\n\nNow, we are going to extend this into multiple dimensions. A random vector is a measurable function that maps from the sample space of random events to the measurable space, now *p*-dimensional sets or intervals of real numbers.\n\nSimply put, if one random event leads to *p* numbers, we are dealing with the random vector **X** and the realisation of this random vector **x**.\n\nWhile random vectors can have $p\\in\\mathbb{N}$ dimensions and can be either discrete, continuous or even a mix of the two, we will make things easier in this course by working only with 2-dimensional random vectors.\n\n$$\n\\textbf{X}={X_1 \\choose{X_2}}; \\textbf{x}={x_1 \\choose{x_2}}\n$$\n\nRandom vectors have distributions that can be described jointly, marginally or conditionally.\n\n## Joint distribution\n\nThe joint distribution of a random vector is described as an intersection. Translated, the joint cumulative distribution function for a two-dimensional random vector is\n\n$$\nF(x_1,x_2)=P(X_1\\leq{x_1}\\cap X_2\\leq{x_2})\n$$\n\njoint probability funcion is\n\n$$ P(x_1,x_2)=P(X_1={x_1}\\cap X_2={x_2}) $$\n\nand joint probability density function is\n\n$$ f(x_1,x_2)=f(X_1={x_1}\\cap X_2={x_2}) $$\n\nWe are omitting a case of a mixed random vector (continuous and discrete random variables in one random vector).\n\nRelations between the mentioned descriptions of joint probability distributions are generalisations of the already seen relations in the random variables\n\n$$ F(x_1,x_2)=\\sum_{t=-\\infty}^{x_1}\\sum_{u=-\\infty}^{x_2}P(x_1,x_2)$$\n\n$$\nF(x_1,x_2)=\\int_{t=-\\infty}^{x_1}\\int_{u=-\\infty}^{x_2}f(x_1,x_2)dx_2dx_1\n$$\n\n$$\nf(x_1,x_2)=\\frac{\\partial\\partial F(x_1,x_2)}{\\partial x_1 \\partial x_2}\n$$\n\n## Marginal distribution\n\nMarginal distribution refers to any subset of the originating random vector independent of the omitted part of that random vector. Say we have a 5-dimensional random vector:\n\n$$\n\\textbf{X}=\n\\begin{pmatrix}\nX_1\\\\\nX_2\\\\\nX_3\\\\\nX_4\\\\\nX_5\\\\\n\\end{pmatrix}\n$$\n\nThen, all the following random vectors and random variables can be called marginal vectors or variables (this is not a complete enumeration, just examples):\n\n$$ \\textbf{X}= \\begin{pmatrix} X_1\\\\ X_2\\\\ X_3\\\\ X_4\\\\\\end{pmatrix},\n\\textbf{X}= \\begin{pmatrix} X_1\\\\ X_4 \\end{pmatrix},\nX_1 $$\n\nMarginal distributions can be obtained from the joint distribution. Using the probability function, one needs to sum joint probabilities across the variables he is *getting rid of*.\n\nThe idea explained in two dimensions is this: You want, e.g. the probability P(*X* = 1); this probability is a union of all the joint probabilities containing *X* = 1, e.g. P(*X*~1~ = 1, *X*~2~ = 0), P(*X*~1~ = 1, *X*~2~ = 1), P(*X*~1~ = 1, *X*~2~ = 2). Because these intersections are disjoint random events, the union is calculated as a simple summation.\n\n$$\nP(x_1)=\\sum_{x_2=-\\infty}^\\infty P(x_1, x_2)\n$$\n\nThe same logic applies to continuous random variables described by a probability density function but using continuous mathematics. i.e. integration\n\n\\\n$$\nf(x_1)=\\int_{x_2=-\\infty}^\\infty f(x_1, x_2)dx_2\n$$\n\nLast but not least comes the relation between marginal and joint cumulative distribution function, which is straightforward\n\n$$\nF(x_1)= P(X_1 \\leq x_1)=P(X_1 \\leq x_1 \\cap X_2 < \\infty)= F(x_1,x_2=\\infty)\n$$\n\nHaving marginal distributions of the random variables, all their characteristics can be calculated as usual. We will use marginal expected values and marginal variances for the characteristics of a random vector.\n\n## Joint characteristics\n\nJoint characteristics, or just characteristics of a random vector, characterise specific properties of a random vector as a whole. Again, for the needs of this course, we simplify and use only the most common characteristics.\n\nThe location of a random vector is commonly characterised by a vector of expected values, which is a vector composed of marginal expected values of marginal random variables. In two dimensions:\n\n$$\nE(\\textbf{X})= \\begin{pmatrix} E(X_1)\\\\ E(X_2) \\end{pmatrix}\n$$\n\nA variance-covariance matrix commonly characterises the variability of a random vector. In this matrix, marginal variances are used together with covariances.\n\n### Covariance\n\nCovariance is a measure of the linear relation between the two random variables. It can take any real value; if it is negative, the two random variables are negatively linearly associated; if it is positive, the two random variables are positively linearly associated; if it is zero, the two random variables are not linearly associated. The word linearly is important here - random variables can have non-linear associations, which can be visible using, e.g. regression or skedastic functions. Linearly can be viewed as *on average*. This will be discussed later in this chapter.\n\nMathematically, covariance is calculated from the definition\n\n$$\ncov(X_1,X_2)=E[(X_1-E(X_1))*(X_2-E(X_2))]\n$$\n\nwhich can be transformed into the evaluation formula\n\n$$\ncov(X_1, X_2)=E(X_1X_2)-E(X_1)E(X_2)\n$$\n\nTo calculate covariance, one can calculate the expected value of the product of the two random variables and subtract the product of their expected values.\n\nWhat if you wanted to calculate the covariance of the random variable with itself?\n\n$$\ncov(X_1,X_1)=E[(X_1-E(X_1))*(X_1-E(X_1))]=E(((X_1-E(X_1))^2\n)=var(X_1)$$\n\nor\n\n$$\ncov(X_1,X_1)=E(X_1X_1)-E(X_1)E(X_1)=E(X_1^2)-(E(X_1))^2=Var(X_1)\n$$\n\nVariance is the specific case of the covariance; thus, the variance-covariance matrix describes all possible covariances.\n\nThe last important property of covariance is its symmetry; from the evaluation formula\n\n$$\ncov(X_1,X_2)=E(X_1X_2)-E(X_1)E(X_2)=E(X_2X_1)-E(X_2)E(X_1)=cov(X_2,X_1)\n$$\n\nThe variance-covariance matrix is thus square and symmetric, having marginal variances on the main diagonal. It is also positive semi-definite, an important property for various statistical techniques using sample variance-covariance matrices.\n\n$$\n\\boldsymbol{\\Sigma}= \\begin{pmatrix} Var(X_1) & cov(X_1,X_2) \\\\\ncov(X_2,X_1) & Var(X_2)\n \\end{pmatrix}\n$$\n\n### Correlation\n\nThe covariance describes the direction of the linear association, but its value is unbounded and depends on the units of the random variables. Thus, we cannot compare values of the covariances across different pairs of random variables. Here comes the standardised version of the covariance that bounds its values to the range \\[-1; 1\\]. This standardised covariance is called correlation and is calculated as\n\n$$\n\\rho(X_1,X_2)=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\frac{cov(X_1,X_2)}{S.D.(X_1)S.D.(X_2)}\n$$\n\nThe standardisation leads to a unitless value, which can be compared across different pairs of random variables. Value 0 still means a linearly independent pair of random variables; value -1 means that one random variable is a negative linear function of the other, and value 1 indicates that one random variable is a positive linear function of the other.\n\nMost commonly, the correlation is somewhere in between. There are no hard bounds to a strong or weak association, even though you might find some simplifying tables with given bounds in various scientific fields. These tables are usually created by looking at the common correlations in the given field. For example, in social sciences, a correlation of 0.6 is typically referred to as strong simply because it is very unusually high value in the social sciences. On the other hand, in physics, a correlation of 0.6 would be considered weak because the correlation coefficients are typically higher than that in physics.\n\nCorrelation is also connected to the concept of *explained variability* in the linear regression analysis. This connection will be described in the chapter on two-dimensional normal distribution.\n\nThe correlation of the random variable with itself yields a value of 1\n\n$$\n\\rho(X_1,X_1)=\\frac{cov(X_1,X_1)}{\\sqrt{Var(X_1)*Var(X_1)}}=\n\\frac{Var(X_1)}{Var(X_1)}=1\n$$\n\nand the correlation is again symmetric:\n\n$$\n\\rho(X_2,X_1)=\\frac{cov(X_2,X_1)}{\\sqrt{Var(X_2)*Var(X_1)}}=\\frac{cov(X_1,X_2)}{\\sqrt{Var(X_1)*Var(X_2)}}=\\rho(X_1,X_2)\n$$\n\nThis together means that the correlation matrix is squared and symmetric, having values one on the main diagonal. This matrix is also a positive semidefinite\n\n$$\n\\boldsymbol{\\rho}= \\begin{pmatrix} \n1 & \\rho(X_1, X_2) \\\\\n\\rho(X_2, X_1) & 1\n\\end{pmatrix}\n$$\n\nTo summarise, a random vector's two main characteristics are a vector of expected values, which describes the location, and a variance-covariance matrix, which describes variability and linear relations inside the vector. The third commonly used characteristic is a correlation matrix, which recalculates a variance-covariance matrix to a standardised form, describing the linear relations inside the vector.\n\n## Conditional distribution\n\nThe joint distribution represents the intersection, and the marginal distribution represents the *independent* distribution so that the last distribution will represent the *conditional* distribution. The notation is changed; instead of *x*~1~ and *x*~2~, *x* and *y* will be used for better clarity.\n\nIt is calculated in a similar way we saw with the two random events, with the probability of intersection divided by the probability of a condition:\n\n$$\nP(x|y)=\\frac{P(x,y)}{P(y)}\n$$\n\n$$ f(x|y)=\\frac{f(x,y)}{f(y)} $$\n\nSo, having joint distribution, we can calculate marginal distribution. And having both joint distribution and marginal distribution, we can calculate conditional distribution. If there is an association between the two variables, the conditional probability function or conditional probability density function will be a function of both the variable (*x*) and the condition (*y*).\n\n### Regression function\n\nThe regression function in probability theory is a function of the expected value of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) E(*X*\\|*y*). It is calculated the very same way as any expected value, but using conditional probability function or probability density function\n\n$$\nE(X|y)=\\sum_{x=-\\infty}^\\infty x*P(x|y)\n$$\n\n$$ E(X|y)=\\int_{x=-\\infty}^\\infty x*f(x|y)dx $$\n\nMathematically, we eliminate X but not Y in the result by integrating or summing across values of random variable X. The conditional expected value *E*(*X*\\|*y*) is usually, but not necessarily, a function of *Y* in case there is a relation between the two variables. The conditional expected values are always constants if no association exists between X and Y.\n\n### Skedastic function\n\nThe skedastic function in probability theory is a function of the variance of one random variable under the condition of the other random variable (reminding we are now assuming only a two-dimensional vector) *Var*(*X*\\|*y*). It is calculated the very same way as any variance, but using conditional probability function or probability density function\n\n$$ E(X^2|y)=\\sum_{x=-\\infty}^\\infty x^2*P(x|y) $$\n\n$$ E(X^2|y)=\\int_{x=-\\infty}^\\infty x^2*f(x|y)dx $$\n\n$$\nVar(X|y)=E(X^2|y)-[E(X|y)]^2\n$$\n\nThe conditional variance *Var*(*X*\\|*y*) is usually, but not necessarily, a function of *Y* in case there is a relation between the two variables. The conditional variances are always constants if no association exists between X and Y.\n\n## Independence\n\nWe stumbled upon association and linear independence notions throughout the previous paragraphs. Let´s now formalise it a little.\n\nIn the same way, we introduced the definition of independence of two random events, we can start here\n\n$P(x|y)=P(x);f(x|y)=f(x);F(x|y)=F(x)$ for each possible value *x* and *y*.\n\nBasically, we claim that the two random variables are independent if the conditioning on one random variable does not change the probability distribution of the second one.\n\nFrom this, we can follow with the condition of independence,\n\n$$\nP(x,y)=P(x)*P(y)\n$$\n\n$$\nf(x,y)=f(x)*f(y)\n$$\n\n$$\n, soF(x,y)=F(x)*F(y)\n$$\n\nThis condition is both necessary and sufficient, meaning that if we can show one of these relations, the two random variables are independent, and if the two random variables are independent, then we can assume these relations.\n\nCovariance and correlation were introduced as measures of linear association. The regression function was introduced to describe how one random variable's expected values react to the other's values. Similarly, the skedastic function describes conditional variances. What are some takeaway points on these descriptions of relations and general independence?\n\n-   A linear relation can be described as a relation in which the regression function is close to the straight line.\n\n-   The regression function can take any shape, and no linearity is required (!). If the regression function is, *on* *average*, increasing, the correlation is positive, and if the regression function is, *on* *average*,decreasing, the correlation is negative.\n\n-   A higher correlation value is connected to the regression function being close to the straight line and the skedastic function being close to 0.\n\n-   Suppose one random variable is a linear function of the other. In that case, the regression is a straight line, and there is no conditional variability, so the skedastic function is equal to 0.\n\n-   If the two random variables are independent, thequalsey are also linearly independent, so covariance and correlation are 0. Also, regression and skedastic functions are constant.\n\n-   The opposite is not necessarily true! There can be non-linear relations such that covariance and correlation equal 0. Two random variables can even be dependent in such a way that regression or skedastic function is constant.\n\n## Mutual Information\n\nEven though the correlation coefficient is the most commonly used measure of the strength of the relation, it has a few flaws, most notably a connection to linearity and a less obviously hard interpretation of values. However, there are other ways to measure the strength of the association of the two random variables.\n\nThe one introduced here is connected to the information theory and notion of entropy introduced in the chapter on characteristics of random variables. We will continue using this characteristic only to measure the strength of the association of discrete random vectors.\n\nStatistically speaking, mutual information is measured as the Kullback-Leibler divergence (type of distance measure) between the joint probability distribution and probability distribution under the condition of independence. Kullback-Leibler divergence of one random variable from the other is\n\n$$\nD_{KL}(P||Q)=\\sum_{x\\in{X}}P(x)*log \\left( \\frac{P(x)}{Q(x)} \\right)\n$$\n\nUsing joint probability function $P(x)=P(x,y)$ and condition of independence $Q(x)=P(x)*P(y)$, we have definition of mutual information\n\n$$\nI(X;Y)=\\sum_{y\\in{Y}} \\sum_{x\\in{X}}P(x,y)*log \\left( \n\\frac{P(x,y)} {P(x)*P(y)}\n \\right)\n$$\n\nApart from the definition, mutual entropy can be calculated using various formulas. One of them uses entropies of marginal random variables and joint random vector\n\n$$\nI(X;Y)=H(X)+H(Y)-H(X;Y)\n$$\n\nThe entropy of the random vector is calculated the same way the entropy of random variables\n\n$$\nH(X;Y)=-\\sum_{y\\in{Y}} \\sum_{x \\in {X}}P(x,y)*log_b{P(x,y)}\n$$\n\nThe formula for calculating mutual information from the entropies shows that mutual information is the information content of both random variables minus the information content of their joint distribution.\n\nTranslated, it means that mutual information quantifies how much knowing the value of one random variable reduces uncertainty about the other random variable, measured, e.g. in bits, nats or dits.\n\n![Mutual information](_site/Pics/MI.png)\n\nThe mutual information has a clear advantage in covering all types of relations between the two random variables, not only linear. Also, if mutual information is 0, the two random variables are independent. It is also symmetric: $I(X;Y)=I(Y;X)$.\n\n### Uncertainty coefficients\n\nMutual information provides information gain in one variable by observing the second variable in bits (or other units depending on the base of logarithms used). The trouble is that the maximal information content depends on the number of possible values of the random variable, as discussed in the chapter on entropy. So sometimes, the need to use the normalised version that can provide us with relative information gain might be useful. The logic behind the uncertainty coefficient is that it will tell us what information gain is in one random variable by knowing the value of the second random variable provided as the percentage of the entropy, and thus maximal possible information gain, of that first random variable.\n\n$$\nU(X|Y)=\\frac{I(X,Y)}{H(X)}\\neq U(Y|X)=\\frac{I(Y,X)}{H(Y)}\n$$\n\nUncertainty coefficients are not generally symmetric because the two random variables usually have different entropies.\n\nTo repeat:\n\n-   Absolute information gain, mutual entropy, is symmetric.\n\n-   Relative information gain, uncertainty coefficient, is not (generally) symmetric.\n\nTo finish this chapter, let´s introduce one of the possible symmetric uncertainty coefficients. This answers the question, what is an average relative information gain in one variable by knowing the value of the second variable, assuming we do not know which of the two is known?\n\n$$\nU(X,Y)=\\frac{H(X)*U(X|Y)+H(Y)*U(Y|X)}{H(X)+H(Y)}=\\frac{2*I(X,Y)}{H(X)+H(y)}\n$$\n\nThis symmetric uncertainty coefficient is calculated as an entropy-weighted uncertainty coefficients.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN5_a.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Random Vectors","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}