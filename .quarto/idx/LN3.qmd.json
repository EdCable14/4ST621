{"title":"Characteristics of Random Variables","markdown":{"yaml":{"title":"Characteristics of Random Variables","author":"Adam Cabla"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nIn the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\n\nWhat is this *other information*? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\n-   Location\n\n-   Variability\n\n-   Skewness\n\n-   Kurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later.\n\n## Moments\n\n### Raw Moments\n\nThe moment is a concept known from ancient Greek physics, e.g. Archimedes on the centre of gravity of a lever: \"*A and B are equally balanced if their distances to the* *centre* *are inversely proportional to their weights.*\" The centre of gravity is generally a point around which the resultant torque due to gravity forces vanishes; the object is in balance.\n\nLetÂ´s imagine two persons weighing 60 and 80 kg swinging on a seesaw; the first is at point 0, and the second is at point 1. Where should the fulcrum be put so they can be balanced? According to Archimedes, their *distance to the centre should be inversely proportional to their weights*. So the first one should be at 8/14 of the distance to the centre, and the second one should be at 6/14 of the distance to the centre. The overall distance is 1, and the fulcrum should be at the point of 8/14. This point is the centre of gravity and the (first) moment.\n\nNow, how can this point be calculated? LetÂ´s imagine a real line; values 0 and 1 represent points on this line, and weights are relative to the sum of all weights, so their sum is 1; the relative weight of the first person sitting at point 0 is $60/(60+80)=6/14$ and the relative weight of the second person sitting at point 1 is $80/(60+80)=8/14$. The centre of gravity value can be calculated as $0*6/14+1*8/14=8/14$.\n\nLetÂ´s add the third person on the seesaw and put him at point 2 with a weight of 100 kg. So the relative weights are now 60/240 = 6/24, 8/24 and 10/24. See, their total is 1. The centre of gravity value can be calculated as $0*6/24+1*8/24+2*10/24=28/24\\approx{1.167}$. So, if you wanted to put a fulcrum below the seesaw for the line to be horizontal and stable, you would have to put it under the seesaw at this point.\n\nThis long introduction here tries to explain the (first raw) moment and why it is useful as a characteristic of the location. However, we derive various properties from three different types of moments and powers.\n\nThe first raw moment has been described so far, which, in probability theory, is also called expected value or simply expectation. We use values x as the points and probabilities P(x) as weights for discrete random variables. Because the sum of the *P*(*x*) equals 1, it is already relative weight. Similarly, we use the probability density function f(x) as the relative weight for continuous random variables. Still, since it is a continuous function, we must integrate instead of summing.\n\n$$\nE(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n$$\n\n$$ E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx $$\n\nNow, the k-th raw moment is defined as the expected value of the *k*-th power of the random variable *X*:\n\n$$ \\mu_k^Â´=E(X^k)= \\sum_{x=-\\infty}^{\\infty}x^k*P(x) $$\n\n$$ \\mu_k^Â´=E(X^k)=\\int_{x=-\\infty}^{\\infty}x^k*f(x)dx $$\n\n### Central Moments\n\nWe can use central moments for some properties besides the raw moments. Centralisation is the term used to put the centre of the random variable to equal 0. This leads to the description of the random variable independent of its location.\n\nWe do this by subtracting the expected value from the values of the random variable: $x-E(X)$. Now, the *k*-th central moment is defined as the expected value of the *k*-th power of the centralised random variable *X*:\n\n$$ \\mu_k=E((X-E(X))^k)= \\sum_{x=-\\infty}^{\\infty}(x-E(X))^k*P(x) $$\n\n$$ \\mu_k=E((X-E(X))^k)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^k*f(x)dx $$\n\n### Standardised Moments\n\nThe last type of moment is the standardised (or normalised) moment. Proces of standardisation start with centralisation (to put the expected value equal to 0) and continues with dividing by the square root of the second central moment, which is called standard deviation. This ensures that a standardised random variable's variance and standard deviation (moment measures of variability) equal 1, thus not influencing other properties we might desire to observe.\n\nNow, the *k*-th standardised moment is defined as the expected value of the *k*-th power of the standardised random variable *X*:\n\n$$ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*P(x) $$\n\n$$ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\int_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*f(x)dx $$\n\n## Characteristics of Location\n\nCharacteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?\n\n### Expected Value\n\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function *P*(*x*) or probability density function *f*(*x*).\n\n$$ \\mu_1^Â´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x) $$\n\n$$ \\mu_1^Â´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx $$\n\nThe expected value has several important properties; for now, we stay with two of them:\n\n-   For real constant *c:* $E(c)=c$\n\n-   Linearity of expectations, for real constants *a* and *c*: $E(a+cX)=a+c*E(X)$\n\n### Median\n\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x~0.5~ is 50% quantile, which means it is a value satisfying the following equation\n\n$$ F(x_{0.5})=P(X\\leq{x_{0.5}})=0.5 $$\n\n$$ x_{0.5}=Q(0.5)=F^{-1}(0.5) $$\n\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\n$$ F(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5 $$\n\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves.\n\n### Mode\n\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value *x*, for which probability density function *f*(*x*) has a local maximum in the range of the random variable.\n\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two *peaks* in its probability density function without looking for the larger of the two maxima.\n\nApart from mode, distributions can have antimode(s); this term refers to the points *x*, for which the probability density function *f*(*x*) has its local minima. These points are not, however, considered characteristic of location.\n\n## Characteristics of Variability\n\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\n### Variance and standard deviation\n\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable $x-E(X)$. This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\n$$E((x-E(X))^1)=\\mu_1$$This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have \"*distances to the* *centre* *inversely proportional to their weights\".*\n\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x) $$\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx $$\n\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\n$$Var(X)=E(X^2)-[E(X)]^2$$Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\n\nSome important properties of the variance are:\n\n-   Non-negativity $Var(X)\\geq{0}$\n\n-   Constant (real value *c*) has zero variance $Var(c)=0$\n\n-   For real constants *a* and *c:* $Var(a+cX)=c^2Var(X)$\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in *cm*, then the variance of the height would be measured in *cm*^2^.\n\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a *typical deviation* in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\n$$ S.D.(X)=\\sqrt{Var(X)} $$\n\n### Mean Absolute Deviation\n\nAnother possibility to deal with the problem that the first central moment is equal to 0 is to use absolute deviations instead of squared deviations:\n\n$$ MAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x) $$\n\n$$ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx $$\n\nIn probability theory, mean absolute deviation most commonly refers to the deviation from the expected value. This way of measuring variability is a more *common sense typical deviation* in the probability distribution than standard deviation.\n\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tightly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to the expected value.\n\n#### Intermezzo\n\nIf we were looking for the real-valued constant *c,* for which the probability-weighted sum of squared deviations from such a constant âˆ‘ð‘¥=âˆ’âˆžâˆž(ð‘¥âˆ’ð‘)2âˆ—ð‘ƒ(ð‘¥) is minimal, then it would be the $c=E(X)$. So, the variance is minimised if we use the expected value as the location characteristic. This has far-reaching consequences, e.g. for linear regression, in which conditional expected values are modelled by a linear combination of variables and the *optimal* constants in the model are found by minimising residual variance. But do not get disturbed too much here with thoughts of linear regression; it is not the scope of this course.\n\n#### Back to MAD\n\nUsing the same logic as in the intermezzo, what real-valued constant c would minimise the probability-weighted sum of absolute deviations $\\sum_{x=-\\infty}^\\infty |x-c|*P(x)$? Maybe surprisingly, the constant is not the expected value in this case, but median $c=x_{0.5}$.\n\nSo, using the same logic, in which the expected value is connected to the variance, the mean absolute deviation should be connected to the median. And by the same reasoning, when modelling conditional medians by a linear combination of variables, the *optimal* constants are found by minimising residual mean absolute deviations. This type of modelling is called median (or, more generally, quantile) regression. Again, do not get disturbed by this.\n\nSo, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Even the median absolute deviation shares the same acronym, MAD. Does it make your head hurt? Sorry, no one promised you it would always be easy.\n\nIn this course, we will use the expected value to measure location when calculating mean absolute deviation, so we will follow the formulas at the beginning of this sub-chapter.\n\n### Inter-quartile range\n\nApart from measuring variability by a moment-based approach, we can also use a quantile-based approach. We can measure ranges between pre-specified quantiles and use them to measure variability with probabilistic meaning. The one that we will stick to within the course is the inter-quartile range\n\n$$ IQR(X)=x_{0.75}-x_{0.25} $$\n\nThe inter-quartile range is the range (distance) needed to capture the middle 50 % of the probability distribution.\n\n### Entropy\n\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures the random variable's *expected ability to surprise* or *information content*.\n\nIt starts with a negative logarithmic transformation of probability $-log_b(P(x))$. Value with probability 0 can be viewed as impossible or *infinitely* *surprising,* and indeed $-log_b(0)=Inf$. On the second part of the scale, a value with probability 1 is sure to happen or *totally unsurprising* and indeed $-log_b(1)=0$. So, the initial transformation gives us the potential measure of *surprisingness,* and entropy measures the expected value of this\n\n$$ H(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x)) $$\n\nNotice that the introduced entropy measure is based on the probabilities and is suitable for only discrete random variables. The trouble with continuous random variables is that ð‘“(ð‘¥)â‰¥0, and if the probability density value is above 1, the logarithm is positive, and entropy can be negative. So, we will skip the problem of measuring the entropy of continuous random variables and use it only as a measure for discrete random variables.\n\nNow, what units is entropy measured in? It depends on the base of the logarithm - *b*. The most common base, the one used in this course, is 2. Then, entropy is measured in *bits*. However, other bases can be used, e.g., base 10 leads to units called *dits,* or base *e* leads to units called *nats*.\n\nThe little trouble with entropy is, that its value depends on the number of possible values *x* of random variable. Maximum entropy can be achieved if all the values of the random variable are equally likely (so-called discrete uniform distribution).\n\nAssume there are *k* possible values, each with probability $P(x)=\\frac{1}{k}$, then maximal entropy is\n\n$$\nH(X)=-\\sum_{x=- \\infty}^{\\infty}P(x)*log_b P(x)=-k*\\frac{1}{k}*log_b \\frac {1}{k}=log_b(k)\n$$\n\nSo for comparison among random variables with different numbers of possible values of random variable (*k*)*,* normalised entropy, called efficiency, can be used\n\n$$ \\eta(X)=\\frac{H(X)}{log_b(k)} $$\n\n## Skewness\n\nApart from the location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis, and the most common way of measuring them is based on standardised moments. These were already introduced, so to repeat, these are moments based on the *k*-th powers of standardised random variables, where the standardisation leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\n\nMoment measure of skewness is the 3rd standardised moment\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) $$\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx $$\n\nOperationally, we can distinguish the skewness or the *lean* of the distribution into three possibilities:\n\n-   Positive skewness - distribution is skewed towards the right, so the larger part of the mass is on the left.\n\n-   Null skewness - distribution is symmetric.\n\n-   Negative skewness - distribution is skewed towards the left, so the larger part of the mass is on the right.\n\n## Kurtosis\n\nThe moment measure of kurtosis is the 4th standardised moment\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) $$\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx $$\n\nWhile skewness is a somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of centralisation - the larger the mass is in the centre, the larger the kurtosis should be. But, nowadays, the prevailing view is that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\n\nDoes it make sense? At first sight, having a larger mass in the centre and the tails is contradictory, but it is true in many probability distributions. What is lacking in these distributions is the mass somewhere in between the centre and the tails.\n\nOperationally, we can distinguish the kurtosis into three possibilities by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\n-   Leptokurtic: $\\alpha_4>3$\n\n-   Mesokurtic: $\\alpha_4=3$\n\n-   Platykurtic: $\\alpha_4<3$\n\nDue to the direct comparability to the normal distribution kurtosis, the excess kurtosis ð›¼~4~âˆ’3 is sometimes used.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nIn the previous chapter, random variables, which serve as probability models, were introduced together with how their probability distributions can be described. The functions used for this description give the full description, meaning no information is hidden, and any other information can be derived from this description.\n\nWhat is this *other information*? Usually, we want to find meaningful shorthand descriptions of the properties of the random variable. These properties are typically called characteristics of random variables. Historically, four types of characteristics are of interest:\n\n-   Location\n\n-   Variability\n\n-   Skewness\n\n-   Kurtosis\n\nFor these types of characteristics, we can use various ways of characterisation; most commonly, we use quantiles - these were already introduced and moments to be introduced in a moment. There are, of course, various other possibilities, some of which will be discussed later.\n\n## Moments\n\n### Raw Moments\n\nThe moment is a concept known from ancient Greek physics, e.g. Archimedes on the centre of gravity of a lever: \"*A and B are equally balanced if their distances to the* *centre* *are inversely proportional to their weights.*\" The centre of gravity is generally a point around which the resultant torque due to gravity forces vanishes; the object is in balance.\n\nLetÂ´s imagine two persons weighing 60 and 80 kg swinging on a seesaw; the first is at point 0, and the second is at point 1. Where should the fulcrum be put so they can be balanced? According to Archimedes, their *distance to the centre should be inversely proportional to their weights*. So the first one should be at 8/14 of the distance to the centre, and the second one should be at 6/14 of the distance to the centre. The overall distance is 1, and the fulcrum should be at the point of 8/14. This point is the centre of gravity and the (first) moment.\n\nNow, how can this point be calculated? LetÂ´s imagine a real line; values 0 and 1 represent points on this line, and weights are relative to the sum of all weights, so their sum is 1; the relative weight of the first person sitting at point 0 is $60/(60+80)=6/14$ and the relative weight of the second person sitting at point 1 is $80/(60+80)=8/14$. The centre of gravity value can be calculated as $0*6/14+1*8/14=8/14$.\n\nLetÂ´s add the third person on the seesaw and put him at point 2 with a weight of 100 kg. So the relative weights are now 60/240 = 6/24, 8/24 and 10/24. See, their total is 1. The centre of gravity value can be calculated as $0*6/24+1*8/24+2*10/24=28/24\\approx{1.167}$. So, if you wanted to put a fulcrum below the seesaw for the line to be horizontal and stable, you would have to put it under the seesaw at this point.\n\nThis long introduction here tries to explain the (first raw) moment and why it is useful as a characteristic of the location. However, we derive various properties from three different types of moments and powers.\n\nThe first raw moment has been described so far, which, in probability theory, is also called expected value or simply expectation. We use values x as the points and probabilities P(x) as weights for discrete random variables. Because the sum of the *P*(*x*) equals 1, it is already relative weight. Similarly, we use the probability density function f(x) as the relative weight for continuous random variables. Still, since it is a continuous function, we must integrate instead of summing.\n\n$$\nE(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x)\n$$\n\n$$ E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx $$\n\nNow, the k-th raw moment is defined as the expected value of the *k*-th power of the random variable *X*:\n\n$$ \\mu_k^Â´=E(X^k)= \\sum_{x=-\\infty}^{\\infty}x^k*P(x) $$\n\n$$ \\mu_k^Â´=E(X^k)=\\int_{x=-\\infty}^{\\infty}x^k*f(x)dx $$\n\n### Central Moments\n\nWe can use central moments for some properties besides the raw moments. Centralisation is the term used to put the centre of the random variable to equal 0. This leads to the description of the random variable independent of its location.\n\nWe do this by subtracting the expected value from the values of the random variable: $x-E(X)$. Now, the *k*-th central moment is defined as the expected value of the *k*-th power of the centralised random variable *X*:\n\n$$ \\mu_k=E((X-E(X))^k)= \\sum_{x=-\\infty}^{\\infty}(x-E(X))^k*P(x) $$\n\n$$ \\mu_k=E((X-E(X))^k)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^k*f(x)dx $$\n\n### Standardised Moments\n\nThe last type of moment is the standardised (or normalised) moment. Proces of standardisation start with centralisation (to put the expected value equal to 0) and continues with dividing by the square root of the second central moment, which is called standard deviation. This ensures that a standardised random variable's variance and standard deviation (moment measures of variability) equal 1, thus not influencing other properties we might desire to observe.\n\nNow, the *k*-th standardised moment is defined as the expected value of the *k*-th power of the standardised random variable *X*:\n\n$$ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\sum_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*P(x) $$\n\n$$ \\alpha_k=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^k \\right)= \\int_{x=-\\infty}^{\\infty}\n\\left(\\frac{x-E(X)}{S.D.(X)}\\right)^k*f(x)dx $$\n\n## Characteristics of Location\n\nCharacteristics of location are an attempt to measure the center or typical value of the random variable. What would it be if we had to choose one number to represent the random variable?\n\n### Expected Value\n\nThe most commonly used characteristic of the location of a random variable is its expected value. This has already been introduced in the previous chapter; the expected value is the first raw moment - the centre of gravity, the point of balance of the mass represented by the probability function *P*(*x*) or probability density function *f*(*x*).\n\n$$ \\mu_1^Â´=E(X)=\\sum_{x=-\\infty}^{\\infty}x*P(x) $$\n\n$$ \\mu_1^Â´=E(X)=\\int_{x=-\\infty}^{\\infty}x*f(x)dx $$\n\nThe expected value has several important properties; for now, we stay with two of them:\n\n-   For real constant *c:* $E(c)=c$\n\n-   Linearity of expectations, for real constants *a* and *c*: $E(a+cX)=a+c*E(X)$\n\n### Median\n\nThe median has been mentioned in the chapter dealing with the quantile functions. Median x~0.5~ is 50% quantile, which means it is a value satisfying the following equation\n\n$$ F(x_{0.5})=P(X\\leq{x_{0.5}})=0.5 $$\n\n$$ x_{0.5}=Q(0.5)=F^{-1}(0.5) $$\n\nIn this course, for simplicity, we stay with the median as a characteristic used only for continuous random variables, so we can even specify\n\n$$ F(x_{0.5})=P(X\\leq{x_{0.5}})= \\int_{-\\infty}^{x_{0.5}}f(t)dt =0.5 $$\n\nIn simple terms, the median (for continuous random variable) is such a value for which half of the probability lies below this value and half above. It thus divides the distribution into two probabilistically equal halves.\n\n### Mode\n\nMode in the probability theory is the most probable value of a random variable. This holds for discrete random variables, while for continuous random variables, having no point probabilities, it is such value *x*, for which probability density function *f*(*x*) has a local maximum in the range of the random variable.\n\nProbability distributions can be multimodal - having more than one mode (in the special case of two modes, it is called bimodal). As stated, modes are most commonly related to local maxima, so bimodal distribution can have two *peaks* in its probability density function without looking for the larger of the two maxima.\n\nApart from mode, distributions can have antimode(s); this term refers to the points *x*, for which the probability density function *f*(*x*) has its local minima. These points are not, however, considered characteristic of location.\n\n## Characteristics of Variability\n\nCharacteristics of variability are an attempt to measure the typical spread of the random variable. What would it be if we had to choose one number to represent a diversity of the probability distribution of the random variable?\n\n### Variance and standard deviation\n\nThe idea of the typical spread of the random variable starts simply by taking the centred random variable $x-E(X)$. This random variable represents deviations of the values from its centre, represented by the expected value. So, it seems logical to find the typical deviation from the centre as another expected value:\n\n$$E((x-E(X))^1)=\\mu_1$$This is the first central moment. The problem of this moment is that it is always 0. This comes straight ahead from the definition of the expected value being the centre of gravity - all points have \"*distances to the* *centre* *inversely proportional to their weights\".*\n\nThere are several possible solutions to this problem, the most common one being to use the second power, hence the second central moment, which is in probability theory also called variance of the random variable:\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\sum_{x=-\\infty}^{\\infty}(x-E(X))^2*P(x) $$\n\n$$ \\mu_2=Var(X)=E((x-E(X))^2)=\\int_{x=-\\infty}^{\\infty}(x-E(X))^2*f(x)dx $$\n\nThis form is called definitional; for evaluation of variance, it might be useful to use a so-called computational form of variance:\n\n$$Var(X)=E(X^2)-[E(X)]^2$$Variance can be calculated as the second raw moment minus the second power of the first raw moment. In other words, as the expected value of the squared random variable minus the square of the expected value of the random variable.\n\nSome important properties of the variance are:\n\n-   Non-negativity $Var(X)\\geq{0}$\n\n-   Constant (real value *c*) has zero variance $Var(c)=0$\n\n-   For real constants *a* and *c:* $Var(a+cX)=c^2Var(X)$\n\nSo, the variance is the expected squared deviation from the expected value. The trouble with this definition is that variance is measured in the squared units of the random variable. Imagine a random variable being, e.g. height measured in *cm*, then the variance of the height would be measured in *cm*^2^.\n\nFrom a descriptive point of view, it is more useful to use standard deviation, which is the squared value of variance. This measures a *typical deviation* in the original units of the random variable but does not have a simple explanation of the meaning. Mathematically, we might refer to the standard deviation as an expected quadratic deviation from the expected value.\n\n$$ S.D.(X)=\\sqrt{Var(X)} $$\n\n### Mean Absolute Deviation\n\nAnother possibility to deal with the problem that the first central moment is equal to 0 is to use absolute deviations instead of squared deviations:\n\n$$ MAD(X)=E(|x-E(X)|)=\\sum_{x=-\\infty}^{\\infty}|x-E(X)|*P(x) $$\n\n$$ MAD(X)=E(|x-E(X)|)=\\int_{x=-\\infty}^{\\infty}|x-E(X)|*f(x)dx $$\n\nIn probability theory, mean absolute deviation most commonly refers to the deviation from the expected value. This way of measuring variability is a more *common sense typical deviation* in the probability distribution than standard deviation.\n\nStill, there are two main reasons why the standard deviation is more commonly used; first, it is tightly connected to the normal distribution, which belongs among the most important probability distributions for reasons we will get into later; second, it has one important connection to the expected value.\n\n#### Intermezzo\n\nIf we were looking for the real-valued constant *c,* for which the probability-weighted sum of squared deviations from such a constant âˆ‘ð‘¥=âˆ’âˆžâˆž(ð‘¥âˆ’ð‘)2âˆ—ð‘ƒ(ð‘¥) is minimal, then it would be the $c=E(X)$. So, the variance is minimised if we use the expected value as the location characteristic. This has far-reaching consequences, e.g. for linear regression, in which conditional expected values are modelled by a linear combination of variables and the *optimal* constants in the model are found by minimising residual variance. But do not get disturbed too much here with thoughts of linear regression; it is not the scope of this course.\n\n#### Back to MAD\n\nUsing the same logic as in the intermezzo, what real-valued constant c would minimise the probability-weighted sum of absolute deviations $\\sum_{x=-\\infty}^\\infty |x-c|*P(x)$? Maybe surprisingly, the constant is not the expected value in this case, but median $c=x_{0.5}$.\n\nSo, using the same logic, in which the expected value is connected to the variance, the mean absolute deviation should be connected to the median. And by the same reasoning, when modelling conditional medians by a linear combination of variables, the *optimal* constants are found by minimising residual mean absolute deviations. This type of modelling is called median (or, more generally, quantile) regression. Again, do not get disturbed by this.\n\nSo, do we use expected value or median when calculating mean absolute deviation? Well, both can and are being used under the same acronym. Even the median absolute deviation shares the same acronym, MAD. Does it make your head hurt? Sorry, no one promised you it would always be easy.\n\nIn this course, we will use the expected value to measure location when calculating mean absolute deviation, so we will follow the formulas at the beginning of this sub-chapter.\n\n### Inter-quartile range\n\nApart from measuring variability by a moment-based approach, we can also use a quantile-based approach. We can measure ranges between pre-specified quantiles and use them to measure variability with probabilistic meaning. The one that we will stick to within the course is the inter-quartile range\n\n$$ IQR(X)=x_{0.75}-x_{0.25} $$\n\nThe inter-quartile range is the range (distance) needed to capture the middle 50 % of the probability distribution.\n\n### Entropy\n\nThe last characteristic to be introduced as a measure of variability comes from the information theory. Entropy measures the random variable's *expected ability to surprise* or *information content*.\n\nIt starts with a negative logarithmic transformation of probability $-log_b(P(x))$. Value with probability 0 can be viewed as impossible or *infinitely* *surprising,* and indeed $-log_b(0)=Inf$. On the second part of the scale, a value with probability 1 is sure to happen or *totally unsurprising* and indeed $-log_b(1)=0$. So, the initial transformation gives us the potential measure of *surprisingness,* and entropy measures the expected value of this\n\n$$ H(X)=E(-log_b(P(x)))=\\sum_{x=-\\infty}^{\\infty}P(x)*-log_b(P(x)) $$\n\nNotice that the introduced entropy measure is based on the probabilities and is suitable for only discrete random variables. The trouble with continuous random variables is that ð‘“(ð‘¥)â‰¥0, and if the probability density value is above 1, the logarithm is positive, and entropy can be negative. So, we will skip the problem of measuring the entropy of continuous random variables and use it only as a measure for discrete random variables.\n\nNow, what units is entropy measured in? It depends on the base of the logarithm - *b*. The most common base, the one used in this course, is 2. Then, entropy is measured in *bits*. However, other bases can be used, e.g., base 10 leads to units called *dits,* or base *e* leads to units called *nats*.\n\nThe little trouble with entropy is, that its value depends on the number of possible values *x* of random variable. Maximum entropy can be achieved if all the values of the random variable are equally likely (so-called discrete uniform distribution).\n\nAssume there are *k* possible values, each with probability $P(x)=\\frac{1}{k}$, then maximal entropy is\n\n$$\nH(X)=-\\sum_{x=- \\infty}^{\\infty}P(x)*log_b P(x)=-k*\\frac{1}{k}*log_b \\frac {1}{k}=log_b(k)\n$$\n\nSo for comparison among random variables with different numbers of possible values of random variable (*k*)*,* normalised entropy, called efficiency, can be used\n\n$$ \\eta(X)=\\frac{H(X)}{log_b(k)} $$\n\n## Skewness\n\nApart from the location and variability of the random variables, we might be interested in other characteristics. The two most common among these are skewness and kurtosis, and the most common way of measuring them is based on standardised moments. These were already introduced, so to repeat, these are moments based on the *k*-th powers of standardised random variables, where the standardisation leads to the same expected value 0 and the same variance 1, thus effectively eliminating them from the evaluation of the shape of the distribution.\n\nMoment measure of skewness is the 3rd standardised moment\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) $$\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx $$\n\nOperationally, we can distinguish the skewness or the *lean* of the distribution into three possibilities:\n\n-   Positive skewness - distribution is skewed towards the right, so the larger part of the mass is on the left.\n\n-   Null skewness - distribution is symmetric.\n\n-   Negative skewness - distribution is skewed towards the left, so the larger part of the mass is on the right.\n\n## Kurtosis\n\nThe moment measure of kurtosis is the 4th standardised moment\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\sum_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*P(x) $$\n\n$$ \\alpha_3=E\\left( \\left( \\frac{X-E(X)}{S.D.(X)} \\right)^3 \\right)= \\int_{x=-\\infty}^{\\infty} \\left(\\frac{x-E(X)}{S.D.(X)}\\right)^3*f(x)dx $$\n\nWhile skewness is a somewhat self-explanatory term, kurtosis is not well known. Historically, kurtosis was viewed as a measure of centralisation - the larger the mass is in the centre, the larger the kurtosis should be. But, nowadays, the prevailing view is that kurtosis is a measure of the heaviness of the tails - the larger the mass is in the tails, the larger the kurtosis is.\n\nDoes it make sense? At first sight, having a larger mass in the centre and the tails is contradictory, but it is true in many probability distributions. What is lacking in these distributions is the mass somewhere in between the centre and the tails.\n\nOperationally, we can distinguish the kurtosis into three possibilities by comparing it to the kurtosis of the normal distribution, which is equal to 3:\n\n-   Leptokurtic: $\\alpha_4>3$\n\n-   Mesokurtic: $\\alpha_4=3$\n\n-   Platykurtic: $\\alpha_4<3$\n\nDue to the direct comparability to the normal distribution kurtosis, the excess kurtosis ð›¼~4~âˆ’3 is sometimes used.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Characteristics of Random Variables","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}