{"title":"Two Selected Multivariate Distributions","markdown":{"yaml":{"title":"Two Selected Multivariate Distributions","author":"Adam Cabla"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis part will continue with two specific named multivariate probability distributions.\n\n## Multinomial distribution $X\\sim Mn(n,\\boldsymbol{\\pi})$\n\nIn binomial distribution, Bernoulli´s random trials produced two possible outcomes - 0, 1, and counting how many 1s (*successes*) we can observe. The multinomial distribution is a generalisation of this idea.\n\nOne random trial can (has to) end up with exactly one of the *k* possible outcomes $i$ with given probabilities $\\pi_i$ for $i=1,2,...,k$. If we repeat this random trial *n* times and observe absolute frequencies $x_i$ of the occurrences of the possible outcomes, the probability distribution of a random vector $\\textbf{X}=(X_1,X_2,...,X_k)´$ is called multinomial. Its probability function is\n\n$$\nP\\left(\\textbf{X}= \\left( \\begin{matrix}\nx_1 \\\\\nx_2 \\\\\n... \\\\\nx_k \n\\end{matrix} \\right)=\\textbf{x} \\right) =\n\\frac{n!}{x_1!x_2!...x_k!}\\pi_1^{x_1}\\pi_2^{x_2}...\\pi_3^{x_3}\n$$\n\nFor $x_i \\in \\{0, 1, 2,...,n\\}; \\sum_{i=1}^kx_i=n$ and $n\\in \\{1,2,... \\}$, $\\pi_i \\in (0;1);\\sum_{i=1}^k \\pi_i=1$.\n\nThe characteristics of the random vector are\n\n$$\nE(\\textbf{X})= \\left( \\begin{matrix}\nn\\pi_1 \\\\\nn\\pi_1 \\\\\n... \\\\\nn\\pi_k\n\\end{matrix} \\right) = n\\boldsymbol{\\pi}\n$$\n\n$$\n\\boldsymbol{\\Sigma} = \\left( \\begin{matrix}\nn\\pi_1(1-\\pi_1) & -n\\pi_1\\pi_2 &... & -n\\pi_1\\pi_k \\\\\n-n\\pi_2\\pi_1 & n\\pi_2(1-\\pi_2) &...& -n\\pi_2\\pi_k \\\\\n... &... &... &... \\\\\n-n\\pi_k\\pi_1 & -n\\pi_k\\pi_2 & ...& n\\pi_k(1-\\pi_k)\n\\end{matrix}\n\\right)\n$$\n\nIf we put *k* = 2, the distribution is identical to the binomial distribution, only differently written.\n\n## Multivariate Normal Distribution $\\textbf{X} \\sim N^k(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$\n\nThe multivariate normal distribution is a generalised version of the normal distribution to *k* dimensions. Its vector parameters are characteristics at the same time.\n\n$$\nE(\\textbf{X})= \\left( \\begin{matrix}  \n\\mu_1 \\\\\n\\mu_2 \\\\\n... \\\\\n\\mu_k\n\\end{matrix}\n\\right)\n$$\n\n$$\n(\\boldsymbol\\Sigma)= \\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} & ... &\\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_2^2 & ... & \\sigma_{2k} \\\\\n...&...&...&... \\\\\n\\sigma_{k1} & \\sigma_{k2} & ... & \\sigma_k^2\n\\end{matrix}\n\\right)\n$$\n\nYou can see that the variance of the *i*-th random variable is noted as $\\sigma^2$, and the covariance between the *i*-th and *j-*th random variables is noted as $\\sigma_{ij}$. This notation is fairly common in the older probability theory texts as a general notation for variance and covariance. It is, however, reserved only for the (multivariate) normal distribution in these lecture notes.\n\nFor now, we will focus on the case of a two-variate (two-dimensional) normal distribution, *k* = 2.\n\nOne of the nice properties of a multivariate normal distribution is the stability of the normality of any linear transformation. In the end, it means that every marginal distribution is (multivariate) normal, and every conditional distribution is (multivariate) normal.\n\nMarginal characteristics are taken from the joint characteristics, and conditional characteristics, regression and skedastic functions are as follows:\n\n$$\nE(X_1|x_2)=\\mu_1+\\rho \\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2)\n$$\n\n$$\nVar(X_1|x_2)=\\sigma_1^2\\left(1-\\rho^2\\right)\n$$\n\nThe regression function is a linear function of $x_2$, and the formula expresses its reliance on the correlation - if the correlation is negative, it has a negative slope and vice versa.\n\nThe skedastic function shows the meaning of the correlation coefficient - its squared value tells us how much is the variance of $X_1$ reduced by knowing the value $x_2$. In linear regression analysis, this concept is known as *R*-*squared* and measures the fit of the regression function.\n\nThe last but not least important property to note here is that in a multivariate normal distribution, all the relations are linear, and the correlation coefficient is thus a correct measure of the strength of the association.\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nThis part will continue with two specific named multivariate probability distributions.\n\n## Multinomial distribution $X\\sim Mn(n,\\boldsymbol{\\pi})$\n\nIn binomial distribution, Bernoulli´s random trials produced two possible outcomes - 0, 1, and counting how many 1s (*successes*) we can observe. The multinomial distribution is a generalisation of this idea.\n\nOne random trial can (has to) end up with exactly one of the *k* possible outcomes $i$ with given probabilities $\\pi_i$ for $i=1,2,...,k$. If we repeat this random trial *n* times and observe absolute frequencies $x_i$ of the occurrences of the possible outcomes, the probability distribution of a random vector $\\textbf{X}=(X_1,X_2,...,X_k)´$ is called multinomial. Its probability function is\n\n$$\nP\\left(\\textbf{X}= \\left( \\begin{matrix}\nx_1 \\\\\nx_2 \\\\\n... \\\\\nx_k \n\\end{matrix} \\right)=\\textbf{x} \\right) =\n\\frac{n!}{x_1!x_2!...x_k!}\\pi_1^{x_1}\\pi_2^{x_2}...\\pi_3^{x_3}\n$$\n\nFor $x_i \\in \\{0, 1, 2,...,n\\}; \\sum_{i=1}^kx_i=n$ and $n\\in \\{1,2,... \\}$, $\\pi_i \\in (0;1);\\sum_{i=1}^k \\pi_i=1$.\n\nThe characteristics of the random vector are\n\n$$\nE(\\textbf{X})= \\left( \\begin{matrix}\nn\\pi_1 \\\\\nn\\pi_1 \\\\\n... \\\\\nn\\pi_k\n\\end{matrix} \\right) = n\\boldsymbol{\\pi}\n$$\n\n$$\n\\boldsymbol{\\Sigma} = \\left( \\begin{matrix}\nn\\pi_1(1-\\pi_1) & -n\\pi_1\\pi_2 &... & -n\\pi_1\\pi_k \\\\\n-n\\pi_2\\pi_1 & n\\pi_2(1-\\pi_2) &...& -n\\pi_2\\pi_k \\\\\n... &... &... &... \\\\\n-n\\pi_k\\pi_1 & -n\\pi_k\\pi_2 & ...& n\\pi_k(1-\\pi_k)\n\\end{matrix}\n\\right)\n$$\n\nIf we put *k* = 2, the distribution is identical to the binomial distribution, only differently written.\n\n## Multivariate Normal Distribution $\\textbf{X} \\sim N^k(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$\n\nThe multivariate normal distribution is a generalised version of the normal distribution to *k* dimensions. Its vector parameters are characteristics at the same time.\n\n$$\nE(\\textbf{X})= \\left( \\begin{matrix}  \n\\mu_1 \\\\\n\\mu_2 \\\\\n... \\\\\n\\mu_k\n\\end{matrix}\n\\right)\n$$\n\n$$\n(\\boldsymbol\\Sigma)= \\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} & ... &\\sigma_{1k} \\\\\n\\sigma_{21} & \\sigma_2^2 & ... & \\sigma_{2k} \\\\\n...&...&...&... \\\\\n\\sigma_{k1} & \\sigma_{k2} & ... & \\sigma_k^2\n\\end{matrix}\n\\right)\n$$\n\nYou can see that the variance of the *i*-th random variable is noted as $\\sigma^2$, and the covariance between the *i*-th and *j-*th random variables is noted as $\\sigma_{ij}$. This notation is fairly common in the older probability theory texts as a general notation for variance and covariance. It is, however, reserved only for the (multivariate) normal distribution in these lecture notes.\n\nFor now, we will focus on the case of a two-variate (two-dimensional) normal distribution, *k* = 2.\n\nOne of the nice properties of a multivariate normal distribution is the stability of the normality of any linear transformation. In the end, it means that every marginal distribution is (multivariate) normal, and every conditional distribution is (multivariate) normal.\n\nMarginal characteristics are taken from the joint characteristics, and conditional characteristics, regression and skedastic functions are as follows:\n\n$$\nE(X_1|x_2)=\\mu_1+\\rho \\frac{\\sigma_1}{\\sigma_2}(x_2-\\mu_2)\n$$\n\n$$\nVar(X_1|x_2)=\\sigma_1^2\\left(1-\\rho^2\\right)\n$$\n\nThe regression function is a linear function of $x_2$, and the formula expresses its reliance on the correlation - if the correlation is negative, it has a negative slope and vice versa.\n\nThe skedastic function shows the meaning of the correlation coefficient - its squared value tells us how much is the variance of $X_1$ reduced by knowing the value $x_2$. In linear regression analysis, this concept is known as *R*-*squared* and measures the fit of the regression function.\n\nThe last but not least important property to note here is that in a multivariate normal distribution, all the relations are linear, and the correlation coefficient is thus a correct measure of the strength of the association.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN7.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Two Selected Multivariate Distributions","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}