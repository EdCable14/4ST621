{"title":"Stochastic convergences","markdown":{"yaml":{"title":"Stochastic convergences","author":"Adam Cabla"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nWe are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\n\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process.\n\n## Convergence in probability & almost sure convergence\n\nStandard *deterministic* convergence usually claims something about the limit of a sequence for an increasing value of the variable, e.g.\n\n$$\n\\frac{1}{n} \\xrightarrow{n\\to\\infty}0\n$$\n\n$$\\lim_{n\\to\\infty}\\left(\\frac{1}{n}\\right)=0$$\n\nIn *stochastic* convergences, our limit is usually put upon some probability, not the variable's value. For convergence *in probability*, we start with a sequence of random variables $X_n$ and claim its convergence in probability to its limit $X$:\n\n$$\nX_n\\xrightarrow{n\\to\\infty;P}X \n$$\n\n$$\np\\lim(X_n)=X\n$$\n\nThe definition of *in probability* convergence is done after specifying some small positive constant $\\varepsilon>0$\n\n$$\nP(|X_n-X|<\\varepsilon)\\xrightarrow{n\\to\\infty}1\n$$\n\nor inversely\n\n$$\nP(|X_n-X|>\\varepsilon)\\xrightarrow{n\\to\\infty}0\n$$\n\nThe first formulation stresses that **the** **probability** of absolute deviation of a sequence from its limit being up to the constant $\\varepsilon$ increases; that is, the sequence converges *long-term*. The second formulation stresses that **the probability** of such deviation or larger is becoming smaller. That can be translated as saying that the probability of an *unusual* outcome decreases.\n\n### Bernoulli´s law\n\nThe first use of the in probability convergence is Bernoulli´s first formulation of the weak law of large numbers: Let $X_1,X_2,...,X_n$ be a sequence of independent random variables with Bernoulli distribution, $X_i\\sim Be(\\pi)$.\n\nLet a sequence of relative frequencies be calculated from these random variables that is, $P_n=\\frac{\\sum_{i=1}^{n}(X_i)}{n}$. This sequence will converge in probability to the $\\pi$.\n\n$$\nP_n \\xrightarrow{n\\to\\infty;P}\\pi\n$$\n\nThis is akin to saying that the larger the sample size, the closer the relative frequency is to the underlying probability *in probability*.\n\n### Knichin´s law\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$.\n\nLet a sequence of means be calculated from these random variables, that is, $\\overline{X_n}=\\frac{\\sum_{i=1}^{n}(X_i)}{n}$. This sequence will converge in probability to the expected value $E(X)$.\n\n$$ \\overline{X_n} \\xrightarrow{n\\to\\infty;P}E(X) $$\n\nThis is akin to saying that the larger the sample size, the closer the mean is to the underlying expected value *in probability*.\n\n### Kolmogorov´s law\n\nSo far, we have used *in* *probability* convergence. However, mathematical development in the early 20th century led to a different type of *stochastic* convergence, defined and proven for the two previous convergences, called *almost sure* (*a.s.*) convergence. This convergence is defined as\n\n$$\nP\\left(\\lim_{n\\to\\infty}X_n=X\\right)=1\n$$\n\nIn this type of convergence, the probability is not converging to 1; rather, the probability of convergence at some step *n* is 1. This claim is stronger than the previous one and leads to the so-called strong law of large numbers, also known as Kolmogorov´s law:\n\n$$ \\overline{X_n} \\xrightarrow{n\\to\\infty;a.s.}E(X) $$\n\nThis fundamental theoretical development, along with the ease of use, basically cemented the frequentist statistic as the default inferential procedure for more than half a century.\n\n## Convergence in distribution\n\nConvergences in probability or almost surely state the limit value of a sequence of random variables. Convergence in distribution state the limit **distribution** of values of a sequence of random variables.\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$, and cumulative distribution function $F(x)$.\n\nFrom this sequence of random variables, let´s have a sequence of cumulative distribution functions $F_n(x)$. Convergence in distribution states that this sequence of $F_n(x)$ converges to a specific *underlying* cumulative distribution function $F(x)$.\n\n$$\nF_n(x)\\xrightarrow{n\\to\\infty;D}F(x)\n$$\n\nOne of the results is that the larger the dataset, the closer the *empirical* cumulative distribution function (cumulative relative frequencies) is to the *theoretical* cumulative distribution function of a generating process.\n\n### de Moivre-Laplace central limit theorem\n\nThis theorem states the limit distribution of binomial distribution: Let $X_1,X_2,...,X_n$ be a sequence of independent random variables with Bernoulli distribution, $X_i\\sim Be(\\pi)$.\n\nLet a sequence of summations be calculated from these random variables, that is, $M_n=\\sum_{i=1}^{n}(X_i)$. This summation follows binomial distribution but also converges *in distribution* to the normal distribution:\n\n$$\nM_n\\sim Bi(n,\\pi)\\xrightarrow{n\\to\\infty;D}N \\left(n\\pi,\\sqrt{n\\pi(1-\\pi)}\\right) \n$$\n\nWe can make a similar claim about a sequence of means by linear transformation.\n\n$$\\overline{X_n}=P=\\frac{\\sum_{i=1}^{n} X_i}{n}\\xrightarrow{n\\to\\infty;D}N \\left(\\pi,\\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\right)$$One of the results is that for *n* large enough, we might approximate the binomial probabilities using the normal distribution. We denote this type of approximative distribution using a double-wave symbol $\\approx$:\n\n$$\n\\sum_{i=1}^nX_i \\approx N\\left(n\\pi, \\sqrt{n\\pi(1-\\pi)}  \\right)\n$$\n\n$$\nP_n\\approx N \\left(\\pi,\\sqrt{\\frac{\\pi(1- \\pi)}{n}}  \\right)\n$$\n\n#### Continuity correction\n\nApproximating binomial distribution, which is integer-valued, with a continuous normal distribution, might pose an interesting problem. This problem arises with a question formulation: for binomial distribution, the question of what is the probability of being less or equal to *x* is the same as the question of what is the probability of being less than *x*+1; for normal distribution, however, these are two different questions because there is now a probability between *x* and *x*+1.\n\n$$\nBi:P(X\\leq x)=P(X<x+1)\n$$\n\n$$ N:F(x)\\neq F(x+1) $$\n\nThe solution to this problem is straightforward: when the question is about less or equal, we add 1/2; when the question is about less, we subtract 1/2. By this operation, we get the same result no matter how the question is posed:\n\n$$\nN+CC:F(x+0.5)=F(x+1-0.5)\n$$\n\nThis is called continuity correction and is applicable any time we want to approximate an integer-valued random variable with a continuous one. If the random variable is discrete but not an integer-valued, we want to meet half the distance between the two values.\n\nThis applies, for example, to the means of discrete variables (and relative frequency as well), where we can derive continuity correction this way:\n\n$$\\frac{\\sum_{i=1}^n X_i \\pm1/2}{n}=\\overline{X} \\pm\\frac{1}{2n}$$\n\n### Lindenberg-Lévy central limit theorem\n\nSimilarly, as Knichin´s law is a generalisation of Bernoulli´s law, Lindenberg-Lévy's theorem generalises the de Moivre-Laplace theorem.\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$.\n\nLet a sequence of summations be calculated from these random variables, that is, $M_n=\\sum_{i=1}^{n}(X_i)$. This sequence convergences in distribution to the normal distribution:\n\n$$\nM_n \\xrightarrow{n\\to\\infty;D}N\\left(nE(X),\\sqrt{nVar(X)}\\right)\n$$\n\nAnd by linear transformation, the sequence of arithmetic means also converges to the normal distribution:\n\n$$\\overline{X_n}=\\frac{\\sum_{i=1}^n X_i}{n} \\xrightarrow{n\\to\\infty;D}N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)$$One of the results is that for *n* large enough, we might use a normal distribution to calculate probabilities of sample means. Again, the approximation notation is commonly used:\n\n$$ M_n \\approx N\\left(nE(X),\\sqrt{nVar(X)}\\right) $$\n\n$$\\overline{X_n}\\approx N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)$$\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nWe are finally getting to the last part of the probability part of our course, 4ST621. Here, a short discussion of and information on the basic stochastic convergences is provided. These convergences tell the story of how, under some circumstances, randomly generated values and distributions of some statistics converge to their limit, which are theoretical characteristics and probability distributions.\n\nIn frequentist statistics, this knowledge is used to make generalising claims (inferences) about the underlying characteristics of the data-generating process.\n\n## Convergence in probability & almost sure convergence\n\nStandard *deterministic* convergence usually claims something about the limit of a sequence for an increasing value of the variable, e.g.\n\n$$\n\\frac{1}{n} \\xrightarrow{n\\to\\infty}0\n$$\n\n$$\\lim_{n\\to\\infty}\\left(\\frac{1}{n}\\right)=0$$\n\nIn *stochastic* convergences, our limit is usually put upon some probability, not the variable's value. For convergence *in probability*, we start with a sequence of random variables $X_n$ and claim its convergence in probability to its limit $X$:\n\n$$\nX_n\\xrightarrow{n\\to\\infty;P}X \n$$\n\n$$\np\\lim(X_n)=X\n$$\n\nThe definition of *in probability* convergence is done after specifying some small positive constant $\\varepsilon>0$\n\n$$\nP(|X_n-X|<\\varepsilon)\\xrightarrow{n\\to\\infty}1\n$$\n\nor inversely\n\n$$\nP(|X_n-X|>\\varepsilon)\\xrightarrow{n\\to\\infty}0\n$$\n\nThe first formulation stresses that **the** **probability** of absolute deviation of a sequence from its limit being up to the constant $\\varepsilon$ increases; that is, the sequence converges *long-term*. The second formulation stresses that **the probability** of such deviation or larger is becoming smaller. That can be translated as saying that the probability of an *unusual* outcome decreases.\n\n### Bernoulli´s law\n\nThe first use of the in probability convergence is Bernoulli´s first formulation of the weak law of large numbers: Let $X_1,X_2,...,X_n$ be a sequence of independent random variables with Bernoulli distribution, $X_i\\sim Be(\\pi)$.\n\nLet a sequence of relative frequencies be calculated from these random variables that is, $P_n=\\frac{\\sum_{i=1}^{n}(X_i)}{n}$. This sequence will converge in probability to the $\\pi$.\n\n$$\nP_n \\xrightarrow{n\\to\\infty;P}\\pi\n$$\n\nThis is akin to saying that the larger the sample size, the closer the relative frequency is to the underlying probability *in probability*.\n\n### Knichin´s law\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$.\n\nLet a sequence of means be calculated from these random variables, that is, $\\overline{X_n}=\\frac{\\sum_{i=1}^{n}(X_i)}{n}$. This sequence will converge in probability to the expected value $E(X)$.\n\n$$ \\overline{X_n} \\xrightarrow{n\\to\\infty;P}E(X) $$\n\nThis is akin to saying that the larger the sample size, the closer the mean is to the underlying expected value *in probability*.\n\n### Kolmogorov´s law\n\nSo far, we have used *in* *probability* convergence. However, mathematical development in the early 20th century led to a different type of *stochastic* convergence, defined and proven for the two previous convergences, called *almost sure* (*a.s.*) convergence. This convergence is defined as\n\n$$\nP\\left(\\lim_{n\\to\\infty}X_n=X\\right)=1\n$$\n\nIn this type of convergence, the probability is not converging to 1; rather, the probability of convergence at some step *n* is 1. This claim is stronger than the previous one and leads to the so-called strong law of large numbers, also known as Kolmogorov´s law:\n\n$$ \\overline{X_n} \\xrightarrow{n\\to\\infty;a.s.}E(X) $$\n\nThis fundamental theoretical development, along with the ease of use, basically cemented the frequentist statistic as the default inferential procedure for more than half a century.\n\n## Convergence in distribution\n\nConvergences in probability or almost surely state the limit value of a sequence of random variables. Convergence in distribution state the limit **distribution** of values of a sequence of random variables.\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$, and cumulative distribution function $F(x)$.\n\nFrom this sequence of random variables, let´s have a sequence of cumulative distribution functions $F_n(x)$. Convergence in distribution states that this sequence of $F_n(x)$ converges to a specific *underlying* cumulative distribution function $F(x)$.\n\n$$\nF_n(x)\\xrightarrow{n\\to\\infty;D}F(x)\n$$\n\nOne of the results is that the larger the dataset, the closer the *empirical* cumulative distribution function (cumulative relative frequencies) is to the *theoretical* cumulative distribution function of a generating process.\n\n### de Moivre-Laplace central limit theorem\n\nThis theorem states the limit distribution of binomial distribution: Let $X_1,X_2,...,X_n$ be a sequence of independent random variables with Bernoulli distribution, $X_i\\sim Be(\\pi)$.\n\nLet a sequence of summations be calculated from these random variables, that is, $M_n=\\sum_{i=1}^{n}(X_i)$. This summation follows binomial distribution but also converges *in distribution* to the normal distribution:\n\n$$\nM_n\\sim Bi(n,\\pi)\\xrightarrow{n\\to\\infty;D}N \\left(n\\pi,\\sqrt{n\\pi(1-\\pi)}\\right) \n$$\n\nWe can make a similar claim about a sequence of means by linear transformation.\n\n$$\\overline{X_n}=P=\\frac{\\sum_{i=1}^{n} X_i}{n}\\xrightarrow{n\\to\\infty;D}N \\left(\\pi,\\sqrt{\\frac{\\pi(1-\\pi)}{n}} \\right)$$One of the results is that for *n* large enough, we might approximate the binomial probabilities using the normal distribution. We denote this type of approximative distribution using a double-wave symbol $\\approx$:\n\n$$\n\\sum_{i=1}^nX_i \\approx N\\left(n\\pi, \\sqrt{n\\pi(1-\\pi)}  \\right)\n$$\n\n$$\nP_n\\approx N \\left(\\pi,\\sqrt{\\frac{\\pi(1- \\pi)}{n}}  \\right)\n$$\n\n#### Continuity correction\n\nApproximating binomial distribution, which is integer-valued, with a continuous normal distribution, might pose an interesting problem. This problem arises with a question formulation: for binomial distribution, the question of what is the probability of being less or equal to *x* is the same as the question of what is the probability of being less than *x*+1; for normal distribution, however, these are two different questions because there is now a probability between *x* and *x*+1.\n\n$$\nBi:P(X\\leq x)=P(X<x+1)\n$$\n\n$$ N:F(x)\\neq F(x+1) $$\n\nThe solution to this problem is straightforward: when the question is about less or equal, we add 1/2; when the question is about less, we subtract 1/2. By this operation, we get the same result no matter how the question is posed:\n\n$$\nN+CC:F(x+0.5)=F(x+1-0.5)\n$$\n\nThis is called continuity correction and is applicable any time we want to approximate an integer-valued random variable with a continuous one. If the random variable is discrete but not an integer-valued, we want to meet half the distance between the two values.\n\nThis applies, for example, to the means of discrete variables (and relative frequency as well), where we can derive continuity correction this way:\n\n$$\\frac{\\sum_{i=1}^n X_i \\pm1/2}{n}=\\overline{X} \\pm\\frac{1}{2n}$$\n\n### Lindenberg-Lévy central limit theorem\n\nSimilarly, as Knichin´s law is a generalisation of Bernoulli´s law, Lindenberg-Lévy's theorem generalises the de Moivre-Laplace theorem.\n\nLet $X_1,X_2,...,X_n$ be a sequence of independent identically distributed random variables with any distribution with finite variance, $X_i\\sim ?(E(X),Var(X))$.\n\nLet a sequence of summations be calculated from these random variables, that is, $M_n=\\sum_{i=1}^{n}(X_i)$. This sequence convergences in distribution to the normal distribution:\n\n$$\nM_n \\xrightarrow{n\\to\\infty;D}N\\left(nE(X),\\sqrt{nVar(X)}\\right)\n$$\n\nAnd by linear transformation, the sequence of arithmetic means also converges to the normal distribution:\n\n$$\\overline{X_n}=\\frac{\\sum_{i=1}^n X_i}{n} \\xrightarrow{n\\to\\infty;D}N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)$$One of the results is that for *n* large enough, we might use a normal distribution to calculate probabilities of sample means. Again, the approximation notation is commonly used:\n\n$$ M_n \\approx N\\left(nE(X),\\sqrt{nVar(X)}\\right) $$\n\n$$\\overline{X_n}\\approx N\\left(E(X),\\sqrt{\\frac{Var(X)}{n}}\\right)$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"LN8.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","editor":"visual","theme":"cosmo","title":"Stochastic convergences","author":"Adam Cabla"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}